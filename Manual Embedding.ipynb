{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9c4008ee"
   },
   "outputs": [],
   "source": [
    "### Don't mind about this \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "###\n",
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import math\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3b7ef2b8"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def sigmoid(self,z):\n",
    "        return (1/(1+np.exp(-1*z)))\n",
    "\n",
    "    def _diff_sigmoid(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "\n",
    "\n",
    "    def _tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def _diff_tanh(self, z):\n",
    "        return 1-np.square(self._tanh(z))\n",
    "\n",
    "    def relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "\n",
    "    def _diff_relu(self, z):\n",
    "        a= np.zeros_like(z,dtype='int')\n",
    "        a[z>0] = 1\n",
    "        return a\n",
    "\n",
    "    def softmax(self,z):\n",
    "        exp = np.exp(z)\n",
    "        tot= exp.sum(axis=0)\n",
    "        t= exp/tot\n",
    "        return t\n",
    "    \n",
    "    def _diff_softmax(self, z,y):\n",
    "        yhat_r = self.softmax(z)\n",
    "        one_yi = y *-1*(1-yhat_r)\n",
    "        z=(1-y)*yhat_r\n",
    "        return one_yi +z\n",
    "\n",
    "    \n",
    "    def __init__(self,n_neurons,n_inputs, activation='relu'):\n",
    "        self.n_neurons = n_neurons #number of neurons \n",
    "        self.n_inputs = n_inputs #past number of samples\n",
    "        self.activation_name = activation\n",
    "\n",
    "        self.W= np.random.randn(self.n_neurons,self.n_inputs)*np.sqrt(2/self.n_inputs)\n",
    "        self.b= np.random.randn(self.n_neurons,1)*np.sqrt(2/self.n_inputs)\n",
    "\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "    \n",
    "    def _set_target(self, t):\n",
    "        self.target=t\n",
    "\n",
    "    def forward(self,Ai):\n",
    "        z = np.dot(self.W,Ai)+ self.b\n",
    "        if (self.activation_name=='relu'):\n",
    "            A = self.relu(z)\n",
    "        elif(self.activation_name=='softmax'):\n",
    "            A = self.softmax(z)\n",
    "        else:\n",
    "            A = self._tanh(z)\n",
    "        self.Ai = Ai\n",
    "        self.Z = z\n",
    "        return A\n",
    "    \n",
    "    def backward(self,inp):\n",
    "        if (self.activation_name=='relu'):\n",
    "            act_diff = self._diff_relu(self.Z)\n",
    "        elif(self.activation_name=='softmax'):\n",
    "            act_diff = self._diff_softmax(self.Z,self.target)\n",
    "        else:\n",
    "            act_diff = self._diff_tanh(self.Z) \n",
    "        e = np.ones((self.Ai.shape[1],1))\n",
    "        bet= inp * act_diff\n",
    "        self.dW= self.dW+ np.dot(bet,self.Ai.T)\n",
    "        self.db= self.db+ np.dot(bet,e)\n",
    "        return np.dot(self.W.T, bet)\n",
    "    \n",
    "    def zeroing_diff(self):\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "65546f45"
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self,lr):\n",
    "        self.alpha = lr\n",
    "        self.layers = []\n",
    "        self.t_loss=[]\n",
    "        self.v_loss=[]\n",
    "\n",
    "    def forward(self,X):\n",
    "        a = X   \n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "    \n",
    "    def backward(self,inp):\n",
    "        gd = inp\n",
    "        for layer in self.layers[::-1]:\n",
    "            gd = layer.backward(gd)\n",
    "            \n",
    "    def add_layer(self,n_neurons,n_inputs,act):\n",
    "        self.layers.append(Layer(n_neurons,n_inputs,act))\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        onehotY= y\n",
    "        yhat_r = np.max(onehotY*y_hat, axis=0,keepdims=True)\n",
    "        return (1/(y.shape[1]))*-1*np.sum(np.log(yhat_r))\n",
    "    \n",
    "    def fit(self,x_train,y_train):\n",
    "        M = x_train.shape[1]\n",
    "        for i in range(1):\n",
    "            y_hat = self.forward(x_train)      \n",
    "            gd=self.loss(y_train, y_hat)\n",
    "            print(\"Training loss: {}....\".format(gd))\n",
    "            self.t_loss.append(gd)\n",
    "            self.layers[-1]._set_target(y_train)\n",
    "            self.backward(1)   \n",
    "            # apply GD\n",
    "            for i in range(len(self.layers)):\n",
    "                self.layers[i].W = self.layers[i].W - (self.alpha * (self.layers[i].dW/M))\n",
    "                self.layers[i].b = self.layers[i].b - (self.alpha * (self.layers[i].db/M))\n",
    "                \n",
    "            for layer in self.layers:\n",
    "                layer.zeroing_diff()\n",
    "        return self.t_loss\n",
    "                \n",
    "    def predict(self,data): \n",
    "        y_hat= self.forward(data)\n",
    "        return y_hat\n",
    "\n",
    "    def save_weights(self, path):\n",
    "        params= []\n",
    "        for layer in self.layers:\n",
    "            layer_param= [layer.W , layer.b]\n",
    "            params.append(layer_param)\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(params, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b55537bd"
   },
   "source": [
    "# WORD2VEC ENCODING SKIPGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7de8a604"
   },
   "outputs": [],
   "source": [
    "df_all= pd.read_csv('All_data.csv')\n",
    "df_all\n",
    "titles_corpus= []\n",
    "for title in df_all[\"dic_words\"][:]:\n",
    "    new_list = list(ast.literal_eval(title).keys())\n",
    "    if (new_list != []):\n",
    "        titles_corpus.append(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2a1f44cd"
   },
   "outputs": [],
   "source": [
    "all_words = sorted(list({x for l in titles_corpus for x in l}))\n",
    "word_index = dict((w, i) for i, w in enumerate(all_words))\n",
    "\n",
    "def get_one_hot(word, size):\n",
    "    word_vec = np.zeros(size)\n",
    "    w_idx = word_index[word]\n",
    "    word_vec[w_idx] = 1\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6868ea43"
   },
   "outputs": [],
   "source": [
    "def get_training_data(titles_corpus, window):\n",
    "    data=[]\n",
    "    for sentence in titles_corpus:\n",
    "        if (len(sentence) >= 3):\n",
    "            for i in range (len(sentence)):\n",
    "                w_enc= get_one_hot(sentence[i], len(all_words))\n",
    "                context_words = []\n",
    "                for j in range(i-window, i+window+1):\n",
    "                    if j!=i and j<=len(sentence)-1 and j>=0:\n",
    "                        context_words.append(sentence[j])\n",
    "                for c in context_words:\n",
    "                    c_hotenc = get_one_hot(c, len(all_words))\n",
    "                    data.append([w_enc,c_hotenc])\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "44052600",
    "outputId": "8ff90b2f-4838-46c9-de34-b81ac14d5f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########epoch####### 1\n",
      "Training loss: 8.056057898894611....\n",
      "Training loss: 8.056171309817975....\n",
      "Training loss: 8.058084173903493....\n",
      "Training loss: 8.057922314912007....\n",
      "Training loss: 8.058757734351987....\n",
      "Training loss: 8.058479684857756....\n",
      "Training loss: 8.056100148834686....\n",
      "Training loss: 8.056970339879848....\n",
      "Training loss: 8.058200026956968....\n",
      "Training loss: 8.061039604602383....\n",
      "Training loss: 8.058186138467878....\n",
      "Training loss: 8.059832972496965....\n",
      "Training loss: 8.059421754190193....\n",
      "Training loss: 8.056722147107681....\n",
      "Training loss: 8.055643132719753....\n",
      "Training loss: 8.056334977769943....\n",
      "Training loss: 8.0559905500657....\n",
      "Training loss: 8.056285055155747....\n",
      "Training loss: 8.056832821064715....\n",
      "Training loss: 8.057701738791819....\n",
      "Training loss: 8.056338440923005....\n",
      "Training loss: 8.057781787410974....\n",
      "Training loss: 8.056426571556324....\n",
      "Training loss: 8.056183985273174....\n",
      "Training loss: 8.054631121892044....\n",
      "Training loss: 8.058535726702889....\n",
      "Training loss: 8.052507277689553....\n",
      "Training loss: 8.056875704663048....\n",
      "Training loss: 8.056935161106795....\n",
      "Training loss: 8.055172967836661....\n",
      "Training loss: 8.057511564964983....\n",
      "Training loss: 8.059660233920109....\n",
      "Training loss: 8.057978362673884....\n",
      "Training loss: 8.05927213465122....\n",
      "Training loss: 8.057482959251207....\n",
      "Training loss: 8.057242939130274....\n",
      "Training loss: 8.06101476920107....\n",
      "Training loss: 8.056282178941911....\n",
      "Training loss: 8.056023262792413....\n",
      "Training loss: 8.055344938234047....\n",
      "Training loss: 8.054215064077376....\n",
      "Training loss: 8.057696615102483....\n",
      "Training loss: 8.057355785450744....\n",
      "Training loss: 8.058167375985665....\n",
      "Training loss: 8.056993561973306....\n",
      "Training loss: 8.056760545205728....\n",
      "Training loss: 8.054738132804735....\n",
      "Training loss: 8.059134959941009....\n",
      "Training loss: 8.060162377450252....\n",
      "Training loss: 8.055707449027816....\n",
      "Training loss: 8.057593376946874....\n",
      "Training loss: 8.05868079851525....\n",
      "Training loss: 8.058532032930483....\n",
      "Training loss: 8.054764613412763....\n",
      "Training loss: 8.056841651573768....\n",
      "Training loss: 8.059322897459579....\n",
      "Training loss: 8.05721271681126....\n",
      "Training loss: 8.055605054161036....\n",
      "Training loss: 8.06188625864008....\n",
      "Training loss: 8.059646214365022....\n",
      "Training loss: 8.055157547358359....\n",
      "Training loss: 8.05283794481491....\n",
      "Training loss: 8.058718554644468....\n",
      "Training loss: 8.05976589475714....\n",
      "Training loss: 8.059751374465696....\n",
      "Training loss: 8.058217141245798....\n",
      "Training loss: 8.057553144060737....\n",
      "Training loss: 8.061484348848278....\n",
      "Training loss: 8.05570426172454....\n",
      "Training loss: 8.057040646404715....\n",
      "Training loss: 8.060026710055084....\n",
      "Training loss: 8.057137570243745....\n",
      "Training loss: 8.056011817953957....\n",
      "Training loss: 8.055777807267527....\n",
      "Training loss: 8.061207270837473....\n",
      "Training loss: 8.054836145040492....\n",
      "Training loss: 8.056443030955464....\n",
      "Training loss: 8.057646005459379....\n",
      "Training loss: 8.059549885335715....\n",
      "Training loss: 8.055160208950038....\n",
      "Training loss: 8.057834421690963....\n",
      "Training loss: 8.054189059199231....\n",
      "Training loss: 8.05688417923457....\n",
      "Training loss: 8.057725145768408....\n",
      "Training loss: 8.058953021740333....\n",
      "Training loss: 8.059988118065709....\n",
      "Training loss: 8.056404768607637....\n",
      "Training loss: 8.055714386008333....\n",
      "Training loss: 8.055875070159086....\n",
      "Training loss: 8.055163428350525....\n",
      "Training loss: 8.05902084459957....\n",
      "Training loss: 8.05620140882912....\n",
      "Training loss: 8.057349904802118....\n",
      "Training loss: 8.058827789110264....\n",
      "Training loss: 8.057169114532213....\n",
      "Training loss: 8.057300790100532....\n",
      "Training loss: 8.05663931159928....\n",
      "Training loss: 8.057749546030935....\n",
      "Training loss: 8.057303587032502....\n",
      "Training loss: 8.061332465001879....\n",
      "Training loss: 8.054270880617638....\n",
      "Training loss: 8.056164618177423....\n",
      "Training loss: 8.052537348653752....\n",
      "Training loss: 8.055170943234424....\n",
      "Training loss: 8.057928695539948....\n",
      "Training loss: 8.058266797493893....\n",
      "Training loss: 8.053187764652053....\n",
      "Training loss: 8.057500425760038....\n",
      "Training loss: 8.054681316684489....\n",
      "Training loss: 8.061606468788815....\n",
      "Training loss: 8.056500832703124....\n",
      "Training loss: 8.05426078961932....\n",
      "Training loss: 8.056023772236925....\n",
      "Training loss: 8.05764149247074....\n",
      "Training loss: 8.058215597281547....\n",
      "Training loss: 8.062181901962058....\n",
      "Training loss: 8.057026526408636....\n",
      "Training loss: 8.058816845066563....\n",
      "Training loss: 8.0579475676006....\n",
      "Training loss: 8.053829290506442....\n",
      "Training loss: 8.0562134003137....\n",
      "Training loss: 8.055345060059947....\n",
      "Training loss: 8.056552494941396....\n",
      "Training loss: 8.057930776444344....\n",
      "Training loss: 8.05536712142763....\n",
      "Training loss: 8.05500082482989....\n",
      "Training loss: 8.054529344811733....\n",
      "Training loss: 8.053320361425547....\n",
      "Training loss: 8.057821458444593....\n",
      "Training loss: 8.058961215984878....\n",
      "Training loss: 8.055041567786422....\n",
      "Training loss: 8.058583604775007....\n",
      "Training loss: 8.051677784376208....\n",
      "Training loss: 8.056605829606617....\n",
      "Training loss: 8.055117569612921....\n",
      "Training loss: 8.057906138509422....\n",
      "Training loss: 8.058481715041298....\n",
      "Training loss: 8.056034714379049....\n",
      "Training loss: 8.058800442469424....\n",
      "Training loss: 8.057005464231489....\n",
      "Training loss: 8.057766250215138....\n",
      "Training loss: 8.057317088090445....\n",
      "Training loss: 8.054988253132953....\n",
      "Training loss: 8.057621021213551....\n",
      "Training loss: 8.056274602846734....\n",
      "Training loss: 8.054004161651724....\n",
      "Training loss: 8.055572726435562....\n",
      "Training loss: 8.05370251651292....\n",
      "Training loss: 8.058997436292186....\n",
      "Training loss: 8.05835520400824....\n",
      "Training loss: 8.057063035065973....\n",
      "Training loss: 8.051491187325357....\n",
      "Training loss: 8.055320768219591....\n",
      "Training loss: 8.053670487581929....\n",
      "Training loss: 8.056078797934642....\n",
      "Training loss: 8.058367238072396....\n",
      "Training loss: 8.059543505495313....\n",
      "Training loss: 8.053236946940066....\n",
      "Training loss: 8.055280291684369....\n",
      "Training loss: 8.054227095293086....\n",
      "Training loss: 8.05501872145662....\n",
      "Training loss: 8.05246388351797....\n",
      "Training loss: 8.055844933117495....\n",
      "Training loss: 8.058433165638913....\n",
      "Training loss: 8.057067657093638....\n",
      "Training loss: 8.056674467296332....\n",
      "Training loss: 8.055015626841666....\n",
      "Training loss: 8.054612663695805....\n",
      "Training loss: 8.054802723647386....\n",
      "Training loss: 8.05538343890276....\n",
      "Training loss: 8.056129689508639....\n",
      "Training loss: 8.052052769660877....\n",
      "Training loss: 8.056503166632853....\n",
      "Training loss: 8.05601645698332....\n",
      "Training loss: 8.054378499835684....\n",
      "Training loss: 8.056454881434036....\n",
      "Training loss: 8.05796732242128....\n",
      "Training loss: 8.055898260141316....\n",
      "Training loss: 8.051374953055891....\n",
      "###########epoch####### 2\n",
      "Training loss: 8.053968716785498....\n",
      "Training loss: 8.05406601286999....\n",
      "Training loss: 8.055818415639607....\n",
      "Training loss: 8.055729352885862....\n",
      "Training loss: 8.056561492673422....\n",
      "Training loss: 8.056278614398492....\n",
      "Training loss: 8.053869462517492....\n",
      "Training loss: 8.054818394594053....\n",
      "Training loss: 8.056166921537521....\n",
      "Training loss: 8.058863181626254....\n",
      "Training loss: 8.056094170372441....\n",
      "Training loss: 8.057798315588308....\n",
      "Training loss: 8.057308335863976....\n",
      "Training loss: 8.054546305710632....\n",
      "Training loss: 8.053515051434696....\n",
      "Training loss: 8.054095684765425....\n",
      "Training loss: 8.053859882734582....\n",
      "Training loss: 8.054107933721701....\n",
      "Training loss: 8.054731527360158....\n",
      "Training loss: 8.055649136843703....\n",
      "Training loss: 8.054208227761288....\n",
      "Training loss: 8.055723528871667....\n",
      "Training loss: 8.054235662046256....\n",
      "Training loss: 8.053963136509994....\n",
      "Training loss: 8.05245395786766....\n",
      "Training loss: 8.056452967973328....\n",
      "Training loss: 8.050309101092562....\n",
      "Training loss: 8.0546433291955....\n",
      "Training loss: 8.054853216682522....\n",
      "Training loss: 8.053007454149094....\n",
      "Training loss: 8.055389112083764....\n",
      "Training loss: 8.057435025826853....\n",
      "Training loss: 8.05579654991707....\n",
      "Training loss: 8.05717444460327....\n",
      "Training loss: 8.055351917371645....\n",
      "Training loss: 8.055037505450507....\n",
      "Training loss: 8.058789644784905....\n",
      "Training loss: 8.054272274844354....\n",
      "Training loss: 8.053922176308262....\n",
      "Training loss: 8.053214697324632....\n",
      "Training loss: 8.05196277960978....\n",
      "Training loss: 8.055557497323843....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.055168001403702....\n",
      "Training loss: 8.055956082478234....\n",
      "Training loss: 8.054818417290454....\n",
      "Training loss: 8.054588516982989....\n",
      "Training loss: 8.052537131310437....\n",
      "Training loss: 8.057013225186843....\n",
      "Training loss: 8.058000171704716....\n",
      "Training loss: 8.05350007002831....\n",
      "Training loss: 8.05546022081079....\n",
      "Training loss: 8.056456578829298....\n",
      "Training loss: 8.056257700035715....\n",
      "Training loss: 8.052704972507268....\n",
      "Training loss: 8.05481762432386....\n",
      "Training loss: 8.057222029172351....\n",
      "Training loss: 8.055255213254378....\n",
      "Training loss: 8.053402611545913....\n",
      "Training loss: 8.059758087319064....\n",
      "Training loss: 8.057480037821769....\n",
      "Training loss: 8.052980122748597....\n",
      "Training loss: 8.050637542050545....\n",
      "Training loss: 8.056465495678935....\n",
      "Training loss: 8.057690871038739....\n",
      "Training loss: 8.057537334254588....\n",
      "Training loss: 8.056122217448491....\n",
      "Training loss: 8.055427283369765....\n",
      "Training loss: 8.05936770193574....\n",
      "Training loss: 8.053552506363259....\n",
      "Training loss: 8.05488733305766....\n",
      "Training loss: 8.057973749119643....\n",
      "Training loss: 8.054884607911555....\n",
      "Training loss: 8.053844483866069....\n",
      "Training loss: 8.053706392486417....\n",
      "Training loss: 8.05906746248411....\n",
      "Training loss: 8.052823765946822....\n",
      "Training loss: 8.054320891866606....\n",
      "Training loss: 8.055492531044738....\n",
      "Training loss: 8.057481256567373....\n",
      "Training loss: 8.052974598665573....\n",
      "Training loss: 8.05569605042249....\n",
      "Training loss: 8.052111602587111....\n",
      "Training loss: 8.054793308686358....\n",
      "Training loss: 8.055556510156356....\n",
      "Training loss: 8.056840819557326....\n",
      "Training loss: 8.057904151330698....\n",
      "Training loss: 8.054211988894325....\n",
      "Training loss: 8.05355961563314....\n",
      "Training loss: 8.053615601208959....\n",
      "Training loss: 8.053015723038536....\n",
      "Training loss: 8.05695140239802....\n",
      "Training loss: 8.05410413939177....\n",
      "Training loss: 8.055324163846407....\n",
      "Training loss: 8.05659761158003....\n",
      "Training loss: 8.05495534672044....\n",
      "Training loss: 8.055214367261339....\n",
      "Training loss: 8.054572934410574....\n",
      "Training loss: 8.05540747814292....\n",
      "Training loss: 8.055174718757563....\n",
      "Training loss: 8.059189449109777....\n",
      "Training loss: 8.052080081708837....\n",
      "Training loss: 8.054018368845481....\n",
      "Training loss: 8.050311688029003....\n",
      "Training loss: 8.053060303494354....\n",
      "Training loss: 8.055744447802896....\n",
      "Training loss: 8.056000081303868....\n",
      "Training loss: 8.050948732899224....\n",
      "Training loss: 8.055525852898553....\n",
      "Training loss: 8.052506423726582....\n",
      "Training loss: 8.059442184976438....\n",
      "Training loss: 8.05436865714155....\n",
      "Training loss: 8.052097805720832....\n",
      "Training loss: 8.053851308736778....\n",
      "Training loss: 8.05549368154748....\n",
      "Training loss: 8.056058402358282....\n",
      "Training loss: 8.060096873710052....\n",
      "Training loss: 8.054913901513196....\n",
      "Training loss: 8.056717782402066....\n",
      "Training loss: 8.055713701514087....\n",
      "Training loss: 8.05163082309786....\n",
      "Training loss: 8.054069662192717....\n",
      "Training loss: 8.053249713247364....\n",
      "Training loss: 8.05448154378933....\n",
      "Training loss: 8.055732398303231....\n",
      "Training loss: 8.053161069021632....\n",
      "Training loss: 8.052839623048866....\n",
      "Training loss: 8.052425013087799....\n",
      "Training loss: 8.051169626339986....\n",
      "Training loss: 8.05573153101678....\n",
      "Training loss: 8.056851716809687....\n",
      "Training loss: 8.052953583351204....\n",
      "Training loss: 8.056469473629855....\n",
      "Training loss: 8.049520404964744....\n",
      "Training loss: 8.054474256894272....\n",
      "Training loss: 8.053076903494176....\n",
      "Training loss: 8.055858937753863....\n",
      "Training loss: 8.05646357290395....\n",
      "Training loss: 8.053894553436868....\n",
      "Training loss: 8.056683457729342....\n",
      "Training loss: 8.05488070677704....\n",
      "Training loss: 8.055681946879838....\n",
      "Training loss: 8.05519804141497....\n",
      "Training loss: 8.052914620329757....\n",
      "Training loss: 8.055507790733243....\n",
      "Training loss: 8.054175948317864....\n",
      "Training loss: 8.05189754764528....\n",
      "Training loss: 8.053345030585959....\n",
      "Training loss: 8.051509893265948....\n",
      "Training loss: 8.056860417765224....\n",
      "Training loss: 8.056149566741855....\n",
      "Training loss: 8.05489551707903....\n",
      "Training loss: 8.049392788630685....\n",
      "Training loss: 8.053124554957357....\n",
      "Training loss: 8.05161990243604....\n",
      "Training loss: 8.054008594607332....\n",
      "Training loss: 8.056222003548651....\n",
      "Training loss: 8.057354204930443....\n",
      "Training loss: 8.051080945726161....\n",
      "Training loss: 8.053148426349713....\n",
      "Training loss: 8.052089333946899....\n",
      "Training loss: 8.052882551757685....\n",
      "Training loss: 8.05044873831443....\n",
      "Training loss: 8.053835554641715....\n",
      "Training loss: 8.05637215347669....\n",
      "Training loss: 8.054851559317775....\n",
      "Training loss: 8.054443608821307....\n",
      "Training loss: 8.052856077411056....\n",
      "Training loss: 8.052455750323498....\n",
      "Training loss: 8.052618060668166....\n",
      "Training loss: 8.05325799589492....\n",
      "Training loss: 8.053974748570122....\n",
      "Training loss: 8.04988253879523....\n",
      "Training loss: 8.054450384373455....\n",
      "Training loss: 8.053958803725106....\n",
      "Training loss: 8.05228370890515....\n",
      "Training loss: 8.054354854457848....\n",
      "Training loss: 8.055899656427759....\n",
      "Training loss: 8.053725944675074....\n",
      "Training loss: 8.049277248820506....\n",
      "###########epoch####### 3\n",
      "Training loss: 8.051903620382149....\n",
      "Training loss: 8.052000505825319....\n",
      "Training loss: 8.053615594216826....\n",
      "Training loss: 8.053603605408702....\n",
      "Training loss: 8.05439952725036....\n",
      "Training loss: 8.054121640490553....\n",
      "Training loss: 8.051691400779218....\n",
      "Training loss: 8.052703053953723....\n",
      "Training loss: 8.054173992996503....\n",
      "Training loss: 8.056739150197645....\n",
      "Training loss: 8.054051562815003....\n",
      "Training loss: 8.055801065777267....\n",
      "Training loss: 8.055247302698081....\n",
      "Training loss: 8.052419121742101....\n",
      "Training loss: 8.051407893274904....\n",
      "Training loss: 8.051895490130862....\n",
      "Training loss: 8.051796491204458....\n",
      "Training loss: 8.051972524098124....\n",
      "Training loss: 8.052669874874143....\n",
      "Training loss: 8.053634284756741....\n",
      "Training loss: 8.052132966746441....\n",
      "Training loss: 8.053691960260274....\n",
      "Training loss: 8.052096355302078....\n",
      "Training loss: 8.051755056151627....\n",
      "Training loss: 8.050319204187085....\n",
      "Training loss: 8.054385157770183....\n",
      "Training loss: 8.048148949622012....\n",
      "Training loss: 8.05244289693946....\n",
      "Training loss: 8.052796610191857....\n",
      "Training loss: 8.050881712477786....\n",
      "Training loss: 8.05332289550175....\n",
      "Training loss: 8.055253267493443....\n",
      "Training loss: 8.053652820998954....\n",
      "Training loss: 8.055112097416334....\n",
      "Training loss: 8.053242146259077....\n",
      "Training loss: 8.052861284794119....\n",
      "Training loss: 8.056632097546636....\n",
      "Training loss: 8.052303035823309....\n",
      "Training loss: 8.051868033079884....\n",
      "Training loss: 8.05112296861648....\n",
      "Training loss: 8.049757246040503....\n",
      "Training loss: 8.053447869374628....\n",
      "Training loss: 8.053017988805129....\n",
      "Training loss: 8.053781480408855....\n",
      "Training loss: 8.05269509068612....\n",
      "Training loss: 8.05244991527307....\n",
      "Training loss: 8.050381801687125....\n",
      "Training loss: 8.05494265989951....\n",
      "Training loss: 8.055875602789971....\n",
      "Training loss: 8.051330333365602....\n",
      "Training loss: 8.053399647584943....\n",
      "Training loss: 8.054273347594345....\n",
      "Training loss: 8.054009218477775....\n",
      "Training loss: 8.050689060621915....\n",
      "Training loss: 8.052832305614412....\n",
      "Training loss: 8.055164456448065....\n",
      "Training loss: 8.05333168878725....\n",
      "Training loss: 8.051235878650495....\n",
      "Training loss: 8.057683541968354....\n",
      "Training loss: 8.055358504621845....\n",
      "Training loss: 8.050853732700723....\n",
      "Training loss: 8.048482262564214....\n",
      "Training loss: 8.054253663791853....\n",
      "Training loss: 8.05565590143607....\n",
      "Training loss: 8.055340829867703....\n",
      "Training loss: 8.054079977806298....\n",
      "Training loss: 8.053320069097666....\n",
      "Training loss: 8.057298953779336....\n",
      "Training loss: 8.051439632431045....\n",
      "Training loss: 8.052756398004703....\n",
      "Training loss: 8.055955608521126....\n",
      "Training loss: 8.052668304172915....\n",
      "Training loss: 8.05172766649531....\n",
      "Training loss: 8.051677354930932....\n",
      "Training loss: 8.056980691069462....\n",
      "Training loss: 8.050869293076849....\n",
      "Training loss: 8.052235718146397....\n",
      "Training loss: 8.0533679078726....\n",
      "Training loss: 8.05544408697929....\n",
      "Training loss: 8.050842233167005....\n",
      "Training loss: 8.053611174032062....\n",
      "Training loss: 8.050063841670617....\n",
      "Training loss: 8.052743037918567....\n",
      "Training loss: 8.053433074622347....\n",
      "Training loss: 8.05475023178255....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.055861310807655....\n",
      "Training loss: 8.052056081358646....\n",
      "Training loss: 8.05142214384874....\n",
      "Training loss: 8.051385577066148....\n",
      "Training loss: 8.050907169861313....\n",
      "Training loss: 8.054942861461832....\n",
      "Training loss: 8.052046225314703....\n",
      "Training loss: 8.053331382137493....\n",
      "Training loss: 8.054397329808685....\n",
      "Training loss: 8.052769374159688....\n",
      "Training loss: 8.05317142067788....\n",
      "Training loss: 8.052550911157518....\n",
      "Training loss: 8.05312022507788....\n",
      "Training loss: 8.053069172872275....\n",
      "Training loss: 8.057082181150161....\n",
      "Training loss: 8.049928191553159....\n",
      "Training loss: 8.051917398154307....\n",
      "Training loss: 8.04810433195457....\n",
      "Training loss: 8.050979735866004....\n",
      "Training loss: 8.053591765692184....\n",
      "Training loss: 8.053778340406089....\n",
      "Training loss: 8.048747283588451....\n",
      "Training loss: 8.053580532089224....\n",
      "Training loss: 8.050387067498281....\n",
      "Training loss: 8.057312666278676....\n",
      "Training loss: 8.052270757720486....\n",
      "Training loss: 8.049990862523671....\n",
      "Training loss: 8.051708346042718....\n",
      "Training loss: 8.053388844584296....\n",
      "Training loss: 8.053949027844272....\n",
      "Training loss: 8.05804852299921....\n",
      "Training loss: 8.052837807993397....\n",
      "Training loss: 8.054657251533843....\n",
      "Training loss: 8.053511981454777....\n",
      "Training loss: 8.049468021284106....\n",
      "Training loss: 8.051974798467898....\n",
      "Training loss: 8.051207878920831....\n",
      "Training loss: 8.05244968642484....\n",
      "Training loss: 8.053585025985557....\n",
      "Training loss: 8.050997660564855....\n",
      "Training loss: 8.050731510645624....\n",
      "Training loss: 8.050364227199621....\n",
      "Training loss: 8.049065286761415....\n",
      "Training loss: 8.053676701122031....\n",
      "Training loss: 8.054786561002766....\n",
      "Training loss: 8.050899604246363....\n",
      "Training loss: 8.05439598942047....\n",
      "Training loss: 8.047391041143419....\n",
      "Training loss: 8.052362565691153....\n",
      "Training loss: 8.051075700649116....\n",
      "Training loss: 8.053852078014673....\n",
      "Training loss: 8.054490435436303....\n",
      "Training loss: 8.051783086556359....\n",
      "Training loss: 8.054601628619999....\n",
      "Training loss: 8.052778090022471....\n",
      "Training loss: 8.05363425937988....\n",
      "Training loss: 8.053133527742183....\n",
      "Training loss: 8.050878243170278....\n",
      "Training loss: 8.05342218335609....\n",
      "Training loss: 8.052116959654558....\n",
      "Training loss: 8.049816671140182....\n",
      "Training loss: 8.051156983566662....\n",
      "Training loss: 8.049340382852185....\n",
      "Training loss: 8.054756821152031....\n",
      "Training loss: 8.053967996515556....\n",
      "Training loss: 8.052757135058982....\n",
      "Training loss: 8.047330835322983....\n",
      "Training loss: 8.050960170809486....\n",
      "Training loss: 8.049594971345982....\n",
      "Training loss: 8.051950852445215....\n",
      "Training loss: 8.054122401945357....\n",
      "Training loss: 8.055208954783934....\n",
      "Training loss: 8.048978017991312....\n",
      "Training loss: 8.05106010962246....\n",
      "Training loss: 8.05001277891462....\n",
      "Training loss: 8.050774899307967....\n",
      "Training loss: 8.048456630641967....\n",
      "Training loss: 8.051846185443033....\n",
      "Training loss: 8.054332124530164....\n",
      "Training loss: 8.052668278418569....\n",
      "Training loss: 8.052244506683703....\n",
      "Training loss: 8.05072443039936....\n",
      "Training loss: 8.050349798656471....\n",
      "Training loss: 8.050463246538174....\n",
      "Training loss: 8.0511550925736....\n",
      "Training loss: 8.05185125116277....\n",
      "Training loss: 8.047736244219392....\n",
      "Training loss: 8.052422025794806....\n",
      "Training loss: 8.05193786544166....\n",
      "Training loss: 8.050213420867333....\n",
      "Training loss: 8.052290437377552....\n",
      "Training loss: 8.053860322646582....\n",
      "Training loss: 8.05157789026517....\n",
      "Training loss: 8.047258119959347....\n",
      "###########epoch####### 4\n",
      "Training loss: 8.049864256382536....\n",
      "Training loss: 8.049957403514261....\n",
      "Training loss: 8.051457913990003....\n",
      "Training loss: 8.051527161630853....\n",
      "Training loss: 8.052279711639768....\n",
      "Training loss: 8.051996370339335....\n",
      "Training loss: 8.049552744761145....\n",
      "Training loss: 8.050615480091668....\n",
      "Training loss: 8.052214663358424....\n",
      "Training loss: 8.054654121620484....\n",
      "Training loss: 8.05205694689169....\n",
      "Training loss: 8.053836817880425....\n",
      "Training loss: 8.053224723970674....\n",
      "Training loss: 8.050324136879325....\n",
      "Training loss: 8.049306223330841....\n",
      "Training loss: 8.049738493818051....\n",
      "Training loss: 8.049756134310675....\n",
      "Training loss: 8.049876680742878....\n",
      "Training loss: 8.050639456568826....\n",
      "Training loss: 8.051650496285246....\n",
      "Training loss: 8.050089050285289....\n",
      "Training loss: 8.051687252233133....\n",
      "Training loss: 8.049996047969701....\n",
      "Training loss: 8.049561339339641....\n",
      "Training loss: 8.048222910371127....\n",
      "Training loss: 8.052333700024242....\n",
      "Training loss: 8.046006700083069....\n",
      "Training loss: 8.050266913479746....\n",
      "Training loss: 8.050755601215561....\n",
      "Training loss: 8.048782745138142....\n",
      "Training loss: 8.05129300272122....\n",
      "Training loss: 8.053107861946492....\n",
      "Training loss: 8.051554627462396....\n",
      "Training loss: 8.053085273778844....\n",
      "Training loss: 8.051155118627593....\n",
      "Training loss: 8.050714645866444....\n",
      "Training loss: 8.054527020587376....\n",
      "Training loss: 8.050351438137662....\n",
      "Training loss: 8.049825569881175....\n",
      "Training loss: 8.049060740484332....\n",
      "Training loss: 8.04759371143473....\n",
      "Training loss: 8.05137316445763....\n",
      "Training loss: 8.05089806226263....\n",
      "Training loss: 8.051630976313312....\n",
      "Training loss: 8.05060948637835....\n",
      "Training loss: 8.050340041275552....\n",
      "Training loss: 8.048261950582729....\n",
      "Training loss: 8.052909666002739....\n",
      "Training loss: 8.05378361961372....\n",
      "Training loss: 8.04918673711197....\n",
      "Training loss: 8.051393635093826....\n",
      "Training loss: 8.052116827082303....\n",
      "Training loss: 8.051775182489362....\n",
      "Training loss: 8.048711426316643....\n",
      "Training loss: 8.050874670981878....\n",
      "Training loss: 8.053140294819269....\n",
      "Training loss: 8.05143767216195....\n",
      "Training loss: 8.04910797025035....\n",
      "Training loss: 8.055649323887808....\n",
      "Training loss: 8.053255252689498....\n",
      "Training loss: 8.048749503387441....\n",
      "Training loss: 8.04635695119277....\n",
      "Training loss: 8.052068845122212....\n",
      "Training loss: 8.053655503724361....\n",
      "Training loss: 8.05315465840728....\n",
      "Training loss: 8.052072497229046....\n",
      "Training loss: 8.051231314560921....\n",
      "Training loss: 8.055265081062373....\n",
      "Training loss: 8.0493429071451....\n",
      "Training loss: 8.050634775593856....\n",
      "Training loss: 8.053964320857203....\n",
      "Training loss: 8.050463216323458....\n",
      "Training loss: 8.049622346040485....\n",
      "Training loss: 8.049681054284004....\n",
      "Training loss: 8.05493009664851....\n",
      "Training loss: 8.048952151238913....\n",
      "Training loss: 8.050180268347603....\n",
      "Training loss: 8.051251967303061....\n",
      "Training loss: 8.053421936706009....\n",
      "Training loss: 8.048737307677891....\n",
      "Training loss: 8.051551444741815....\n",
      "Training loss: 8.048022803060546....\n",
      "Training loss: 8.050733903453848....\n",
      "Training loss: 8.051336982679667....\n",
      "Training loss: 8.052680005238187....\n",
      "Training loss: 8.053834494420142....\n",
      "Training loss: 8.049923989726674....\n",
      "Training loss: 8.049295790866434....\n",
      "Training loss: 8.049176650375015....\n",
      "Training loss: 8.048827607080867....\n",
      "Training loss: 8.052964427386312....\n",
      "Training loss: 8.050009645824423....\n",
      "Training loss: 8.051348231081723....\n",
      "Training loss: 8.052218332730007....\n",
      "Training loss: 8.050603129579363....\n",
      "Training loss: 8.051143125229512....\n",
      "Training loss: 8.050557895542928....\n",
      "Training loss: 8.050864104003002....\n",
      "Training loss: 8.050975947688071....\n",
      "Training loss: 8.055007961068442....\n",
      "Training loss: 8.047797529706445....\n",
      "Training loss: 8.049842078617502....\n",
      "Training loss: 8.04590667185842....\n",
      "Training loss: 8.048916012983238....\n",
      "Training loss: 8.05146167859709....\n",
      "Training loss: 8.05158162536597....\n",
      "Training loss: 8.046579384351494....\n",
      "Training loss: 8.051640031555....\n",
      "Training loss: 8.04829331213518....\n",
      "Training loss: 8.055211890490055....\n",
      "Training loss: 8.050189716460128....\n",
      "Training loss: 8.047932490780358....\n",
      "Training loss: 8.049581152516717....\n",
      "Training loss: 8.051297753622924....\n",
      "Training loss: 8.051878513578117....\n",
      "Training loss: 8.056020002371078....\n",
      "Training loss: 8.050781829692019....\n",
      "Training loss: 8.052621177352389....\n",
      "Training loss: 8.051330206133999....\n",
      "Training loss: 8.047318214622697....\n",
      "Training loss: 8.049900498571056....\n",
      "Training loss: 8.049203223489952....\n",
      "Training loss: 8.05044235983961....\n",
      "Training loss: 8.051468939084451....\n",
      "Training loss: 8.048863116531994....\n",
      "Training loss: 8.048655497520738....\n",
      "Training loss: 8.048326802035277....\n",
      "Training loss: 8.04698694221941....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.051644736907006....\n",
      "Training loss: 8.05274433517165....\n",
      "Training loss: 8.048865310957291....\n",
      "Training loss: 8.052345263080896....\n",
      "Training loss: 8.045280948894298....\n",
      "Training loss: 8.050261851657655....\n",
      "Training loss: 8.049100663064708....\n",
      "Training loss: 8.05187411813049....\n",
      "Training loss: 8.052532098828284....\n",
      "Training loss: 8.049682055586503....\n",
      "Training loss: 8.052547630493269....\n",
      "Training loss: 8.05067545112261....\n",
      "Training loss: 8.051607313112196....\n",
      "Training loss: 8.051091939287852....\n",
      "Training loss: 8.04886127844926....\n",
      "Training loss: 8.051352982919099....\n",
      "Training loss: 8.05007653235047....\n",
      "Training loss: 8.047750867518369....\n",
      "Training loss: 8.049001586841605....\n",
      "Training loss: 8.047177569776544....\n",
      "Training loss: 8.052676028992378....\n",
      "Training loss: 8.051802202668737....\n",
      "Training loss: 8.050648183734461....\n",
      "Training loss: 8.045280463021387....\n",
      "Training loss: 8.048818354727663....\n",
      "Training loss: 8.047590320294503....\n",
      "Training loss: 8.049905914786319....\n",
      "Training loss: 8.052055586408922....\n",
      "Training loss: 8.05308212838284....\n",
      "Training loss: 8.04690038183298....\n",
      "Training loss: 8.048992058057273....\n",
      "Training loss: 8.047969678802781....\n",
      "Training loss: 8.04867645357778....\n",
      "Training loss: 8.046487841047504....\n",
      "Training loss: 8.049860567362723....\n",
      "Training loss: 8.052311449570523....\n",
      "Training loss: 8.050503003864106....\n",
      "Training loss: 8.050065732416176....\n",
      "Training loss: 8.048599799603489....\n",
      "Training loss: 8.048272932572857....\n",
      "Training loss: 8.048329290999916....\n",
      "Training loss: 8.04907174465508....\n",
      "Training loss: 8.049732569072622....\n",
      "Training loss: 8.04561534048022....\n",
      "Training loss: 8.050405894919697....\n",
      "Training loss: 8.049926626097273....\n",
      "Training loss: 8.048160712173217....\n",
      "Training loss: 8.050244629337579....\n",
      "Training loss: 8.051833643151442....\n",
      "Training loss: 8.049435258139763....\n",
      "Training loss: 8.045256890862904....\n",
      "###########epoch####### 5\n",
      "Training loss: 8.047840833482763....\n",
      "Training loss: 8.047923498008954....\n",
      "Training loss: 8.049318984712873....\n",
      "Training loss: 8.049477419012009....\n",
      "Training loss: 8.050178919482857....\n",
      "Training loss: 8.049890404353365....\n",
      "Training loss: 8.047436313437958....\n",
      "Training loss: 8.048541183886611....\n",
      "Training loss: 8.050262139345607....\n",
      "Training loss: 8.052589174202627....\n",
      "Training loss: 8.050090445458327....\n",
      "Training loss: 8.051874951348257....\n",
      "Training loss: 8.051228641758247....\n",
      "Training loss: 8.048252789794363....\n",
      "Training loss: 8.047200282947491....\n",
      "Training loss: 8.047606161869624....\n",
      "Training loss: 8.047724199593542....\n",
      "Training loss: 8.047795270870605....\n",
      "Training loss: 8.048632150911821....\n",
      "Training loss: 8.04966896810346....\n",
      "Training loss: 8.048061675699637....\n",
      "Training loss: 8.049700158110772....\n",
      "Training loss: 8.047923989719024....\n",
      "Training loss: 8.047368719370565....\n",
      "Training loss: 8.046145213473926....\n",
      "Training loss: 8.050288238221887....\n",
      "Training loss: 8.04386691167472....\n",
      "Training loss: 8.048102122385286....\n",
      "Training loss: 8.048722029351026....\n",
      "Training loss: 8.046683058860637....\n",
      "Training loss: 8.04928870015705....\n",
      "Training loss: 8.050985748934464....\n",
      "Training loss: 8.04947769718123....\n",
      "Training loss: 8.051074511456212....\n",
      "Training loss: 8.049092821879977....\n",
      "Training loss: 8.04857889446292....\n",
      "Training loss: 8.052447399049093....\n",
      "Training loss: 8.048419711687082....\n",
      "Training loss: 8.047798767825743....\n",
      "Training loss: 8.047011467337711....\n",
      "Training loss: 8.045452091422577....\n",
      "Training loss: 8.049323354528617....\n",
      "Training loss: 8.048802016936179....\n",
      "Training loss: 8.049496156170468....\n",
      "Training loss: 8.048535162669937....\n",
      "Training loss: 8.04823509294064....\n",
      "Training loss: 8.046167556840702....\n",
      "Training loss: 8.050895014271147....\n",
      "Training loss: 8.051708867380409....\n",
      "Training loss: 8.047046483098057....\n",
      "Training loss: 8.049411582209279....\n",
      "Training loss: 8.049976265024766....\n",
      "Training loss: 8.049549650616113....\n",
      "Training loss: 8.046746742397843....\n",
      "Training loss: 8.04893004981943....\n",
      "Training loss: 8.05113192392911....\n",
      "Training loss: 8.04955899644912....\n",
      "Training loss: 8.046999551069561....\n",
      "Training loss: 8.053635655081507....\n",
      "Training loss: 8.051160284653868....\n",
      "Training loss: 8.046657193726666....\n",
      "Training loss: 8.04425377174483....\n",
      "Training loss: 8.049907619343045....\n",
      "Training loss: 8.051674968250074....\n",
      "Training loss: 8.050981056313262....\n",
      "Training loss: 8.050088035161586....\n",
      "Training loss: 8.049153831580936....\n",
      "Training loss: 8.053249747480448....\n",
      "Training loss: 8.047254639157924....\n",
      "Training loss: 8.04852441368567....\n",
      "Training loss: 8.051987570562202....\n",
      "Training loss: 8.048265408229582....\n",
      "Training loss: 8.04752210633418....\n",
      "Training loss: 8.04770297676327....\n",
      "Training loss: 8.052903612822478....\n",
      "Training loss: 8.047060629283799....\n",
      "Training loss: 8.048145594789535....\n",
      "Training loss: 8.049140661647968....\n",
      "Training loss: 8.051407751179342....\n",
      "Training loss: 8.04665134003878....\n",
      "Training loss: 8.049517294880323....\n",
      "Training loss: 8.04598661381393....\n",
      "Training loss: 8.048749717049423....\n",
      "Training loss: 8.049245738514992....\n",
      "Training loss: 8.050626084945604....\n",
      "Training loss: 8.051817266629714....\n",
      "Training loss: 8.047812240716086....\n",
      "Training loss: 8.047173751137183....\n",
      "Training loss: 8.046976000132148....\n",
      "Training loss: 8.046773595060776....\n",
      "Training loss: 8.051009222550798....\n",
      "Training loss: 8.047981772557163....\n",
      "Training loss: 8.049373760114374....\n",
      "Training loss: 8.050064550255481....\n",
      "Training loss: 8.04845214687372....\n",
      "Training loss: 8.049127822252418....\n",
      "Training loss: 8.048587855052121....\n",
      "Training loss: 8.048624409984782....\n",
      "Training loss: 8.048899164036975....\n",
      "Training loss: 8.052948564826124....\n",
      "Training loss: 8.045682770439175....\n",
      "Training loss: 8.047782108114985....\n",
      "Training loss: 8.04371914752834....\n",
      "Training loss: 8.046864541717612....\n",
      "Training loss: 8.049334270307192....\n",
      "Training loss: 8.049401577782069....\n",
      "Training loss: 8.04442342050376....\n",
      "Training loss: 8.04969701566385....\n",
      "Training loss: 8.046209800966514....\n",
      "Training loss: 8.053129596924194....\n",
      "Training loss: 8.048133817887305....\n",
      "Training loss: 8.0458976696991....\n",
      "Training loss: 8.047462177881728....\n",
      "Training loss: 8.04922092250718....\n",
      "Training loss: 8.049831265821675....\n",
      "Training loss: 8.054003919779154....\n",
      "Training loss: 8.048743533490333....\n",
      "Training loss: 8.05060369797833....\n",
      "Training loss: 8.049167116721787....\n",
      "Training loss: 8.045180075879395....\n",
      "Training loss: 8.047851710778303....\n",
      "Training loss: 8.047220703927746....\n",
      "Training loss: 8.048444720829552....\n",
      "Training loss: 8.049379885395036....\n",
      "Training loss: 8.046743561018165....\n",
      "Training loss: 8.046599776064571....\n",
      "Training loss: 8.046304781095008....\n",
      "Training loss: 8.044928697299119....\n",
      "Training loss: 8.049631891012606....\n",
      "Training loss: 8.05072078740763....\n",
      "Training loss: 8.046845169008177....\n",
      "Training loss: 8.050308771452565....\n",
      "Training loss: 8.043188441401258....\n",
      "Training loss: 8.048173755166596....\n",
      "Training loss: 8.047133881779095....\n",
      "Training loss: 8.049914286452363....\n",
      "Training loss: 8.050590934170359....\n",
      "Training loss: 8.047596500177782....\n",
      "Training loss: 8.05051311022862....\n",
      "Training loss: 8.04858572544644....\n",
      "Training loss: 8.049593759523225....\n",
      "Training loss: 8.049063294991928....\n",
      "Training loss: 8.046853926942513....\n",
      "Training loss: 8.049300395857543....\n",
      "Training loss: 8.048052054677074....\n",
      "Training loss: 8.04569814946594....\n",
      "Training loss: 8.0468727686952....\n",
      "Training loss: 8.045031028481848....\n",
      "Training loss: 8.050612268876952....\n",
      "Training loss: 8.049653249340128....\n",
      "Training loss: 8.04855804816824....\n",
      "Training loss: 8.043248883040595....\n",
      "Training loss: 8.046699001698345....\n",
      "Training loss: 8.04560170334689....\n",
      "Training loss: 8.047875373795547....\n",
      "Training loss: 8.050014873376318....\n",
      "Training loss: 8.050983912076466....\n",
      "Training loss: 8.044850395585334....\n",
      "Training loss: 8.046953522381871....\n",
      "Training loss: 8.045947970745644....\n",
      "Training loss: 8.046597870268943....\n",
      "Training loss: 8.044531031772163....\n",
      "Training loss: 8.047883052318188....\n",
      "Training loss: 8.05031062477286....\n",
      "Training loss: 8.048352595833071....\n",
      "Training loss: 8.047910437926225....\n",
      "Training loss: 8.04649532109933....\n",
      "Training loss: 8.046225270574174....\n",
      "Training loss: 8.046210067887088....\n",
      "Training loss: 8.047004927710457....\n",
      "Training loss: 8.047633526956613....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.043530361460563....\n",
      "Training loss: 8.048401883771149....\n",
      "Training loss: 8.047934623682325....\n",
      "Training loss: 8.046126674290209....\n",
      "Training loss: 8.04820367011913....\n",
      "Training loss: 8.049819851336236....\n",
      "Training loss: 8.047297956596532....\n",
      "Training loss: 8.043294933451229....\n",
      "###########epoch####### 6\n",
      "Training loss: 8.045835926681939....\n",
      "Training loss: 8.045898178367331....\n",
      "Training loss: 8.047204011510258....\n",
      "Training loss: 8.047452129553541....\n",
      "Training loss: 8.048096791827586....\n",
      "Training loss: 8.047807022941505....\n",
      "Training loss: 8.04533779787438....\n",
      "Training loss: 8.046492242993011....\n",
      "Training loss: 8.048320105428806....\n",
      "Training loss: 8.050549650123425....\n",
      "Training loss: 8.048157247240745....\n",
      "Training loss: 8.049925965590921....\n",
      "Training loss: 8.049257928279372....\n",
      "Training loss: 8.046202705658846....\n",
      "Training loss: 8.045111501492174....\n",
      "Training loss: 8.045498465815246....\n",
      "Training loss: 8.045704216457684....\n",
      "Training loss: 8.045738556558122....\n",
      "Training loss: 8.046650954583995....\n",
      "Training loss: 8.047694743149775....\n",
      "Training loss: 8.04605184768326....\n",
      "Training loss: 8.047731345075505....\n",
      "Training loss: 8.045887976878365....\n",
      "Training loss: 8.045183913111588....\n",
      "Training loss: 8.044092756657571....\n",
      "Training loss: 8.048260800359154....\n",
      "Training loss: 8.041734919464748....\n",
      "Training loss: 8.045961954326911....\n",
      "Training loss: 8.046702500738618....\n",
      "Training loss: 8.044596793359032....\n",
      "Training loss: 8.047307800750602....\n",
      "Training loss: 8.048895341522472....\n",
      "Training loss: 8.04742391572224....\n",
      "Training loss: 8.04907805476242....\n",
      "Training loss: 8.047044011358462....\n",
      "Training loss: 8.046451778929454....\n",
      "Training loss: 8.050393375831577....\n",
      "Training loss: 8.046511808969422....\n",
      "Training loss: 8.045791213245984....\n",
      "Training loss: 8.044980521822724....\n",
      "Training loss: 8.04333894545778....\n",
      "Training loss: 8.047304005579837....\n",
      "Training loss: 8.046733934816507....\n",
      "Training loss: 8.047380987737858....\n",
      "Training loss: 8.046482704336194....\n",
      "Training loss: 8.046153082931054....\n",
      "Training loss: 8.044099666219493....\n",
      "Training loss: 8.048893343977818....\n",
      "Training loss: 8.049648801439403....\n",
      "Training loss: 8.044922477403205....\n",
      "Training loss: 8.047454681583867....\n",
      "Training loss: 8.047863188776729....\n",
      "Training loss: 8.047349489636149....\n",
      "Training loss: 8.04479333859199....\n",
      "Training loss: 8.04700119170303....\n",
      "Training loss: 8.049141492632032....\n",
      "Training loss: 8.047697020846288....\n",
      "Training loss: 8.044915932302636....\n",
      "Training loss: 8.051641327348921....\n",
      "Training loss: 8.049081514060473....\n",
      "Training loss: 8.044598550927928....\n",
      "Training loss: 8.042180109161146....\n",
      "Training loss: 8.04778120217538....\n",
      "Training loss: 8.049718753129152....\n",
      "Training loss: 8.048824196910756....\n",
      "Training loss: 8.048127176815658....\n",
      "Training loss: 8.047097419200801....\n",
      "Training loss: 8.05125997671551....\n",
      "Training loss: 8.045185842339002....\n",
      "Training loss: 8.04643272583255....\n",
      "Training loss: 8.050036097568096....\n",
      "Training loss: 8.046088965532514....\n",
      "Training loss: 8.045438064978818....\n",
      "Training loss: 8.045736701155898....\n",
      "Training loss: 8.050901608700595....\n",
      "Training loss: 8.045189815404655....\n",
      "Training loss: 8.046133431173208....\n",
      "Training loss: 8.047033702243496....\n",
      "Training loss: 8.04940528376266....\n",
      "Training loss: 8.044586163441615....\n",
      "Training loss: 8.04750946330415....\n",
      "Training loss: 8.043966053045798....\n",
      "Training loss: 8.046785829506858....\n",
      "Training loss: 8.047170442057734....\n",
      "Training loss: 8.048594869188932....\n",
      "Training loss: 8.049819553085886....\n",
      "Training loss: 8.045718449800367....\n",
      "Training loss: 8.045061153777029....\n",
      "Training loss: 8.044790293225674....\n",
      "Training loss: 8.044746253208434....\n",
      "Training loss: 8.049078220977934....\n",
      "Training loss: 8.0459715278617....\n",
      "Training loss: 8.047412914779722....\n",
      "Training loss: 8.047942919762276....\n",
      "Training loss: 8.046317966839855....\n",
      "Training loss: 8.047142371608087....\n",
      "Training loss: 8.046632829995824....\n",
      "Training loss: 8.046406660709668....\n",
      "Training loss: 8.046847060960813....\n",
      "Training loss: 8.050913115032765....\n",
      "Training loss: 8.043580062533094....\n",
      "Training loss: 8.04573729275853....\n",
      "Training loss: 8.04155488968441....\n",
      "Training loss: 8.044821906166066....\n",
      "Training loss: 8.047222698902743....\n",
      "Training loss: 8.047244884960731....\n",
      "Training loss: 8.042289845050869....\n",
      "Training loss: 8.047756978756324....\n",
      "Training loss: 8.044144745465067....\n",
      "Training loss: 8.051067382608235....\n",
      "Training loss: 8.046095770021301....\n",
      "Training loss: 8.043885112729866....\n",
      "Training loss: 8.04535528155127....\n",
      "Training loss: 8.047168338906099....\n",
      "Training loss: 8.047802555970797....\n",
      "Training loss: 8.052004248219351....\n",
      "Training loss: 8.046726045411873....\n",
      "Training loss: 8.048604517973125....\n",
      "Training loss: 8.047031390444035....\n",
      "Training loss: 8.043050891914204....\n",
      "Training loss: 8.045821381870182....\n",
      "Training loss: 8.045262238279888....\n",
      "Training loss: 8.046460674679....\n",
      "Training loss: 8.047321717487078....\n",
      "Training loss: 8.044644363213829....\n",
      "Training loss: 8.044569849910305....\n",
      "Training loss: 8.04430886156524....\n",
      "Training loss: 8.042891839454676....\n",
      "Training loss: 8.04763829181517....\n",
      "Training loss: 8.048716583645467....\n",
      "Training loss: 8.044837384541406....\n",
      "Training loss: 8.04829612474811....\n",
      "Training loss: 8.041111907574415....\n",
      "Training loss: 8.046099397385662....\n",
      "Training loss: 8.045174135550967....\n",
      "Training loss: 8.047965664148144....\n",
      "Training loss: 8.048659425137517....\n",
      "Training loss: 8.045532267098045....\n",
      "Training loss: 8.048493610082275....\n",
      "Training loss: 8.046503895764186....\n",
      "Training loss: 8.047593056032483....\n",
      "Training loss: 8.047055342666498....\n",
      "Training loss: 8.04486192125897....\n",
      "Training loss: 8.047263540525572....\n",
      "Training loss: 8.046048357050648....\n",
      "Training loss: 8.043660781405162....\n",
      "Training loss: 8.044776194457182....\n",
      "Training loss: 8.0429021472719....\n",
      "Training loss: 8.048571446248165....\n",
      "Training loss: 8.047527289697166....\n",
      "Training loss: 8.046487669003778....\n",
      "Training loss: 8.04122974053102....\n",
      "Training loss: 8.044600061842736....\n",
      "Training loss: 8.043630469317346....\n",
      "Training loss: 8.045861814286935....\n",
      "Training loss: 8.048000899888486....\n",
      "Training loss: 8.048909599600982....\n",
      "Training loss: 8.042819021044336....\n",
      "Training loss: 8.044936368220629....\n",
      "Training loss: 8.043952598940427....\n",
      "Training loss: 8.0445372851393....\n",
      "Training loss: 8.042579243092126....\n",
      "Training loss: 8.045914615213908....\n",
      "Training loss: 8.04832789341071....\n",
      "Training loss: 8.046220913309103....\n",
      "Training loss: 8.045773973484614....\n",
      "Training loss: 8.044401946037631....\n",
      "Training loss: 8.044198142610833....\n",
      "Training loss: 8.044100808890263....\n",
      "Training loss: 8.044950299336685....\n",
      "Training loss: 8.045552222300293....\n",
      "Training loss: 8.041462260494148....\n",
      "Training loss: 8.046410128826228....\n",
      "Training loss: 8.04595490031305....\n",
      "Training loss: 8.044104795731885....\n",
      "Training loss: 8.046180825578688....\n",
      "Training loss: 8.047807312288558....\n",
      "Training loss: 8.045174663109046....\n",
      "Training loss: 8.041330804314276....\n",
      "###########epoch####### 7\n",
      "Training loss: 8.043846691533746....\n",
      "Training loss: 8.043880119906134....\n",
      "Training loss: 8.045110598865332....\n",
      "Training loss: 8.045443975516283....\n",
      "Training loss: 8.046025154640217....\n",
      "Training loss: 8.045736474276593....\n",
      "Training loss: 8.043258434734417....\n",
      "Training loss: 8.044459183748243....\n",
      "Training loss: 8.046379153182412....\n",
      "Training loss: 8.048521186744793....\n",
      "Training loss: 8.046241457034663....\n",
      "Training loss: 8.047988762409043....\n",
      "Training loss: 8.047310764632467....\n",
      "Training loss: 8.04416509770002....\n",
      "Training loss: 8.043029770886621....\n",
      "Training loss: 8.043399671187931....\n",
      "Training loss: 8.04369817868431....\n",
      "Training loss: 8.043693960334652....\n",
      "Training loss: 8.04468502175765....\n",
      "Training loss: 8.045727022627142....\n",
      "Training loss: 8.044054548355975....\n",
      "Training loss: 8.045775528412234....\n",
      "Training loss: 8.043868875781081....\n",
      "Training loss: 8.043010778378708....\n",
      "Training loss: 8.042049590181836....\n",
      "Training loss: 8.046240517033755....\n",
      "Training loss: 8.039605728920412....\n",
      "Training loss: 8.04384273596345....\n",
      "Training loss: 8.044689788988865....\n",
      "Training loss: 8.042512188873804....\n",
      "Training loss: 8.045339855816135....\n",
      "Training loss: 8.046820899447885....\n",
      "Training loss: 8.045383879817228....\n",
      "Training loss: 8.04709072542801....\n",
      "Training loss: 8.044997537235648....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.044326519320437....\n",
      "Training loss: 8.04836551949425....\n",
      "Training loss: 8.044610126874561....\n",
      "Training loss: 8.043800354225354....\n",
      "Training loss: 8.042957728486904....\n",
      "Training loss: 8.041231271140742....\n",
      "Training loss: 8.045299769244398....\n",
      "Training loss: 8.044687942566947....\n",
      "Training loss: 8.045281887796854....\n",
      "Training loss: 8.044451321068703....\n",
      "Training loss: 8.044076307598383....\n",
      "Training loss: 8.042044989068968....\n",
      "Training loss: 8.046894776303956....\n",
      "Training loss: 8.047594196339281....\n",
      "Training loss: 8.042798417368198....\n",
      "Training loss: 8.045508306204376....\n",
      "Training loss: 8.045752272042233....\n",
      "Training loss: 8.045163902892602....\n",
      "Training loss: 8.042838859946007....\n",
      "Training loss: 8.04507826583697....\n",
      "Training loss: 8.047160876389245....\n",
      "Training loss: 8.04583650938552....\n",
      "Training loss: 8.042853505351301....\n",
      "Training loss: 8.049661516890698....\n",
      "Training loss: 8.047009787773018....\n",
      "Training loss: 8.04255189331382....\n",
      "Training loss: 8.040116742107362....\n",
      "Training loss: 8.045671914553122....\n",
      "Training loss: 8.047772305608424....\n",
      "Training loss: 8.0466795572655....\n",
      "Training loss: 8.046171839543023....\n",
      "Training loss: 8.045041950505054....\n",
      "Training loss: 8.049285055571229....\n",
      "Training loss: 8.043121744229651....\n",
      "Training loss: 8.044349924250499....\n",
      "Training loss: 8.048089398447086....\n",
      "Training loss: 8.043917887049439....\n",
      "Training loss: 8.04335407820787....\n",
      "Training loss: 8.04377621213692....\n",
      "Training loss: 8.048902917229999....\n",
      "Training loss: 8.043329387364318....\n",
      "Training loss: 8.044128557247038....\n",
      "Training loss: 8.044924821579427....\n",
      "Training loss: 8.047402418611842....\n",
      "Training loss: 8.04252932049966....\n",
      "Training loss: 8.045510345987093....\n",
      "Training loss: 8.041953689823407....\n",
      "Training loss: 8.044827028276778....\n",
      "Training loss: 8.045099285772817....\n",
      "Training loss: 8.046574159732815....\n",
      "Training loss: 8.047821094389038....\n",
      "Training loss: 8.043627127215842....\n",
      "Training loss: 8.042944944482985....\n",
      "Training loss: 8.04260817722327....\n",
      "Training loss: 8.042732958898275....\n",
      "Training loss: 8.047156647842904....\n",
      "Training loss: 8.043966549958288....\n",
      "Training loss: 8.045452594565829....\n",
      "Training loss: 8.045824423736068....\n",
      "Training loss: 8.044194252052995....\n",
      "Training loss: 8.04516620874306....\n",
      "Training loss: 8.044684819379528....\n",
      "Training loss: 8.044194273478295....\n",
      "Training loss: 8.044805562251213....\n",
      "Training loss: 8.048888975907714....\n",
      "Training loss: 8.041479067045998....\n",
      "Training loss: 8.043696871003055....\n",
      "Training loss: 8.039395821585101....\n",
      "Training loss: 8.042771847089284....\n",
      "Training loss: 8.04511390177899....\n",
      "Training loss: 8.045095165162644....\n",
      "Training loss: 8.040165249549487....\n",
      "Training loss: 8.045817220518398....\n",
      "Training loss: 8.04208662839062....\n",
      "Training loss: 8.049018504195038....\n",
      "Training loss: 8.04406027192545....\n",
      "Training loss: 8.041882391904387....\n",
      "Training loss: 8.043254774714356....\n",
      "Training loss: 8.045123721566988....\n",
      "Training loss: 8.045782874395806....\n",
      "Training loss: 8.050001825974581....\n",
      "Training loss: 8.04471843705729....\n",
      "Training loss: 8.04661251232931....\n",
      "Training loss: 8.044906131584757....\n",
      "Training loss: 8.04092124676683....\n",
      "Training loss: 8.043797733881059....\n",
      "Training loss: 8.043307801150803....\n",
      "Training loss: 8.044479702927173....\n",
      "Training loss: 8.045272524590619....\n",
      "Training loss: 8.042551548848301....\n",
      "Training loss: 8.042545076830121....\n",
      "Training loss: 8.042323792475472....\n",
      "Training loss: 8.04086567323565....\n",
      "Training loss: 8.045657465213974....\n",
      "Training loss: 8.04671455576672....\n",
      "Training loss: 8.042832552758071....\n",
      "Training loss: 8.046296181790455....\n",
      "Training loss: 8.039037223676445....\n",
      "Training loss: 8.044020868949119....\n",
      "Training loss: 8.043213178431403....\n",
      "Training loss: 8.046013883218253....\n",
      "Training loss: 8.046721955183118....\n",
      "Training loss: 8.043466088462267....\n",
      "Training loss: 8.04647150944086....\n",
      "Training loss: 8.044418735170451....\n",
      "Training loss: 8.045598555923137....\n",
      "Training loss: 8.045043568703512....\n",
      "Training loss: 8.042874442530405....\n",
      "Training loss: 8.045224236648563....\n",
      "Training loss: 8.044052313361417....\n",
      "Training loss: 8.041616923920168....\n",
      "Training loss: 8.042684974849792....\n",
      "Training loss: 8.040775116957752....\n",
      "Training loss: 8.046528773947285....\n",
      "Training loss: 8.045410100538572....\n",
      "Training loss: 8.044422699115527....\n",
      "Training loss: 8.039212284494043....\n",
      "Training loss: 8.042513742576858....\n",
      "Training loss: 8.041655336200677....\n",
      "Training loss: 8.043850450529154....\n",
      "Training loss: 8.045998099288314....\n",
      "Training loss: 8.04684872883313....\n",
      "Training loss: 8.040799394633146....\n",
      "Training loss: 8.04292037040574....\n",
      "Training loss: 8.041961999246682....\n",
      "Training loss: 8.04248031459292....\n",
      "Training loss: 8.040631257809922....\n",
      "Training loss: 8.04394053109812....\n",
      "Training loss: 8.04634725813929....\n",
      "Training loss: 8.044098923825066....\n",
      "Training loss: 8.04364897082365....\n",
      "Training loss: 8.042308698013578....\n",
      "Training loss: 8.042179658397425....\n",
      "Training loss: 8.041992657985462....\n",
      "Training loss: 8.042906960712683....\n",
      "Training loss: 8.043472613882717....\n",
      "Training loss: 8.039390088531508....\n",
      "Training loss: 8.044430825899429....\n",
      "Training loss: 8.04396786976541....\n",
      "Training loss: 8.042087564997837....\n",
      "Training loss: 8.044159763608533....\n",
      "Training loss: 8.045790284703902....\n",
      "Training loss: 8.04305382656405....\n",
      "Training loss: 8.039341646865244....\n",
      "###########epoch####### 8\n",
      "Training loss: 8.04186373002857....\n",
      "Training loss: 8.041857343315773....\n",
      "Training loss: 8.043020826227425....\n",
      "Training loss: 8.043436788770084....\n",
      "Training loss: 8.043950368499535....\n",
      "Training loss: 8.043664638667817....\n",
      "Training loss: 8.041181004151385....\n",
      "Training loss: 8.042434826123685....\n",
      "Training loss: 8.044435979983987....\n",
      "Training loss: 8.046491230916855....\n",
      "Training loss: 8.04432921253012....\n",
      "Training loss: 8.04605553827981....\n",
      "Training loss: 8.045369406551057....\n",
      "Training loss: 8.04212700922145....\n",
      "Training loss: 8.0409511757062....\n",
      "Training loss: 8.041308323804376....\n",
      "Training loss: 8.041692599774294....\n",
      "Training loss: 8.0416477420999....\n",
      "Training loss: 8.042725808266312....\n",
      "Training loss: 8.043751814575044....\n",
      "Training loss: 8.042064844619908....\n",
      "Training loss: 8.043821724329442....\n",
      "Training loss: 8.041858730432367....\n",
      "Training loss: 8.040843168482846....\n",
      "Training loss: 8.040010785509544....\n",
      "Training loss: 8.044221879898732....\n",
      "Training loss: 8.037475299915585....\n",
      "Training loss: 8.041729803848037....\n",
      "Training loss: 8.042678166433943....\n",
      "Training loss: 8.040426398967885....\n",
      "Training loss: 8.043374301288665....\n",
      "Training loss: 8.044754462281134....\n",
      "Training loss: 8.043347519582964....\n",
      "Training loss: 8.045110638656135....\n",
      "Training loss: 8.042948555560288....\n",
      "Training loss: 8.042203872798254....\n",
      "Training loss: 8.046358184293904....\n",
      "Training loss: 8.04271147516035....\n",
      "Training loss: 8.041822799205574....\n",
      "Training loss: 8.040938634566764....\n",
      "Training loss: 8.039124005036207....\n",
      "Training loss: 8.043296850572082....\n",
      "Training loss: 8.042653360044469....\n",
      "Training loss: 8.043183000918875....\n",
      "Training loss: 8.042424977237246....\n",
      "Training loss: 8.041999083650545....\n",
      "Training loss: 8.039993449283454....\n",
      "Training loss: 8.04489718814198....\n",
      "Training loss: 8.045536329497358....\n",
      "Training loss: 8.040664550723637....\n",
      "Training loss: 8.043564247680454....\n",
      "Training loss: 8.04363959030275....\n",
      "Training loss: 8.04298652050895....\n",
      "Training loss: 8.040876519009606....\n",
      "Training loss: 8.043161647526873....\n",
      "Training loss: 8.04518303290277....\n",
      "Training loss: 8.043973918944078....\n",
      "Training loss: 8.040798489502917....\n",
      "Training loss: 8.047685437214048....\n",
      "Training loss: 8.044937210033607....\n",
      "Training loss: 8.04051269803095....\n",
      "Training loss: 8.038052842407051....\n",
      "Training loss: 8.043574355728987....\n",
      "Training loss: 8.045828722685194....\n",
      "Training loss: 8.044541381516838....\n",
      "Training loss: 8.044216948936855....\n",
      "Training loss: 8.042984895947399....\n",
      "Training loss: 8.047322505176096....\n",
      "Training loss: 8.041064230399483....\n",
      "Training loss: 8.042271525831268....\n",
      "Training loss: 8.046140438378917....\n",
      "Training loss: 8.04174185872167....\n",
      "Training loss: 8.041266294223629....\n",
      "Training loss: 8.04181505807768....\n",
      "Training loss: 8.04690620199612....\n",
      "Training loss: 8.041469584144206....\n",
      "Training loss: 8.042123992486543....\n",
      "Training loss: 8.042808635017945....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.045403924933222....\n",
      "Training loss: 8.040472155999025....\n",
      "Training loss: 8.043513852710813....\n",
      "Training loss: 8.039944924425553....\n",
      "Training loss: 8.042866247613853....\n",
      "Training loss: 8.043035655570424....\n",
      "Training loss: 8.044561201240683....\n",
      "Training loss: 8.045816397002499....\n",
      "Training loss: 8.041534783434075....\n",
      "Training loss: 8.04082358111584....\n",
      "Training loss: 8.040425855652483....\n",
      "Training loss: 8.040727056354799....\n",
      "Training loss: 8.045236450871045....\n",
      "Training loss: 8.04196643051669....\n",
      "Training loss: 8.043494340709815....\n",
      "Training loss: 8.043700749068405....\n",
      "Training loss: 8.042079595168898....\n",
      "Training loss: 8.043193973710599....\n",
      "Training loss: 8.042737331965526....\n",
      "Training loss: 8.04198426850731....\n",
      "Training loss: 8.042766341854005....\n",
      "Training loss: 8.046866813858824....\n",
      "Training loss: 8.039378928984924....\n",
      "Training loss: 8.041660049857558....\n",
      "Training loss: 8.037240895100707....\n",
      "Training loss: 8.040708173641102....\n",
      "Training loss: 8.043009102291828....\n",
      "Training loss: 8.04293966318043....\n",
      "Training loss: 8.038041804512483....\n",
      "Training loss: 8.043869422088862....\n",
      "Training loss: 8.04003659315712....\n",
      "Training loss: 8.04697625068458....\n",
      "Training loss: 8.042026034458653....\n",
      "Training loss: 8.039884217246536....\n",
      "Training loss: 8.041158064991029....\n",
      "Training loss: 8.043083376653893....\n",
      "Training loss: 8.043769033394806....\n",
      "Training loss: 8.047997203426647....\n",
      "Training loss: 8.04271895550099....\n",
      "Training loss: 8.044624871429962....\n",
      "Training loss: 8.042787623452188....\n",
      "Training loss: 8.038794961149184....\n",
      "Training loss: 8.04178208478615....\n",
      "Training loss: 8.04135850583698....\n",
      "Training loss: 8.042498503387401....\n",
      "Training loss: 8.043222727762203....\n",
      "Training loss: 8.04046489265231....\n",
      "Training loss: 8.040530792437448....\n",
      "Training loss: 8.040335153682314....\n",
      "Training loss: 8.038842838964136....\n",
      "Training loss: 8.043678992505601....\n",
      "Training loss: 8.044707295877089....\n",
      "Training loss: 8.040833890678007....\n",
      "Training loss: 8.044302242151181....\n",
      "Training loss: 8.03697070708535....\n",
      "Training loss: 8.04193825901454....\n",
      "Training loss: 8.041245965417849....\n",
      "Training loss: 8.044055652541651....\n",
      "Training loss: 8.044780258701207....\n",
      "Training loss: 8.041398621549776....\n",
      "Training loss: 8.044445743966154....\n",
      "Training loss: 8.042343876167864....\n",
      "Training loss: 8.043600574645458....\n",
      "Training loss: 8.043024984622772....\n",
      "Training loss: 8.040882961944336....\n",
      "Training loss: 8.043184054791947....\n",
      "Training loss: 8.042055504810302....\n",
      "Training loss: 8.039570219999629....\n",
      "Training loss: 8.040599627724482....\n",
      "Training loss: 8.038645223738689....\n",
      "Training loss: 8.044484522063643....\n",
      "Training loss: 8.043296401817951....\n",
      "Training loss: 8.042358759848929....\n",
      "Training loss: 8.037198378866425....\n",
      "Training loss: 8.040432781429859....\n",
      "Training loss: 8.039685257245692....\n",
      "Training loss: 8.041838588441264....\n",
      "Training loss: 8.044001886989113....\n",
      "Training loss: 8.044791851997859....\n",
      "Training loss: 8.038784169071722....\n",
      "Training loss: 8.040910319865652....\n",
      "Training loss: 8.03997150024266....\n",
      "Training loss: 8.04043047906431....\n",
      "Training loss: 8.038683383544626....\n",
      "Training loss: 8.041961544126483....\n",
      "Training loss: 8.0443648474834....\n",
      "Training loss: 8.041987894430473....\n",
      "Training loss: 8.041529340189687....\n",
      "Training loss: 8.040213269420326....\n",
      "Training loss: 8.040157453168947....\n",
      "Training loss: 8.039891028265119....\n",
      "Training loss: 8.040870129639128....\n",
      "Training loss: 8.041387829226917....\n",
      "Training loss: 8.03732050739551....\n",
      "Training loss: 8.04244694807888....\n",
      "Training loss: 8.04197478846085....\n",
      "Training loss: 8.040070039076202....\n",
      "Training loss: 8.042133722794217....\n",
      "Training loss: 8.043766683191638....\n",
      "Training loss: 8.04093031635003....\n",
      "Training loss: 8.037307357429052....\n",
      "###########epoch####### 9\n",
      "Training loss: 8.039887691749444....\n",
      "Training loss: 8.039826729090466....\n",
      "Training loss: 8.040931552764611....\n",
      "Training loss: 8.041430193600553....\n",
      "Training loss: 8.04186950155166....\n",
      "Training loss: 8.04158904719887....\n",
      "Training loss: 8.039105055820286....\n",
      "Training loss: 8.040410010816835....\n",
      "Training loss: 8.042492294158436....\n",
      "Training loss: 8.044460852078037....\n",
      "Training loss: 8.042423184692217....\n",
      "Training loss: 8.044118323149682....\n",
      "Training loss: 8.043432965850856....\n",
      "Training loss: 8.04008620636213....\n",
      "Training loss: 8.03886857230799....\n",
      "Training loss: 8.039219413249576....\n",
      "Training loss: 8.039683798583239....\n",
      "Training loss: 8.03959947963074....\n",
      "Training loss: 8.040765905999814....\n",
      "Training loss: 8.041768739156627....\n",
      "Training loss: 8.040078605399778....\n",
      "Training loss: 8.041869384315614....\n",
      "Training loss: 8.039845834659463....\n",
      "Training loss: 8.038672530639698....\n",
      "Training loss: 8.03796958948465....\n",
      "Training loss: 8.042205353283977....\n",
      "Training loss: 8.035333669860162....\n",
      "Training loss: 8.039613721950689....\n",
      "Training loss: 8.040667017749074....\n",
      "Training loss: 8.0383353623364....\n",
      "Training loss: 8.04140489027977....\n",
      "Training loss: 8.042687278274984....\n",
      "Training loss: 8.041302864149484....\n",
      "Training loss: 8.04312767985907....\n",
      "Training loss: 8.040891996082964....\n",
      "Training loss: 8.040075997727818....\n",
      "Training loss: 8.04435733536246....\n",
      "Training loss: 8.040811800692094....\n",
      "Training loss: 8.03984925877064....\n",
      "Training loss: 8.038915670758097....\n",
      "Training loss: 8.03701442836831....\n",
      "Training loss: 8.04128684512934....\n",
      "Training loss: 8.04062245914058....\n",
      "Training loss: 8.041079918409828....\n",
      "Training loss: 8.040395983588164....\n",
      "Training loss: 8.039916678356802....\n",
      "Training loss: 8.037943599896929....\n",
      "Training loss: 8.042900523783063....\n",
      "Training loss: 8.043468769766042....\n",
      "Training loss: 8.03852472120051....\n",
      "Training loss: 8.041617455814162....\n",
      "Training loss: 8.041518167390294....\n",
      "Training loss: 8.040817225809384....\n",
      "Training loss: 8.038904058724214....\n",
      "Training loss: 8.041239176568164....\n",
      "Training loss: 8.04319678208083....\n",
      "Training loss: 8.042103684571961....\n",
      "Training loss: 8.038743039253498....\n",
      "Training loss: 8.04570012893754....\n",
      "Training loss: 8.042857985228268....\n",
      "Training loss: 8.03847461253776....\n",
      "Training loss: 8.035980952494251....\n",
      "Training loss: 8.041479690794407....\n",
      "Training loss: 8.043883270429324....\n",
      "Training loss: 8.04239759883173....\n",
      "Training loss: 8.042253214698874....\n",
      "Training loss: 8.04092234942711....\n",
      "Training loss: 8.045361288252193....\n",
      "Training loss: 8.039007362701645....\n",
      "Training loss: 8.0401905739503....\n",
      "Training loss: 8.044184639244815....\n",
      "Training loss: 8.03955143512527....\n",
      "Training loss: 8.039168245711622....\n",
      "Training loss: 8.039846498990837....\n",
      "Training loss: 8.044910525486209....\n",
      "Training loss: 8.03960392065661....\n",
      "Training loss: 8.040115627148978....\n",
      "Training loss: 8.040679134431882....\n",
      "Training loss: 8.043396720669486....\n",
      "Training loss: 8.038410675171695....\n",
      "Training loss: 8.041520007045808....\n",
      "Training loss: 8.037935019301244....\n",
      "Training loss: 8.040886764145593....\n",
      "Training loss: 8.040969581309877....\n",
      "Training loss: 8.042549029886095....\n",
      "Training loss: 8.043795290041732....\n",
      "Training loss: 8.039436358620998....\n",
      "Training loss: 8.038694571127706....\n",
      "Training loss: 8.038237038414124....\n",
      "Training loss: 8.03871750999923....\n",
      "Training loss: 8.043314560508579....\n",
      "Training loss: 8.039961949632607....\n",
      "Training loss: 8.041531962576554....\n",
      "Training loss: 8.041563073321159....\n",
      "Training loss: 8.03995852710149....\n",
      "Training loss: 8.04122458988746....\n",
      "Training loss: 8.040777539738434....\n",
      "Training loss: 8.039771548562411....\n",
      "Training loss: 8.040720916362782....\n",
      "Training loss: 8.044836844621765....\n",
      "Training loss: 8.037264292270077....\n",
      "Training loss: 8.039616261953679....\n",
      "Training loss: 8.035077169831089....\n",
      "Training loss: 8.03863037368331....\n",
      "Training loss: 8.04089639223995....\n",
      "Training loss: 8.040772458121525....\n",
      "Training loss: 8.035913911582254....\n",
      "Training loss: 8.041902844010911....\n",
      "Training loss: 8.037981766780932....\n",
      "Training loss: 8.04493093388678....\n",
      "Training loss: 8.039981934008896....\n",
      "Training loss: 8.037884553072558....\n",
      "Training loss: 8.039053049297886....\n",
      "Training loss: 8.041034294232766....\n",
      "Training loss: 8.041756483692103....\n",
      "Training loss: 8.045980808785712....\n",
      "Training loss: 8.040714843016826....\n",
      "Training loss: 8.0426250453106....\n",
      "Training loss: 8.040670234335767....\n",
      "Training loss: 8.03665758654458....\n",
      "Training loss: 8.039760047949503....\n",
      "Training loss: 8.039417576374964....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.040501667674127....\n",
      "Training loss: 8.041161917519322....\n",
      "Training loss: 8.03837166704315....\n",
      "Training loss: 8.03851100737191....\n",
      "Training loss: 8.03833470606606....\n",
      "Training loss: 8.036815140404954....\n",
      "Training loss: 8.041691281079467....\n",
      "Training loss: 8.042685949382156....\n",
      "Training loss: 8.038829314791087....\n",
      "Training loss: 8.042301433200306....\n",
      "Training loss: 8.034898016247984....\n",
      "Training loss: 8.039839211485551....\n",
      "Training loss: 8.039261401093146....\n",
      "Training loss: 8.042082056968036....\n",
      "Training loss: 8.04282687497242....\n",
      "Training loss: 8.039321557978825....\n",
      "Training loss: 8.042411141330874....\n",
      "Training loss: 8.040261049189722....\n",
      "Training loss: 8.041589524285063....\n",
      "Training loss: 8.040993122697836....\n",
      "Training loss: 8.038878825950423....\n",
      "Training loss: 8.041131582677929....\n",
      "Training loss: 8.040055062774043....\n",
      "Training loss: 8.03750868837339....\n",
      "Training loss: 8.038510786253198....\n",
      "Training loss: 8.036497661139979....\n",
      "Training loss: 8.042432110384757....\n",
      "Training loss: 8.04117720819842....\n",
      "Training loss: 8.040286975424891....\n",
      "Training loss: 8.035178936117521....\n",
      "Training loss: 8.038344887081497....\n",
      "Training loss: 8.037711019674857....\n",
      "Training loss: 8.039820088355683....\n",
      "Training loss: 8.04200219565887....\n",
      "Training loss: 8.042735270036841....\n",
      "Training loss: 8.036766057553104....\n",
      "Training loss: 8.038891841506294....\n",
      "Training loss: 8.03796858997609....\n",
      "Training loss: 8.038367420823935....\n",
      "Training loss: 8.036722749045763....\n",
      "Training loss: 8.039969920728147....\n",
      "Training loss: 8.042372231581881....\n",
      "Training loss: 8.039871808638802....\n",
      "Training loss: 8.039403900116879....\n",
      "Training loss: 8.038106832209664....\n",
      "Training loss: 8.038124901132164....\n",
      "Training loss: 8.037782071720935....\n",
      "Training loss: 8.038830703253721....\n",
      "Training loss: 8.039293035424498....\n",
      "Training loss: 8.035246549384397....\n",
      "Training loss: 8.040448954110447....\n",
      "Training loss: 8.039964045792319....\n",
      "Training loss: 8.038044891465603....\n",
      "Training loss: 8.040090602456749....\n",
      "Training loss: 8.041728787718233....\n",
      "Training loss: 8.03879600948202....\n",
      "Training loss: 8.035269286216828....\n",
      "###########epoch####### 10\n",
      "Training loss: 8.037903933021752....\n",
      "Training loss: 8.03778039574706....\n",
      "Training loss: 8.038837363474176....\n",
      "Training loss: 8.039411090193886....\n",
      "Training loss: 8.039779021321488....\n",
      "Training loss: 8.039506137407157....\n",
      "Training loss: 8.037016060428455....\n",
      "Training loss: 8.038372738208357....\n",
      "Training loss: 8.040539944101388....\n",
      "Training loss: 8.042423840294802....\n",
      "Training loss: 8.040512337821955....\n",
      "Training loss: 8.042167610555325....\n",
      "Training loss: 8.041490940321834....\n",
      "Training loss: 8.038039447273263....\n",
      "Training loss: 8.036770933668679....\n",
      "Training loss: 8.037124868167528....\n",
      "Training loss: 8.037665852436552....\n",
      "Training loss: 8.03754130336907....\n",
      "Training loss: 8.038793720289451....\n",
      "Training loss: 8.039768158903922....\n",
      "Training loss: 8.038082863172995....\n",
      "Training loss: 8.039907978706719....\n",
      "Training loss: 8.037824874169306....\n",
      "Training loss: 8.036489243291987....\n",
      "Training loss: 8.03591869472215....\n",
      "Training loss: 8.040179166372967....\n",
      "Training loss: 8.033174981623704....\n",
      "Training loss: 8.037485339179401....\n",
      "Training loss: 8.038644061185305....\n",
      "Training loss: 8.036227834942903....\n",
      "Training loss: 8.039428416617993....\n",
      "Training loss: 8.040609947204477....\n",
      "Training loss: 8.039242617018076....\n",
      "Training loss: 8.04113060623502....\n",
      "Training loss: 8.038817226760425....\n",
      "Training loss: 8.037931024461537....\n",
      "Training loss: 8.042350589418263....\n",
      "Training loss: 8.03890017536001....\n",
      "Training loss: 8.037867744989889....\n",
      "Training loss: 8.036878483399223....\n",
      "Training loss: 8.034889714695327....\n",
      "Training loss: 8.039261095343397....\n",
      "Training loss: 8.038590154580653....\n",
      "Training loss: 8.03896862217422....\n",
      "Training loss: 8.038356190314719....\n",
      "Training loss: 8.037822028621576....\n",
      "Training loss: 8.035884690384297....\n",
      "Training loss: 8.04089574917....\n",
      "Training loss: 8.041386178848365....\n",
      "Training loss: 8.036372095953702....\n",
      "Training loss: 8.039657152485617....\n",
      "Training loss: 8.039384736491062....\n",
      "Training loss: 8.03864420343347....\n",
      "Training loss: 8.036912875363734....\n",
      "Training loss: 8.039309092675076....\n",
      "Training loss: 8.041200832054134....\n",
      "Training loss: 8.040214121731175....\n",
      "Training loss: 8.036676605030397....\n",
      "Training loss: 8.043698757621724....\n",
      "Training loss: 8.040764101807268....\n",
      "Training loss: 8.036435201035859....\n",
      "Training loss: 8.033896136083628....\n",
      "Training loss: 8.03937576453661....\n",
      "Training loss: 8.04193175280239....\n",
      "Training loss: 8.04024376692719....\n",
      "Training loss: 8.040269791225667....\n",
      "Training loss: 8.038845107674739....\n",
      "Training loss: 8.043389026903046....\n",
      "Training loss: 8.036943574048008....\n",
      "Training loss: 8.038095685412802....\n",
      "Training loss: 8.042213253534486....\n",
      "Training loss: 8.037342241072999....\n",
      "Training loss: 8.037053852805657....\n",
      "Training loss: 8.037867342503173....\n",
      "Training loss: 8.042901633715719....\n",
      "Training loss: 8.03773136212612....\n",
      "Training loss: 8.03809245687153....\n",
      "Training loss: 8.038533207511568....\n",
      "Training loss: 8.041374121052755....\n",
      "Training loss: 8.036339687876536....\n",
      "Training loss: 8.03952247347685....\n",
      "Training loss: 8.035911595665896....\n",
      "Training loss: 8.038892243116818....\n",
      "Training loss: 8.038895601855987....\n",
      "Training loss: 8.040529326065952....\n",
      "Training loss: 8.041755254872678....\n",
      "Training loss: 8.037320122741722....\n",
      "Training loss: 8.0365473777714....\n",
      "Training loss: 8.036034296275057....\n",
      "Training loss: 8.036695251392816....\n",
      "Training loss: 8.041382851100545....\n",
      "Training loss: 8.037949404799429....\n",
      "Training loss: 8.039561895926589....\n",
      "Training loss: 8.039405551993985....\n",
      "Training loss: 8.037823018861024....\n",
      "Training loss: 8.039242385691097....\n",
      "Training loss: 8.038799517404378....\n",
      "Training loss: 8.037550177205377....\n",
      "Training loss: 8.038664849996804....\n",
      "Training loss: 8.042787372837031....\n",
      "Training loss: 8.035137246459918....\n",
      "Training loss: 8.037556657958003....\n",
      "Training loss: 8.032898523820078....\n",
      "Training loss: 8.03652957073081....\n",
      "Training loss: 8.038768228380977....\n",
      "Training loss: 8.038589839332559....\n",
      "Training loss: 8.033770196837258....\n",
      "Training loss: 8.039916662205645....\n",
      "Training loss: 8.03591871975167....\n",
      "Training loss: 8.042875451681722....\n",
      "Training loss: 8.037918436876312....\n",
      "Training loss: 8.035874754290736....\n",
      "Training loss: 8.036930735719805....\n",
      "Training loss: 8.038966720143895....\n",
      "Training loss: 8.039734345496395....\n",
      "Training loss: 8.043947144260512....\n",
      "Training loss: 8.038697200378508....\n",
      "Training loss: 8.040615900104443....\n",
      "Training loss: 8.038538521809901....\n",
      "Training loss: 8.034500537226414....\n",
      "Training loss: 8.03772946183363....\n",
      "Training loss: 8.037471908914345....\n",
      "Training loss: 8.038488010984949....\n",
      "Training loss: 8.03908127932932....\n",
      "Training loss: 8.036269352335408....\n",
      "Training loss: 8.036477543684942....\n",
      "Training loss: 8.036313934612547....\n",
      "Training loss: 8.034777696362594....\n",
      "Training loss: 8.039689578339095....\n",
      "Training loss: 8.04064553111669....\n",
      "Training loss: 8.03681062747083....\n",
      "Training loss: 8.040289835678308....\n",
      "Training loss: 8.032811320824633....\n",
      "Training loss: 8.037722312908297....\n",
      "Training loss: 8.03725832658637....\n",
      "Training loss: 8.040088752741184....\n",
      "Training loss: 8.040846962580115....\n",
      "Training loss: 8.037232497427235....\n",
      "Training loss: 8.04036003163463....\n",
      "Training loss: 8.038166358787466....\n",
      "Training loss: 8.039565522675678....\n",
      "Training loss: 8.038948218705851....\n",
      "Training loss: 8.036852047527224....\n",
      "Training loss: 8.039061832452136....\n",
      "Training loss: 8.038038452772287....\n",
      "Training loss: 8.035425823619878....\n",
      "Training loss: 8.036408789658262....\n",
      "Training loss: 8.034327447960507....\n",
      "Training loss: 8.040362888887916....\n",
      "Training loss: 8.039041363198972....\n",
      "Training loss: 8.038208568423348....\n",
      "Training loss: 8.033139596048526....\n",
      "Training loss: 8.03624246285799....\n",
      "Training loss: 8.035725019506096....\n",
      "Training loss: 8.037789446783487....\n",
      "Training loss: 8.039996707851335....\n",
      "Training loss: 8.040668468749766....\n",
      "Training loss: 8.034739694084267....\n",
      "Training loss: 8.036853190742773....\n",
      "Training loss: 8.03594393455041....\n",
      "Training loss: 8.036292043756086....\n",
      "Training loss: 8.03474639626021....\n",
      "Training loss: 8.037958473084512....\n",
      "Training loss: 8.040365774189542....\n",
      "Training loss: 8.037743307948475....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.037262370571138....\n",
      "Training loss: 8.035985664749452....\n",
      "Training loss: 8.036073740748442....\n",
      "Training loss: 8.035662137230375....\n",
      "Training loss: 8.036778561496487....\n",
      "Training loss: 8.037185686355476....\n",
      "Training loss: 8.033158549709322....\n",
      "Training loss: 8.038433824065008....\n",
      "Training loss: 8.037929132555318....\n",
      "Training loss: 8.036002219544553....\n",
      "Training loss: 8.038034667751766....\n",
      "Training loss: 8.039668270345333....\n",
      "Training loss: 8.036644921595505....\n",
      "Training loss: 8.033209810004397....\n",
      "###########epoch####### 11\n",
      "Training loss: 8.035903780426157....\n",
      "Training loss: 8.035716646246591....\n",
      "Training loss: 8.03673142669316....\n",
      "Training loss: 8.037371887831346....\n",
      "Training loss: 8.037666259772214....\n",
      "Training loss: 8.037408501638101....\n",
      "Training loss: 8.034907488447573....\n",
      "Training loss: 8.03631861435285....\n",
      "Training loss: 8.038568797255104....\n",
      "Training loss: 8.04038115386466....\n",
      "Training loss: 8.038588466724258....\n",
      "Training loss: 8.040193103292268....\n",
      "Training loss: 8.039543970215963....\n",
      "Training loss: 8.035981904446645....\n",
      "Training loss: 8.03465134471534....\n",
      "Training loss: 8.035019108523013....\n",
      "Training loss: 8.035630203224821....\n",
      "Training loss: 8.035464204549568....\n",
      "Training loss: 8.036804664821192....\n",
      "Training loss: 8.037749558007953....\n",
      "Training loss: 8.036072335099265....\n",
      "Training loss: 8.037932099291314....\n",
      "Training loss: 8.03578536521092....\n",
      "Training loss: 8.034292446647804....\n",
      "Training loss: 8.033853119725931....\n",
      "Training loss: 8.038137639583809....\n",
      "Training loss: 8.030994331526145....\n",
      "Training loss: 8.035340235027807....\n",
      "Training loss: 8.036612090775442....\n",
      "Training loss: 8.03409822635317....\n",
      "Training loss: 8.037439939535398....\n",
      "Training loss: 8.038513948178167....\n",
      "Training loss: 8.037165714737743....\n",
      "Training loss: 8.039117069212786....\n",
      "Training loss: 8.036722116190347....\n",
      "Training loss: 8.035767007316826....\n",
      "Training loss: 8.04033365159833....\n",
      "Training loss: 8.036973936078155....\n",
      "Training loss: 8.03587047667303....\n",
      "Training loss: 8.034828521822282....\n",
      "Training loss: 8.032746287518027....\n",
      "Training loss: 8.037221672802707....\n",
      "Training loss: 8.036551625082831....\n",
      "Training loss: 8.036843249729007....\n",
      "Training loss: 8.036300820869075....\n",
      "Training loss: 8.035712432644408....\n",
      "Training loss: 8.03380996337629....\n",
      "Training loss: 8.038877596534125....\n",
      "Training loss: 8.039283952714525....\n",
      "Training loss: 8.034201605171676....\n",
      "Training loss: 8.037681564086386....\n",
      "Training loss: 8.037237364730048....\n",
      "Training loss: 8.036458818469479....\n",
      "Training loss: 8.034897606785183....\n",
      "Training loss: 8.037362617075305....\n",
      "Training loss: 8.039185541508724....\n",
      "Training loss: 8.03830801507024....\n",
      "Training loss: 8.034599209793555....\n",
      "Training loss: 8.04167777860553....\n",
      "Training loss: 8.038648815977837....\n",
      "Training loss: 8.034382075150326....\n",
      "Training loss: 8.03179678053949....\n",
      "Training loss: 8.03726128703519....\n",
      "Training loss: 8.039975344949942....\n",
      "Training loss: 8.03807637062174....\n",
      "Training loss: 8.038265734868846....\n",
      "Training loss: 8.036748951780355....\n",
      "Training loss: 8.041403216879967....\n",
      "Training loss: 8.034867118687993....\n",
      "Training loss: 8.03598926045474....\n",
      "Training loss: 8.040224408787756....\n",
      "Training loss: 8.035109412525024....\n",
      "Training loss: 8.034920372082013....\n",
      "Training loss: 8.03587411296326....\n",
      "Training loss: 8.040875263634911....\n",
      "Training loss: 8.035852248512649....\n",
      "Training loss: 8.036054410372763....\n",
      "Training loss: 8.036368667574907....\n",
      "Training loss: 8.039337522289824....\n",
      "Training loss: 8.03425615390014....\n",
      "Training loss: 8.037512123621084....\n",
      "Training loss: 8.033869902083003....\n",
      "Training loss: 8.036882095914207....\n",
      "Training loss: 8.036808876757458....\n",
      "Training loss: 8.038494885916178....\n",
      "Training loss: 8.039693257049974....\n",
      "Training loss: 8.0351811149155....\n",
      "Training loss: 8.034380535847859....\n",
      "Training loss: 8.033818218361253....\n",
      "Training loss: 8.034654248571377....\n",
      "Training loss: 8.039435060592716....\n",
      "Training loss: 8.03592664989523....\n",
      "Training loss: 8.037582100250404....\n",
      "Training loss: 8.037227757770808....\n",
      "Training loss: 8.035675048124892....\n",
      "Training loss: 8.037244893770875....\n",
      "Training loss: 8.03680765171984....\n",
      "Training loss: 8.035315255491248....\n",
      "Training loss: 8.036595558554042....\n",
      "Training loss: 8.040720473520663....\n",
      "Training loss: 8.032992392896292....\n",
      "Training loss: 8.035481037422704....\n",
      "Training loss: 8.030702453207788....\n",
      "Training loss: 8.034401359926195....\n",
      "Training loss: 8.036626245618713....\n",
      "Training loss: 8.036390728872783....\n",
      "Training loss: 8.031610789348226....\n",
      "Training loss: 8.037910392542662....\n",
      "Training loss: 8.033835366067356....\n",
      "Training loss: 8.040809133534419....\n",
      "Training loss: 8.035838580760647....\n",
      "Training loss: 8.033852961790373....\n",
      "Training loss: 8.034797309828324....\n",
      "Training loss: 8.036878954101068....\n",
      "Training loss: 8.037701834772191....\n",
      "Training loss: 8.041898595976964....\n",
      "Training loss: 8.036663123712705....\n",
      "Training loss: 8.038594553723497....\n",
      "Training loss: 8.036395094614937....\n",
      "Training loss: 8.032322215744744....\n",
      "Training loss: 8.035686885707285....\n",
      "Training loss: 8.035518294364392....\n",
      "Training loss: 8.03645449468982....\n",
      "Training loss: 8.036985507614284....\n",
      "Training loss: 8.034152928526568....\n",
      "Training loss: 8.034433652579727....\n",
      "Training loss: 8.034274481974991....\n",
      "Training loss: 8.032727209723644....\n",
      "Training loss: 8.037668162510453....\n",
      "Training loss: 8.038591554671225....\n",
      "Training loss: 8.034777762767728....\n",
      "Training loss: 8.03826177240714....\n",
      "Training loss: 8.030712329727843....\n",
      "Training loss: 8.035584114276576....\n",
      "Training loss: 8.03523562477213....\n",
      "Training loss: 8.038073881188495....\n",
      "Training loss: 8.03884437833202....\n",
      "Training loss: 8.035129493283236....\n",
      "Training loss: 8.03829045310014....\n",
      "Training loss: 8.036056475206761....\n",
      "Training loss: 8.037527366277555....\n",
      "Training loss: 8.036884465178717....\n",
      "Training loss: 8.03480485273398....\n",
      "Training loss: 8.036976378783427....\n",
      "Training loss: 8.036006130840985....\n",
      "Training loss: 8.033320805537864....\n",
      "Training loss: 8.0342961254002....\n",
      "Training loss: 8.032135418855498....\n",
      "Training loss: 8.03827669746507....\n",
      "Training loss: 8.036885428026155....\n",
      "Training loss: 8.03612233270741....\n",
      "Training loss: 8.031082519972063....\n",
      "Training loss: 8.034120070602382....\n",
      "Training loss: 8.03372521832009....\n",
      "Training loss: 8.035745515693055....\n",
      "Training loss: 8.03797329616908....\n",
      "Training loss: 8.038585015542717....\n",
      "Training loss: 8.032701578839637....\n",
      "Training loss: 8.034795816625476....\n",
      "Training loss: 8.033902565807523....\n",
      "Training loss: 8.034195937232457....\n",
      "Training loss: 8.032750944367804....\n",
      "Training loss: 8.03592936312108....\n",
      "Training loss: 8.038342981793877....\n",
      "Training loss: 8.035600209850694....\n",
      "Training loss: 8.035106615830733....\n",
      "Training loss: 8.03385220977653....\n",
      "Training loss: 8.034006410017833....\n",
      "Training loss: 8.033522685873201....\n",
      "Training loss: 8.034706209876989....\n",
      "Training loss: 8.035060191786977....\n",
      "Training loss: 8.03105901495774....\n",
      "Training loss: 8.036397613145589....\n",
      "Training loss: 8.035874073553142....\n",
      "Training loss: 8.033944768327942....\n",
      "Training loss: 8.035963614613742....\n",
      "Training loss: 8.037584048128647....\n",
      "Training loss: 8.034474083411025....\n",
      "Training loss: 8.031103200774096....\n",
      "###########epoch####### 12\n",
      "Training loss: 8.033892356329009....\n",
      "Training loss: 8.033632083139507....\n",
      "Training loss: 8.034610072905558....\n",
      "Training loss: 8.035311731411632....\n",
      "Training loss: 8.035534718094313....\n",
      "Training loss: 8.035296495049822....\n",
      "Training loss: 8.032777521520838....\n",
      "Training loss: 8.034245816622454....\n",
      "Training loss: 8.036576280230616....\n",
      "Training loss: 8.038323633135334....\n",
      "Training loss: 8.036647213907681....\n",
      "Training loss: 8.038195091657867....\n",
      "Training loss: 8.037578511418822....\n",
      "Training loss: 8.033906311122815....\n",
      "Training loss: 8.032510991014084....\n",
      "Training loss: 8.032892688637963....\n",
      "Training loss: 8.033575372718238....\n",
      "Training loss: 8.033370024210218....\n",
      "Training loss: 8.034792087971525....\n",
      "Training loss: 8.035704297477158....\n",
      "Training loss: 8.034048392701932....\n",
      "Training loss: 8.035942102534362....\n",
      "Training loss: 8.03372760060804....\n",
      "Training loss: 8.032084975629724....\n",
      "Training loss: 8.031768821567924....\n",
      "Training loss: 8.036077795933101....\n",
      "Training loss: 8.028794111821892....\n",
      "Training loss: 8.033175328398574....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.034563676060976....\n",
      "Training loss: 8.031945054487123....\n",
      "Training loss: 8.035428702640843....\n",
      "Training loss: 8.036395040321013....\n",
      "Training loss: 8.035071698464401....\n",
      "Training loss: 8.037080815682462....\n",
      "Training loss: 8.034604710274614....\n",
      "Training loss: 8.033579366785602....\n",
      "Training loss: 8.038307141300516....\n",
      "Training loss: 8.035033237857892....\n",
      "Training loss: 8.033856232330217....\n",
      "Training loss: 8.032761707591....\n",
      "Training loss: 8.030582242348954....\n",
      "Training loss: 8.035163569325679....\n",
      "Training loss: 8.034499086818808....\n",
      "Training loss: 8.034697077216018....\n",
      "Training loss: 8.034223106010371....\n",
      "Training loss: 8.033581777009337....\n",
      "Training loss: 8.031718396423235....\n",
      "Training loss: 8.036839269224378....\n",
      "Training loss: 8.037162690161978....\n",
      "Training loss: 8.032010787441592....\n",
      "Training loss: 8.035688827358385....\n",
      "Training loss: 8.035073722720352....\n",
      "Training loss: 8.03426092493221....\n",
      "Training loss: 8.032856540116676....\n",
      "Training loss: 8.035395492747023....\n",
      "Training loss: 8.03714912556957....\n",
      "Training loss: 8.036378486110364....\n",
      "Training loss: 8.032503509685098....\n",
      "Training loss: 8.039635797404301....\n",
      "Training loss: 8.036515549366788....\n",
      "Training loss: 8.032308941902784....\n",
      "Training loss: 8.029677504488113....\n",
      "Training loss: 8.035133342679094....\n",
      "Training loss: 8.038013169663387....\n",
      "Training loss: 8.035894269761098....\n",
      "Training loss: 8.036245257609503....\n",
      "Training loss: 8.034630062544334....\n",
      "Training loss: 8.039400698269166....\n",
      "Training loss: 8.032775753130885....\n",
      "Training loss: 8.03386775897337....\n",
      "Training loss: 8.038212278759891....\n",
      "Training loss: 8.032848649159963....\n",
      "Training loss: 8.032766311781765....\n",
      "Training loss: 8.033862262072986....\n",
      "Training loss: 8.038827242894786....\n",
      "Training loss: 8.033957362561257....\n",
      "Training loss: 8.033993710596425....\n",
      "Training loss: 8.034181741566034....\n",
      "Training loss: 8.037281548331856....\n",
      "Training loss: 8.032149743804746....\n",
      "Training loss: 8.03548201316707....\n",
      "Training loss: 8.031811937540441....\n",
      "Training loss: 8.034847478917243....\n",
      "Training loss: 8.034706291832622....\n",
      "Training loss: 8.036444946483998....\n",
      "Training loss: 8.037604399701772....\n",
      "Training loss: 8.033015219291489....\n",
      "Training loss: 8.032192462304378....\n",
      "Training loss: 8.031578653111731....\n",
      "Training loss: 8.032595375743414....\n",
      "Training loss: 8.0374641488485....\n",
      "Training loss: 8.033882419986952....\n",
      "Training loss: 8.03558215251354....\n",
      "Training loss: 8.035025054689863....\n",
      "Training loss: 8.033502677756273....\n",
      "Training loss: 8.035230109895544....\n",
      "Training loss: 8.034796143668203....\n",
      "Training loss: 8.03306362737973....\n",
      "Training loss: 8.034505149754478....\n",
      "Training loss: 8.03862945201798....\n",
      "Training loss: 8.030823701072213....\n",
      "Training loss: 8.033388391815993....\n",
      "Training loss: 8.028484233657236....\n",
      "Training loss: 8.032246710818576....\n",
      "Training loss: 8.034463504423307....\n",
      "Training loss: 8.034168303930226....\n",
      "Training loss: 8.029429333772153....\n",
      "Training loss: 8.035878691735208....\n",
      "Training loss: 8.031721759976792....\n",
      "Training loss: 8.038720550369709....\n",
      "Training loss: 8.033740390643283....\n",
      "Training loss: 8.031813801790937....\n",
      "Training loss: 8.03263759446782....\n",
      "Training loss: 8.034772062408742....\n",
      "Training loss: 8.035655999717822....\n",
      "Training loss: 8.039826658622292....\n",
      "Training loss: 8.03460798754918....\n",
      "Training loss: 8.036551561943481....\n",
      "Training loss: 8.034231443016237....\n",
      "Training loss: 8.030115759162353....\n",
      "Training loss: 8.033622232125891....\n",
      "Training loss: 8.033548532460278....\n",
      "Training loss: 8.03439565694428....\n",
      "Training loss: 8.034868801041492....\n",
      "Training loss: 8.032015564070235....\n",
      "Training loss: 8.03237083377082....\n",
      "Training loss: 8.032214337332723....\n",
      "Training loss: 8.030659430593442....\n",
      "Training loss: 8.035626070971324....\n",
      "Training loss: 8.036516869874154....\n",
      "Training loss: 8.032725471861381....\n",
      "Training loss: 8.036214245016552....\n",
      "Training loss: 8.028594383049853....\n",
      "Training loss: 8.033423145043628....\n",
      "Training loss: 8.033184211720174....\n",
      "Training loss: 8.036033484175006....\n",
      "Training loss: 8.036816432102421....\n",
      "Training loss: 8.033004965730273....\n",
      "Training loss: 8.03619694955524....\n",
      "Training loss: 8.033927475608337....\n",
      "Training loss: 8.03547190791076....\n",
      "Training loss: 8.034794102991759....\n",
      "Training loss: 8.03273450167106....\n",
      "Training loss: 8.034871367785975....\n",
      "Training loss: 8.033954562162407....\n",
      "Training loss: 8.031192928888462....\n",
      "Training loss: 8.032162160853158....\n",
      "Training loss: 8.029912082253228....\n",
      "Training loss: 8.036173740148584....\n",
      "Training loss: 8.034708186447679....\n",
      "Training loss: 8.034019364090867....\n",
      "Training loss: 8.02900424337654....\n",
      "Training loss: 8.031972337102456....\n",
      "Training loss: 8.031700919490081....\n",
      "Training loss: 8.03368099904933....\n",
      "Training loss: 8.035918813157163....\n",
      "Training loss: 8.036479514865572....\n",
      "Training loss: 8.030647742779394....\n",
      "Training loss: 8.032719481346808....\n",
      "Training loss: 8.03183796255217....\n",
      "Training loss: 8.032074982369354....\n",
      "Training loss: 8.030730814331589....\n",
      "Training loss: 8.033875863188113....\n",
      "Training loss: 8.036299962310162....\n",
      "Training loss: 8.033442850178002....\n",
      "Training loss: 8.032931746856905....\n",
      "Training loss: 8.031691534904462....\n",
      "Training loss: 8.031918929130013....\n",
      "Training loss: 8.031364625121048....\n",
      "Training loss: 8.032615111551996....\n",
      "Training loss: 8.032906211737993....\n",
      "Training loss: 8.028939528822796....\n",
      "Training loss: 8.034335792666303....\n",
      "Training loss: 8.033791370597463....\n",
      "Training loss: 8.031862805244392....\n",
      "Training loss: 8.033867127412801....\n",
      "Training loss: 8.035470770260924....\n",
      "Training loss: 8.032279647964353....\n",
      "Training loss: 8.028977121981528....\n",
      "###########epoch####### 13\n",
      "Training loss: 8.031862714805982....\n",
      "Training loss: 8.031521130865428....\n",
      "Training loss: 8.032468158494916....\n",
      "Training loss: 8.033229922485054....\n",
      "Training loss: 8.033385250458805....\n",
      "Training loss: 8.03315938702001....\n",
      "Training loss: 8.030626055188739....\n",
      "Training loss: 8.032149080246466....\n",
      "Training loss: 8.034558992560196....\n",
      "Training loss: 8.036243303284724....\n",
      "Training loss: 8.034688278964982....\n",
      "Training loss: 8.036176470158697....\n",
      "Training loss: 8.035588401239858....\n",
      "Training loss: 8.031812596674623....\n",
      "Training loss: 8.030346363281271....\n",
      "Training loss: 8.03074414880015....\n",
      "Training loss: 8.031498150176317....\n",
      "Training loss: 8.031250696185337....\n",
      "Training loss: 8.0327595179656....\n",
      "Training loss: 8.033634873395197....\n",
      "Training loss: 8.032002067255444....\n",
      "Training loss: 8.033933467194922....\n",
      "Training loss: 8.031645119807036....\n",
      "Training loss: 8.029854978298099....\n",
      "Training loss: 8.029662124516678....\n",
      "Training loss: 8.033998141833553....\n",
      "Training loss: 8.026571504945407....\n",
      "Training loss: 8.030984596758055....\n",
      "Training loss: 8.032490446948108....\n",
      "Training loss: 8.029762312070654....\n",
      "Training loss: 8.033392217947572....\n",
      "Training loss: 8.034245241419047....\n",
      "Training loss: 8.032951835791716....\n",
      "Training loss: 8.035020613997096....\n",
      "Training loss: 8.032460279886815....\n",
      "Training loss: 8.031362483011764....\n",
      "Training loss: 8.03625942473079....\n",
      "Training loss: 8.033074930357026....\n",
      "Training loss: 8.031826019482551....\n",
      "Training loss: 8.030669752283968....\n",
      "Training loss: 8.028395083529192....\n",
      "Training loss: 8.033081639316906....\n",
      "Training loss: 8.032424996452669....\n",
      "Training loss: 8.032526626261763....\n",
      "Training loss: 8.032122236855102....\n",
      "Training loss: 8.031429677633673....\n",
      "Training loss: 8.029601593119743....\n",
      "Training loss: 8.034780475977495....\n",
      "Training loss: 8.035020927412042....\n",
      "Training loss: 8.029798474864512....\n",
      "Training loss: 8.033674112329619....\n",
      "Training loss: 8.032884625066334....\n",
      "Training loss: 8.032041787542626....\n",
      "Training loss: 8.030787598418323....\n",
      "Training loss: 8.033403756433266....\n",
      "Training loss: 8.035089018555972....\n",
      "Training loss: 8.03442435648701....\n",
      "Training loss: 8.030385657553222....\n",
      "Training loss: 8.037571355585577....\n",
      "Training loss: 8.034353398815336....\n",
      "Training loss: 8.030215440269739....\n",
      "Training loss: 8.027535204428563....\n",
      "Training loss: 8.032986002549102....\n",
      "Training loss: 8.036037806159422....\n",
      "Training loss: 8.033686874610826....\n",
      "Training loss: 8.034196035904127....\n",
      "Training loss: 8.03248509392557....\n",
      "Training loss: 8.037378335067771....\n",
      "Training loss: 8.030665078855193....\n",
      "Training loss: 8.031718187242605....\n",
      "Training loss: 8.036177893101872....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.030563947796017....\n",
      "Training loss: 8.030586597749961....\n",
      "Training loss: 8.031827655027287....\n",
      "Training loss: 8.036749329848794....\n",
      "Training loss: 8.032040537982052....\n",
      "Training loss: 8.03190779921672....\n",
      "Training loss: 8.031963403004818....\n",
      "Training loss: 8.03520292488548....\n",
      "Training loss: 8.03002368015977....\n",
      "Training loss: 8.033426395305105....\n",
      "Training loss: 8.02973221489679....\n",
      "Training loss: 8.032788372177892....\n",
      "Training loss: 8.032582369462473....\n",
      "Training loss: 8.03437453528633....\n",
      "Training loss: 8.035490427667163....\n",
      "Training loss: 8.03081584174988....\n",
      "Training loss: 8.029976521462213....\n",
      "Training loss: 8.02931274967617....\n",
      "Training loss: 8.030516316047885....\n",
      "Training loss: 8.035466500642334....\n",
      "Training loss: 8.03180941830407....\n",
      "Training loss: 8.033559603446077....\n",
      "Training loss: 8.03279370360825....\n",
      "Training loss: 8.03130312342904....\n",
      "Training loss: 8.033188827133909....\n",
      "Training loss: 8.032756657113394....\n",
      "Training loss: 8.030789227678714....\n",
      "Training loss: 8.032384303551899....\n",
      "Training loss: 8.036509465259464....\n",
      "Training loss: 8.028630354656727....\n",
      "Training loss: 8.0312697221898....\n",
      "Training loss: 8.026244329217457....\n",
      "Training loss: 8.030068050394432....\n",
      "Training loss: 8.032271906503777....\n",
      "Training loss: 8.031921622633682....\n",
      "Training loss: 8.027221591681785....\n",
      "Training loss: 8.03381457724763....\n",
      "Training loss: 8.029580132379667....\n",
      "Training loss: 8.036605103604035....\n",
      "Training loss: 8.031617500230187....\n",
      "Training loss: 8.029750333559926....\n",
      "Training loss: 8.030450348732744....\n",
      "Training loss: 8.032639244874535....\n",
      "Training loss: 8.033590143916918....\n",
      "Training loss: 8.037730118616487....\n",
      "Training loss: 8.032527006164006....\n",
      "Training loss: 8.034480731496554....\n",
      "Training loss: 8.03204065795037....\n",
      "Training loss: 8.02787695673294....\n",
      "Training loss: 8.031535728963423....\n",
      "Training loss: 8.031553348937999....\n",
      "Training loss: 8.032303772683262....\n",
      "Training loss: 8.032727445084127....\n",
      "Training loss: 8.029853451568371....\n",
      "Training loss: 8.030285049296475....\n",
      "Training loss: 8.030123050588115....\n",
      "Training loss: 8.028568588062413....\n",
      "Training loss: 8.03355659657118....\n",
      "Training loss: 8.03441028480845....\n",
      "Training loss: 8.030646187664976....\n",
      "Training loss: 8.034142508349001....\n",
      "Training loss: 8.02644740306503....\n",
      "Training loss: 8.031228024632464....\n",
      "Training loss: 8.031100136657772....\n",
      "Training loss: 8.033956917030046....\n",
      "Training loss: 8.03475732844796....\n",
      "Training loss: 8.030852351165427....\n",
      "Training loss: 8.03407397639401....\n",
      "Training loss: 8.031771001086895....\n",
      "Training loss: 8.033393291809013....\n",
      "Training loss: 8.032675790181182....\n",
      "Training loss: 8.030639392412526....\n",
      "Training loss: 8.032737429962761....\n",
      "Training loss: 8.031877254523575....\n",
      "Training loss: 8.029035314359923....\n",
      "Training loss: 8.029992943610944....\n",
      "Training loss: 8.027652489059934....\n",
      "Training loss: 8.034042793081893....\n",
      "Training loss: 8.032503612960575....\n",
      "Training loss: 8.031890556493416....\n",
      "Training loss: 8.0269004898221....\n",
      "Training loss: 8.029790833574847....\n",
      "Training loss: 8.029648092046516....\n",
      "Training loss: 8.031594570327858....\n",
      "Training loss: 8.033832116177376....\n",
      "Training loss: 8.034349742488653....\n",
      "Training loss: 8.028567521520717....\n",
      "Training loss: 8.0306153217777....\n",
      "Training loss: 8.02974324079825....\n",
      "Training loss: 8.029923385418387....\n",
      "Training loss: 8.028679521909577....\n",
      "Training loss: 8.031798387638759....\n",
      "Training loss: 8.034230236983655....\n",
      "Training loss: 8.03126048188716....\n",
      "Training loss: 8.030730905545283....\n",
      "Training loss: 8.02949910486429....\n",
      "Training loss: 8.029798258831933....\n",
      "Training loss: 8.029181347573482....\n",
      "Training loss: 8.030493251977255....\n",
      "Training loss: 8.030718965015254....\n",
      "Training loss: 8.026786825319066....\n",
      "Training loss: 8.032241474502472....\n",
      "Training loss: 8.031676323317328....\n",
      "Training loss: 8.029753327536218....\n",
      "Training loss: 8.031744798924503....\n",
      "Training loss: 8.033324268732411....\n",
      "Training loss: 8.030057572812305....\n",
      "Training loss: 8.026821042500012....\n",
      "###########epoch####### 14\n",
      "Training loss: 8.029801227083636....\n",
      "Training loss: 8.029377090114046....\n",
      "Training loss: 8.030299333078224....\n",
      "Training loss: 8.031119370415505....\n",
      "Training loss: 8.031206556271954....\n",
      "Training loss: 8.030990661789254....\n",
      "Training loss: 8.028442690440706....\n",
      "Training loss: 8.030027168739016....\n",
      "Training loss: 8.03251598063553....\n",
      "Training loss: 8.03413311319727....\n",
      "Training loss: 8.032700540607552....\n",
      "Training loss: 8.034125457205642....\n",
      "Training loss: 8.033568988879422....\n",
      "Training loss: 8.02969205792034....\n",
      "Training loss: 8.028151296238896....\n",
      "Training loss: 8.028570211343789....\n",
      "Training loss: 8.0293950862102....\n",
      "Training loss: 8.029100591611101....\n",
      "Training loss: 8.030702729109352....\n",
      "Training loss: 8.031539134215357....\n",
      "Training loss: 8.029925655083806....\n",
      "Training loss: 8.031901073745066....\n",
      "Training loss: 8.02952712936843....\n",
      "Training loss: 8.027594495608653....\n",
      "Training loss: 8.027526195277465....\n",
      "Training loss: 8.031889198266645....\n",
      "Training loss: 8.024318126255007....\n",
      "Training loss: 8.028760242576693....\n",
      "Training loss: 8.030387855252696....\n",
      "Training loss: 8.027546662239885....\n",
      "Training loss: 8.031328571807377....\n",
      "Training loss: 8.032059018554188....\n",
      "Training loss: 8.030801306384644....\n",
      "Training loss: 8.032928327763699....\n",
      "Training loss: 8.03028283638329....\n",
      "Training loss: 8.029109163975898....\n",
      "Training loss: 8.03418442267694....\n",
      "Training loss: 8.031090304560912....\n",
      "Training loss: 8.02976943628842....\n",
      "Training loss: 8.028541801430567....\n",
      "Training loss: 8.0261761540694....\n",
      "Training loss: 8.030971875763122....\n",
      "Training loss: 8.030321784940917....\n",
      "Training loss: 8.030328977177259....\n",
      "Training loss: 8.029991255470694....\n",
      "Training loss: 8.029251148009362....\n",
      "Training loss: 8.027451420682555....\n",
      "Training loss: 8.03269071711907....\n",
      "Training loss: 8.032851691512944....\n",
      "Training loss: 8.027553075684047....\n",
      "Training loss: 8.031629582830886....\n",
      "Training loss: 8.030665143584637....\n",
      "Training loss: 8.02978983764755....\n",
      "Training loss: 8.028682977637247....\n",
      "Training loss: 8.031379508308456....\n",
      "Training loss: 8.032998619234222....\n",
      "Training loss: 8.032439903920121....\n",
      "Training loss: 8.028239771835....\n",
      "Training loss: 8.035474958352404....\n",
      "Training loss: 8.032158174766248....\n",
      "Training loss: 8.028087358001306....\n",
      "Training loss: 8.025361044249946....\n",
      "Training loss: 8.030811515636618....\n",
      "Training loss: 8.034036703887546....\n",
      "Training loss: 8.031447984944736....\n",
      "Training loss: 8.032111239975578....\n",
      "Training loss: 8.030307814656501....\n",
      "Training loss: 8.035330517784784....\n",
      "Training loss: 8.02852482970403....\n",
      "Training loss: 8.029532138587873....\n",
      "Training loss: 8.034116856929845....\n",
      "Training loss: 8.028250310127293....\n",
      "Training loss: 8.028373977999447....\n",
      "Training loss: 8.029759816763352....\n",
      "Training loss: 8.03463836922172....\n",
      "Training loss: 8.03009568007493....\n",
      "Training loss: 8.029790250011047....\n",
      "Training loss: 8.02971013618547....\n",
      "Training loss: 8.033093605568888....\n",
      "Training loss: 8.027870122796813....\n",
      "Training loss: 8.031344697650212....\n",
      "Training loss: 8.027625080546438....\n",
      "Training loss: 8.030697749985007....\n",
      "Training loss: 8.030428582209552....\n",
      "Training loss: 8.03227475222417....\n",
      "Training loss: 8.033347824660945....\n",
      "Training loss: 8.028576563981193....\n",
      "Training loss: 8.027727101911536....\n",
      "Training loss: 8.027015685107896....\n",
      "Training loss: 8.028409329787635....\n",
      "Training loss: 8.033439327209669....\n",
      "Training loss: 8.029706310024821....\n",
      "Training loss: 8.031507619531657....\n",
      "Training loss: 8.030529813664415....\n",
      "Training loss: 8.029070868131443....\n",
      "Training loss: 8.031113040155743....\n",
      "Training loss: 8.03068344066297....\n",
      "Training loss: 8.028486803424945....\n",
      "Training loss: 8.030231476434976....\n",
      "Training loss: 8.034355476057733....\n",
      "Training loss: 8.026399005434458....\n",
      "Training loss: 8.02912044976776....\n",
      "Training loss: 8.023974203361021....\n",
      "Training loss: 8.027857567393864....\n",
      "Training loss: 8.030048983072575....\n",
      "Training loss: 8.029644608654658....\n",
      "Training loss: 8.024979714838997....\n",
      "Training loss: 8.031719536233743....\n",
      "Training loss: 8.027405438567861....\n",
      "Training loss: 8.034457747511336....\n",
      "Training loss: 8.029463026200792....\n",
      "Training loss: 8.02765632471206....\n",
      "Training loss: 8.028229687631661....\n",
      "Training loss: 8.030479481297867....\n",
      "Training loss: 8.031493286135431....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.035602822930336....\n",
      "Training loss: 8.030416482002567....\n",
      "Training loss: 8.032377816310154....\n",
      "Training loss: 8.029820431178626....\n",
      "Training loss: 8.025604545948978....\n",
      "Training loss: 8.029421515957637....\n",
      "Training loss: 8.029537401374887....\n",
      "Training loss: 8.030171926475212....\n",
      "Training loss: 8.030553170342051....\n",
      "Training loss: 8.027655251026866....\n",
      "Training loss: 8.02816771359852....\n",
      "Training loss: 8.02799655652368....\n",
      "Training loss: 8.026444111627256....\n",
      "Training loss: 8.03145272404185....\n",
      "Training loss: 8.032271709219865....\n",
      "Training loss: 8.028538239073768....\n",
      "Training loss: 8.032035816427529....\n",
      "Training loss: 8.024267432574737....\n",
      "Training loss: 8.028999314366567....\n",
      "Training loss: 8.028981507872611....\n",
      "Training loss: 8.031839998472291....\n",
      "Training loss: 8.032662886923099....\n",
      "Training loss: 8.028668403234315....\n",
      "Training loss: 8.031921070711384....\n",
      "Training loss: 8.029582882910393....\n",
      "Training loss: 8.031285900510811....\n",
      "Training loss: 8.030526644500759....\n",
      "Training loss: 8.0285129601301....\n",
      "Training loss: 8.030571702341174....\n",
      "Training loss: 8.029767731438357....\n",
      "Training loss: 8.02684763368609....\n",
      "Training loss: 8.027787998832004....\n",
      "Training loss: 8.025353257119383....\n",
      "Training loss: 8.031879985161089....\n",
      "Training loss: 8.030266054175152....\n",
      "Training loss: 8.029732918727007....\n",
      "Training loss: 8.024765434967987....\n",
      "Training loss: 8.02757351385782....\n",
      "Training loss: 8.027561607023316....\n",
      "Training loss: 8.029486379462986....\n",
      "Training loss: 8.031715486423717....\n",
      "Training loss: 8.032189349155484....\n",
      "Training loss: 8.0264546043679....\n",
      "Training loss: 8.028475078904385....\n",
      "Training loss: 8.027616964868649....\n",
      "Training loss: 8.02773577860471....\n",
      "Training loss: 8.026591717913561....\n",
      "Training loss: 8.029694517565297....\n",
      "Training loss: 8.032128783055045....\n",
      "Training loss: 8.029043448946885....\n",
      "Training loss: 8.028501843346024....\n",
      "Training loss: 8.027274880131205....\n",
      "Training loss: 8.027647344493959....\n",
      "Training loss: 8.026964571233....\n",
      "Training loss: 8.028338278698651....\n",
      "Training loss: 8.028494425442156....\n",
      "Training loss: 8.02459554797615....\n",
      "Training loss: 8.030113724333676....\n",
      "Training loss: 8.029529203025247....\n",
      "Training loss: 8.027612054324615....\n",
      "Training loss: 8.029585769524008....\n",
      "Training loss: 8.03114625662792....\n",
      "Training loss: 8.02780439116026....\n",
      "Training loss: 8.024629146539716....\n",
      "###########epoch####### 15\n",
      "Training loss: 8.027707828516895....\n",
      "Training loss: 8.027201147312082....\n",
      "Training loss: 8.028095505314198....\n",
      "Training loss: 8.028977063097182....\n",
      "Training loss: 8.02899581330006....\n",
      "Training loss: 8.028791172642645....\n",
      "Training loss: 8.026222971816289....\n",
      "Training loss: 8.02787307245103....\n",
      "Training loss: 8.030444537970036....\n",
      "Training loss: 8.031993272869755....\n",
      "Training loss: 8.030676213778726....\n",
      "Training loss: 8.032041056903052....\n",
      "Training loss: 8.031517938584027....\n",
      "Training loss: 8.02753753498004....\n",
      "Training loss: 8.025924351881521....\n",
      "Training loss: 8.0263684525863....\n",
      "Training loss: 8.0272657548117....\n",
      "Training loss: 8.026919497886423....\n",
      "Training loss: 8.028613408166779....\n",
      "Training loss: 8.029411753309782....\n",
      "Training loss: 8.02781554032956....\n",
      "Training loss: 8.029841197748219....\n",
      "Training loss: 8.027370920231826....\n",
      "Training loss: 8.025297061557245....\n",
      "Training loss: 8.025358433563317....\n",
      "Training loss: 8.029753176980776....\n",
      "Training loss: 8.022030969678234....\n",
      "Training loss: 8.02650284315412....\n",
      "Training loss: 8.02825110659328....\n",
      "Training loss: 8.02529952146858....\n",
      "Training loss: 8.029238466267643....\n",
      "Training loss: 8.02983824443788....\n",
      "Training loss: 8.028618808241625....\n",
      "Training loss: 8.030801876990425....\n",
      "Training loss: 8.028066639421727....\n",
      "Training loss: 8.02681848710227....\n",
      "Training loss: 8.032082215040237....\n",
      "Training loss: 8.029080006750167....\n",
      "Training loss: 8.027681602047954....\n",
      "Training loss: 8.026383942792425....\n",
      "Training loss: 8.023925704628974....\n",
      "Training loss: 8.028834004899778....\n",
      "Training loss: 8.028190266983154....\n",
      "Training loss: 8.028098005676734....\n",
      "Training loss: 8.027823323630798....\n",
      "Training loss: 8.027038754666231....\n",
      "Training loss: 8.025272914156183....\n",
      "Training loss: 8.03056365760854....\n",
      "Training loss: 8.030649427524436....\n",
      "Training loss: 8.02527000926062....\n",
      "Training loss: 8.029558103813578....\n",
      "Training loss: 8.02841260285195....\n",
      "Training loss: 8.027503748260028....\n",
      "Training loss: 8.026544673829495....\n",
      "Training loss: 8.029322584309575....\n",
      "Training loss: 8.030877681976321....\n",
      "Training loss: 8.030418434084622....\n",
      "Training loss: 8.0260598371775....\n",
      "Training loss: 8.033341977093032....\n",
      "Training loss: 8.029929389194264....\n",
      "Training loss: 8.025928270898167....\n",
      "Training loss: 8.023154271407659....\n",
      "Training loss: 8.028605165426224....\n",
      "Training loss: 8.032007909384058....\n",
      "Training loss: 8.029172993502831....\n",
      "Training loss: 8.02998913331923....\n",
      "Training loss: 8.02809893629333....\n",
      "Training loss: 8.033252429603898....\n",
      "Training loss: 8.026349709639684....\n",
      "Training loss: 8.0273055117285....\n",
      "Training loss: 8.032026090560692....\n",
      "Training loss: 8.025903577588792....\n",
      "Training loss: 8.026127071721083....\n",
      "Training loss: 8.027651913626341....\n",
      "Training loss: 8.032494230491011....\n",
      "Training loss: 8.028124341154378....\n",
      "Training loss: 8.027642126823547....\n",
      "Training loss: 8.027420172844879....\n",
      "Training loss: 8.030946404855332....\n",
      "Training loss: 8.025683638415583....\n",
      "Training loss: 8.029234603990767....\n",
      "Training loss: 8.025486130082214....\n",
      "Training loss: 8.028574920689136....\n",
      "Training loss: 8.028238112207307....\n",
      "Training loss: 8.030142519435163....\n",
      "Training loss: 8.031176992282305....\n",
      "Training loss: 8.026292168758964....\n",
      "Training loss: 8.02544568911122....\n",
      "Training loss: 8.024684955197264....\n",
      "Training loss: 8.026272374564027....\n",
      "Training loss: 8.031383022695179....\n",
      "Training loss: 8.02757357586989....\n",
      "Training loss: 8.029421994963762....\n",
      "Training loss: 8.02822906115472....\n",
      "Training loss: 8.026803123351687....\n",
      "Training loss: 8.02900679437932....\n",
      "Training loss: 8.028573676866584....\n",
      "Training loss: 8.026151592499508....\n",
      "Training loss: 8.028041164737717....\n",
      "Training loss: 8.032165836778564....\n",
      "Training loss: 8.024131060215824....\n",
      "Training loss: 8.026933616055057....\n",
      "Training loss: 8.02166855380893....\n",
      "Training loss: 8.025612495318725....\n",
      "Training loss: 8.0277900948022....\n",
      "Training loss: 8.027332633867678....\n",
      "Training loss: 8.022701326470727....\n",
      "Training loss: 8.029593183597866....\n",
      "Training loss: 8.025190831016488....\n",
      "Training loss: 8.032276748601936....\n",
      "Training loss: 8.02727309050586....\n",
      "Training loss: 8.025532909749648....\n",
      "Training loss: 8.025970670262305....\n",
      "Training loss: 8.028287392354477....\n",
      "Training loss: 8.029359638713832....\n",
      "Training loss: 8.033442256637468....\n",
      "Training loss: 8.028274440867126....\n",
      "Training loss: 8.030236453352975....\n",
      "Training loss: 8.027567232419136....\n",
      "Training loss: 8.023294519977863....\n",
      "Training loss: 8.027281921320027....\n",
      "Training loss: 8.027495176846847....\n",
      "Training loss: 8.027999421149255....\n",
      "Training loss: 8.028343951590786....\n",
      "Training loss: 8.025424042023468....\n",
      "Training loss: 8.026016391359331....\n",
      "Training loss: 8.025835297377643....\n",
      "Training loss: 8.024284789797044....\n",
      "Training loss: 8.029310627276624....\n",
      "Training loss: 8.030100506658483....\n",
      "Training loss: 8.02639971990411....\n",
      "Training loss: 8.029894227379517....\n",
      "Training loss: 8.022050002376805....\n",
      "Training loss: 8.026730948555281....\n",
      "Training loss: 8.026826330291613....\n",
      "Training loss: 8.02968361142304....\n",
      "Training loss: 8.030529977721791....\n",
      "Training loss: 8.026452218104026....\n",
      "Training loss: 8.029729945807873....\n",
      "Training loss: 8.027361187849916....\n",
      "Training loss: 8.029146072461335....\n",
      "Training loss: 8.028343315936377....\n",
      "Training loss: 8.026353681104952....\n",
      "Training loss: 8.028367438679117....\n",
      "Training loss: 8.027622621828769....\n",
      "Training loss: 8.024625503502277....\n",
      "Training loss: 8.025543572746813....\n",
      "Training loss: 8.023013462224409....\n",
      "Training loss: 8.029679589258242....\n",
      "Training loss: 8.02799088546887....\n",
      "Training loss: 8.027541084523993....\n",
      "Training loss: 8.022592975211758....\n",
      "Training loss: 8.025320545702831....\n",
      "Training loss: 8.025443479342337....\n",
      "Training loss: 8.027346927805532....\n",
      "Training loss: 8.029563786429533....\n",
      "Training loss: 8.029993432622051....\n",
      "Training loss: 8.024306784844557....\n",
      "Training loss: 8.02629551741058....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.025455747129838....\n",
      "Training loss: 8.025505849585379....\n",
      "Training loss: 8.024466339084945....\n",
      "Training loss: 8.027559048016002....\n",
      "Training loss: 8.029994040055573....\n",
      "Training loss: 8.026787340433756....\n",
      "Training loss: 8.026236777157836....\n",
      "Training loss: 8.025011386136555....\n",
      "Training loss: 8.0254629757729....\n",
      "Training loss: 8.024709243340332....\n",
      "Training loss: 8.026148686608522....\n",
      "Training loss: 8.026232928527724....\n",
      "Training loss: 8.022361383541483....\n",
      "Training loss: 8.027954159267354....\n",
      "Training loss: 8.027349738696026....\n",
      "Training loss: 8.025437641021629....\n",
      "Training loss: 8.02738761084792....\n",
      "Training loss: 8.028931889293505....\n",
      "Training loss: 8.025518617983224....\n",
      "Training loss: 8.022394600475296....\n",
      "###########epoch####### 16\n",
      "Training loss: 8.025581199302284....\n",
      "Training loss: 8.02498760184772....\n",
      "Training loss: 8.025855452435348....\n",
      "Training loss: 8.026796566814776....\n",
      "Training loss: 8.026748165552393....\n",
      "Training loss: 8.026557295561384....\n",
      "Training loss: 8.023963515297913....\n",
      "Training loss: 8.025682988761451....\n",
      "Training loss: 8.028336785449277....\n",
      "Training loss: 8.029816221132085....\n",
      "Training loss: 8.02861530053954....\n",
      "Training loss: 8.029919088773804....\n",
      "Training loss: 8.029433095719572....\n",
      "Training loss: 8.025351795678516....\n",
      "Training loss: 8.023661741783451....\n",
      "Training loss: 8.024132403544456....\n",
      "Training loss: 8.0251071972473....\n",
      "Training loss: 8.024704236836284....\n",
      "Training loss: 8.026487503167315....\n",
      "Training loss: 8.027248700684657....\n",
      "Training loss: 8.025665590320617....\n",
      "Training loss: 8.027747482899564....\n",
      "Training loss: 8.025173658626892....\n",
      "Training loss: 8.02296180218658....\n",
      "Training loss: 8.023153986359087....\n",
      "Training loss: 8.02758795708732....\n",
      "Training loss: 8.019704925939948....\n",
      "Training loss: 8.024206158924606....\n",
      "Training loss: 8.026072856889238....\n",
      "Training loss: 8.023014977643442....\n",
      "Training loss: 8.027118357478786....\n",
      "Training loss: 8.027579909601904....\n",
      "Training loss: 8.026403035565002....\n",
      "Training loss: 8.02863652374164....\n",
      "Training loss: 8.02580772824112....\n",
      "Training loss: 8.024483607582514....\n",
      "Training loss: 8.029949063310013....\n",
      "Training loss: 8.02704128075994....\n",
      "Training loss: 8.025562771731163....\n",
      "Training loss: 8.02418846081162....\n",
      "Training loss: 8.02163812520167....\n",
      "Training loss: 8.026657059709969....\n",
      "Training loss: 8.026022083319024....\n",
      "Training loss: 8.025831452490024....\n",
      "Training loss: 8.025610112853206....\n",
      "Training loss: 8.024790266788521....\n",
      "Training loss: 8.023056716209084....\n",
      "Training loss: 8.028402092799857....\n",
      "Training loss: 8.02840564769245....\n",
      "Training loss: 8.022945994781702....\n",
      "Training loss: 8.027455874313098....\n",
      "Training loss: 8.026122808037693....\n",
      "Training loss: 8.025177456293733....\n",
      "Training loss: 8.024371126911987....\n",
      "Training loss: 8.027223291574074....\n",
      "Training loss: 8.028721662624443....\n",
      "Training loss: 8.028358101288783....\n",
      "Training loss: 8.023843969321126....\n",
      "Training loss: 8.031166940077721....\n",
      "Training loss: 8.02766229093439....\n",
      "Training loss: 8.02373028162074....\n",
      "Training loss: 8.020905735640238....\n",
      "Training loss: 8.026359335530275....\n",
      "Training loss: 8.029946797984648....\n",
      "Training loss: 8.02685508277349....\n",
      "Training loss: 8.027826767043384....\n",
      "Training loss: 8.025854306681836....\n",
      "Training loss: 8.031141802135291....\n",
      "Training loss: 8.024141104584722....\n",
      "Training loss: 8.025030479654072....\n",
      "Training loss: 8.029899099913946....\n",
      "Training loss: 8.02352573447111....\n",
      "Training loss: 8.02384261195184....\n",
      "Training loss: 8.025501951948566....\n",
      "Training loss: 8.030312529559877....\n",
      "Training loss: 8.026122632308285....\n",
      "Training loss: 8.025459710554586....\n",
      "Training loss: 8.025088461567522....\n",
      "Training loss: 8.028758331722173....\n",
      "Training loss: 8.02345997166248....\n",
      "Training loss: 8.02708841546948....\n",
      "Training loss: 8.023308630315162....\n",
      "Training loss: 8.026414038547806....\n",
      "Training loss: 8.026011059009662....\n",
      "Training loss: 8.027973636796446....\n",
      "Training loss: 8.028970132917866....\n",
      "Training loss: 8.023965205848544....\n",
      "Training loss: 8.023129854635044....\n",
      "Training loss: 8.022316529267355....\n",
      "Training loss: 8.024101045144038....\n",
      "Training loss: 8.029289226981543....\n",
      "Training loss: 8.025402563159519....\n",
      "Training loss: 8.027297459863778....\n",
      "Training loss: 8.025886316758376....\n",
      "Training loss: 8.024496185609722....\n",
      "Training loss: 8.026865314692282....\n",
      "Training loss: 8.02642291362951....\n",
      "Training loss: 8.02378253256856....\n",
      "Training loss: 8.025805102138502....\n",
      "Training loss: 8.029939485375039....\n",
      "Training loss: 8.021821137979694....\n",
      "Training loss: 8.02470108872094....\n",
      "Training loss: 8.019326054452076....\n",
      "Training loss: 8.02332555052936....\n",
      "Training loss: 8.02549003456062....\n",
      "Training loss: 8.024981173612503....\n",
      "Training loss: 8.020384016866071....\n",
      "Training loss: 8.027427928422279....\n",
      "Training loss: 8.022929999966992....\n",
      "Training loss: 8.030055930208785....\n",
      "Training loss: 8.025048047311975....\n",
      "Training loss: 8.023374181267963....\n",
      "Training loss: 8.023670447358292....\n",
      "Training loss: 8.02605336083892....\n",
      "Training loss: 8.027186849749233....\n",
      "Training loss: 8.031242884206634....\n",
      "Training loss: 8.026096139450528....\n",
      "Training loss: 8.028055106678648....\n",
      "Training loss: 8.025274357811583....\n",
      "Training loss: 8.020943662271192....\n",
      "Training loss: 8.02510750443423....\n",
      "Training loss: 8.025417631713696....\n",
      "Training loss: 8.025782943940689....\n",
      "Training loss: 8.026092501191153....\n",
      "Training loss: 8.023151532644304....\n",
      "Training loss: 8.023827416743307....\n",
      "Training loss: 8.023633855440808....\n",
      "Training loss: 8.02208685774481....\n",
      "Training loss: 8.027127683330301....\n",
      "Training loss: 8.027888640354686....\n",
      "Training loss: 8.024223051289457....\n",
      "Training loss: 8.027708139156934....\n",
      "Training loss: 8.019787408112574....\n",
      "Training loss: 8.024417526791622....\n",
      "Training loss: 8.024627824574958....\n",
      "Training loss: 8.027484858813878....\n",
      "Training loss: 8.028359565219096....\n",
      "Training loss: 8.02419729141084....\n",
      "Training loss: 8.027492502098607....\n",
      "Training loss: 8.025098013137283....\n",
      "Training loss: 8.026971065557172....\n",
      "Training loss: 8.026120565191766....\n",
      "Training loss: 8.024154801342062....\n",
      "Training loss: 8.026121830016509....\n",
      "Training loss: 8.025439459885476....\n",
      "Training loss: 8.022360545485485....\n",
      "Training loss: 8.023253130101596....\n",
      "Training loss: 8.020629040514166....\n",
      "Training loss: 8.027440074384558....\n",
      "Training loss: 8.025675188136857....\n",
      "Training loss: 8.025314128895285....\n",
      "Training loss: 8.02038247595531....\n",
      "Training loss: 8.023024584211688....\n",
      "Training loss: 8.023285108215015....\n",
      "Training loss: 8.02517009450101....\n",
      "Training loss: 8.02737308362412....\n",
      "Training loss: 8.02775283157665....\n",
      "Training loss: 8.022115909759941....\n",
      "Training loss: 8.024070053980308....\n",
      "Training loss: 8.023251720519008....\n",
      "Training loss: 8.023232214465741....\n",
      "Training loss: 8.022299603720507....\n",
      "Training loss: 8.025387940160249....\n",
      "Training loss: 8.027816163517302....\n",
      "Training loss: 8.024488279748539....\n",
      "Training loss: 8.023928754754593....\n",
      "Training loss: 8.022704120517693....\n",
      "Training loss: 8.023235368819682....\n",
      "Training loss: 8.022411598696197....\n",
      "Training loss: 8.023917597276464....\n",
      "Training loss: 8.023928516558776....\n",
      "Training loss: 8.020081191982426....\n",
      "Training loss: 8.025756777099014....\n",
      "Training loss: 8.025130739295125....\n",
      "Training loss: 8.023222449139734....\n",
      "Training loss: 8.025141430713344....\n",
      "Training loss: 8.026675250128443....\n",
      "Training loss: 8.02318990516979....\n",
      "Training loss: 8.02010207439345....\n",
      "###########epoch####### 17\n",
      "Training loss: 8.023415753977451....\n",
      "Training loss: 8.022730499184638....\n",
      "Training loss: 8.023574276596117....\n",
      "Training loss: 8.024573439176585....\n",
      "Training loss: 8.024460337963339....\n",
      "Training loss: 8.024281105680592....\n",
      "Training loss: 8.021658655268562....\n",
      "Training loss: 8.023453795754401....\n",
      "Training loss: 8.026189331796976....\n",
      "Training loss: 8.027597860232012....\n",
      "Training loss: 8.026515958657539....\n",
      "Training loss: 8.027752133574829....\n",
      "Training loss: 8.027311782275667....\n",
      "Training loss: 8.023128037809366....\n",
      "Training loss: 8.021357453181137....\n",
      "Training loss: 8.02185796200228....\n",
      "Training loss: 8.022908124134828....\n",
      "Training loss: 8.022452246449031....\n",
      "Training loss: 8.024318716728475....\n",
      "Training loss: 8.025046051440388....\n",
      "Training loss: 8.023471634546853....\n",
      "Training loss: 8.025615882619828....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.022938665078136....\n",
      "Training loss: 8.02058129863545....\n",
      "Training loss: 8.02090645692741....\n",
      "Training loss: 8.025382329526064....\n",
      "Training loss: 8.017339227025996....\n",
      "Training loss: 8.021865947149886....\n",
      "Training loss: 8.023849486909612....\n",
      "Training loss: 8.020688372569513....\n",
      "Training loss: 8.024957866629453....\n",
      "Training loss: 8.025278653495834....\n",
      "Training loss: 8.024147946971626....\n",
      "Training loss: 8.026428360013117....\n",
      "Training loss: 8.023509491049554....\n",
      "Training loss: 8.022101910058606....\n",
      "Training loss: 8.02777803357444....\n",
      "Training loss: 8.024966835464008....\n",
      "Training loss: 8.023406837702861....\n",
      "Training loss: 8.021950786792242....\n",
      "Training loss: 8.019307282317053....\n",
      "Training loss: 8.02443546246484....\n",
      "Training loss: 8.02381552521776....\n",
      "Training loss: 8.023526562522555....\n",
      "Training loss: 8.023348753585127....\n",
      "Training loss: 8.022502264154529....\n",
      "Training loss: 8.02079758544069....\n",
      "Training loss: 8.026204487090194....\n",
      "Training loss: 8.026120218748366....\n",
      "Training loss: 8.020573620380196....\n",
      "Training loss: 8.025315862481783....\n",
      "Training loss: 8.02379153579589....\n",
      "Training loss: 8.022806017242566....\n",
      "Training loss: 8.022157786957571....\n",
      "Training loss: 8.025077713870164....\n",
      "Training loss: 8.026528153965533....\n",
      "Training loss: 8.026259863077083....\n",
      "Training loss: 8.021583584948141....\n",
      "Training loss: 8.02895086741615....\n",
      "Training loss: 8.025352870006092....\n",
      "Training loss: 8.021484798087263....\n",
      "Training loss: 8.018610043435718....\n",
      "Training loss: 8.024071317452503....\n",
      "Training loss: 8.02784545520226....\n",
      "Training loss: 8.024494581755945....\n",
      "Training loss: 8.02562315366584....\n",
      "Training loss: 8.023565461084818....\n",
      "Training loss: 8.028989272942349....\n",
      "Training loss: 8.021895051557065....\n",
      "Training loss: 8.022709113133775....\n",
      "Training loss: 8.027729929154003....\n",
      "Training loss: 8.02110966459446....\n",
      "Training loss: 8.02151429482463....\n",
      "Training loss: 8.023306471681671....\n",
      "Training loss: 8.028090379150782....\n",
      "Training loss: 8.024083603501236....\n",
      "Training loss: 8.023233175753377....\n",
      "Training loss: 8.022711389604234....\n",
      "Training loss: 8.02652938930813....\n",
      "Training loss: 8.021193739650908....\n",
      "Training loss: 8.024902447868744....\n",
      "Training loss: 8.02108981864772....\n",
      "Training loss: 8.024210904286821....\n",
      "Training loss: 8.023740260985084....\n",
      "Training loss: 8.025763408462169....\n",
      "Training loss: 8.02672480530798....\n",
      "Training loss: 8.021590258385352....\n",
      "Training loss: 8.020770537665548....\n",
      "Training loss: 8.019902967518464....\n",
      "Training loss: 8.021889205239928....\n",
      "Training loss: 8.027153528219937....\n",
      "Training loss: 8.023189413372663....\n",
      "Training loss: 8.025129692772722....\n",
      "Training loss: 8.023497733722726....\n",
      "Training loss: 8.022144073964414....\n",
      "Training loss: 8.02468413288967....\n",
      "Training loss: 8.024227453534207....\n",
      "Training loss: 8.021371252165238....\n",
      "Training loss: 8.023522983485012....\n",
      "Training loss: 8.02767255501242....\n",
      "Training loss: 8.019465251013779....\n",
      "Training loss: 8.022423659657205....\n",
      "Training loss: 8.016944241209668....\n",
      "Training loss: 8.02099223932157....\n",
      "Training loss: 8.023144345568745....\n",
      "Training loss: 8.022583603992771....\n",
      "Training loss: 8.018024585649004....\n",
      "Training loss: 8.025220894120581....\n",
      "Training loss: 8.020621439744879....\n",
      "Training loss: 8.027792698859198....\n",
      "Training loss: 8.02277898805874....\n",
      "Training loss: 8.021175508608746....\n",
      "Training loss: 8.021326711087669....\n",
      "Training loss: 8.023777327487295....\n",
      "Training loss: 8.024970574691977....\n",
      "Training loss: 8.028999024733318....\n",
      "Training loss: 8.023878271089878....\n",
      "Training loss: 8.025831332937226....\n",
      "Training loss: 8.022937938161919....\n",
      "Training loss: 8.01855145676197....\n",
      "Training loss: 8.022893254452265....\n",
      "Training loss: 8.023303972822484....\n",
      "Training loss: 8.02351897182204....\n",
      "Training loss: 8.023796515098084....\n",
      "Training loss: 8.020836749783275....\n",
      "Training loss: 8.02159749476939....\n",
      "Training loss: 8.021385021694316....\n",
      "Training loss: 8.019844802889738....\n",
      "Training loss: 8.024902912337499....\n",
      "Training loss: 8.025636920895776....\n",
      "Training loss: 8.02200320051096....\n",
      "Training loss: 8.025481003879987....\n",
      "Training loss: 8.017478247514973....\n",
      "Training loss: 8.022059742857747....\n",
      "Training loss: 8.02238371858044....\n",
      "Training loss: 8.02524216493698....\n",
      "Training loss: 8.026142358103819....\n",
      "Training loss: 8.021900194731431....\n",
      "Training loss: 8.025209928067856....\n",
      "Training loss: 8.022792947982463....\n",
      "Training loss: 8.024754917774468....\n",
      "Training loss: 8.023850930899462....\n",
      "Training loss: 8.02191223005212....\n",
      "Training loss: 8.023832453087833....\n",
      "Training loss: 8.023219136319447....\n",
      "Training loss: 8.02005414036917....\n",
      "Training loss: 8.020919094281233....\n",
      "Training loss: 8.018201579379811....\n",
      "Training loss: 8.025159218351602....\n",
      "Training loss: 8.023310746650484....\n",
      "Training loss: 8.023049966637362....\n",
      "Training loss: 8.018129849837363....\n",
      "Training loss: 8.02068318648058....\n",
      "Training loss: 8.021082878666407....\n",
      "Training loss: 8.022953197448134....\n",
      "Training loss: 8.02513834822493....\n",
      "Training loss: 8.025473733729369....\n",
      "Training loss: 8.019885329063637....\n",
      "Training loss: 8.021798617259284....\n",
      "Training loss: 8.02100141272147....\n",
      "Training loss: 8.020911855130468....\n",
      "Training loss: 8.02008922636412....\n",
      "Training loss: 8.023176760971097....\n",
      "Training loss: 8.025595211171888....\n",
      "Training loss: 8.022145916017939....\n",
      "Training loss: 8.021575913714273....\n",
      "Training loss: 8.020355120562119....\n",
      "Training loss: 8.020962480124737....\n",
      "Training loss: 8.020070793576036....\n",
      "Training loss: 8.02164626225056....\n",
      "Training loss: 8.021580027097144....\n",
      "Training loss: 8.017755967657353....\n",
      "Training loss: 8.023518934381082....\n",
      "Training loss: 8.022864348817025....\n",
      "Training loss: 8.020960747797172....\n",
      "Training loss: 8.022847539859102....\n",
      "Training loss: 8.024375197413079....\n",
      "Training loss: 8.020820242252436....\n",
      "Training loss: 8.017758561211158....\n",
      "###########epoch####### 18\n",
      "Training loss: 8.02121008604119....\n",
      "Training loss: 8.020428044999733....\n",
      "Training loss: 8.021245791795167....\n",
      "Training loss: 8.022308883110613....\n",
      "Training loss: 8.02213129470153....\n",
      "Training loss: 8.021962084590772....\n",
      "Training loss: 8.019306268916864....\n",
      "Training loss: 8.021183008664035....\n",
      "Training loss: 8.023997870473826....\n",
      "Training loss: 8.025337212828582....\n",
      "Training loss: 8.02437472189927....\n",
      "Training loss: 8.025543871423404....\n",
      "Training loss: 8.0251503328318....\n",
      "Training loss: 8.020861671943006....\n",
      "Training loss: 8.019008392973404....\n",
      "Training loss: 8.019542370016504....\n",
      "Training loss: 8.02066671882981....\n",
      "Training loss: 8.020155987710428....\n",
      "Training loss: 8.022107314353828....\n",
      "Training loss: 8.022801280892946....\n",
      "Training loss: 8.021231242174483....\n",
      "Training loss: 8.023441462925662....\n",
      "Training loss: 8.020660743085923....\n",
      "Training loss: 8.01815206430744....\n",
      "Training loss: 8.01861104785219....\n",
      "Training loss: 8.02313284940254....\n",
      "Training loss: 8.014930776450354....\n",
      "Training loss: 8.019482125702673....\n",
      "Training loss: 8.021581571688397....\n",
      "Training loss: 8.018317655885816....\n",
      "Training loss: 8.022754722831184....\n",
      "Training loss: 8.022931390220917....\n",
      "Training loss: 8.021848088660267....\n",
      "Training loss: 8.024175666898604....\n",
      "Training loss: 8.021166908638804....\n",
      "Training loss: 8.019671311194465....\n",
      "Training loss: 8.02556256951824....\n",
      "Training loss: 8.02285219953096....\n",
      "Training loss: 8.021208033525138....\n",
      "Training loss: 8.019668588732841....\n",
      "Training loss: 8.016932207124288....\n",
      "Training loss: 8.022170152075542....\n",
      "Training loss: 8.021568875456898....\n",
      "Training loss: 8.021174421099367....\n",
      "Training loss: 8.021043330625902....\n",
      "Training loss: 8.020170706266832....\n",
      "Training loss: 8.018492106188175....\n",
      "Training loss: 8.023963069777972....\n",
      "Training loss: 8.023791478642861....\n",
      "Training loss: 8.018152851012461....\n",
      "Training loss: 8.023136851238268....\n",
      "Training loss: 8.021418111514638....\n",
      "Training loss: 8.020382766386037....\n",
      "Training loss: 8.019901483140892....\n",
      "Training loss: 8.022885735833583....\n",
      "Training loss: 8.024295366993199....\n",
      "Training loss: 8.024117327758537....\n",
      "Training loss: 8.019276514204051....\n",
      "Training loss: 8.02668976713207....\n",
      "Training loss: 8.022999545086305....\n",
      "Training loss: 8.01919021734636....\n",
      "Training loss: 8.016264775986391....\n",
      "Training loss: 8.021738260050753....\n",
      "Training loss: 8.02570629494799....\n",
      "Training loss: 8.022089092460922....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.023377074844657....\n",
      "Training loss: 8.021232382793023....\n",
      "Training loss: 8.026788917215473....\n",
      "Training loss: 8.019607497940545....\n",
      "Training loss: 8.020338023876581....\n",
      "Training loss: 8.0255151119193....\n",
      "Training loss: 8.018647899230615....\n",
      "Training loss: 8.019137324191513....\n",
      "Training loss: 8.02106296471265....\n",
      "Training loss: 8.025823851682125....\n",
      "Training loss: 8.022003671101105....\n",
      "Training loss: 8.020962703621187....\n",
      "Training loss: 8.020286398825123....\n",
      "Training loss: 8.024252483783913....\n",
      "Training loss: 8.018884324790264....\n",
      "Training loss: 8.022673808259913....\n",
      "Training loss: 8.018829146245801....\n",
      "Training loss: 8.021963823654184....\n",
      "Training loss: 8.0214194681455....\n",
      "Training loss: 8.023511447100748....\n",
      "Training loss: 8.024435920840794....\n",
      "Training loss: 8.019164158619047....\n",
      "Training loss: 8.0183673157927....\n",
      "Training loss: 8.01743926081195....\n",
      "Training loss: 8.019632766899225....\n",
      "Training loss: 8.02497501940497....\n",
      "Training loss: 8.020932224123175....\n",
      "Training loss: 8.022919920993921....\n",
      "Training loss: 8.021060942887225....\n",
      "Training loss: 8.019742744169735....\n",
      "Training loss: 8.022461129516381....\n",
      "Training loss: 8.021984905996813....\n",
      "Training loss: 8.01891431613656....\n",
      "Training loss: 8.021194372180567....\n",
      "Training loss: 8.025356355992923....\n",
      "Training loss: 8.017061077016573....\n",
      "Training loss: 8.020096067857326....\n",
      "Training loss: 8.014518314616442....\n",
      "Training loss: 8.018611021099236....\n",
      "Training loss: 8.020750228681125....\n",
      "Training loss: 8.02013782033217....\n",
      "Training loss: 8.015616222561174....\n",
      "Training loss: 8.022968968440205....\n",
      "Training loss: 8.018262599856996....\n",
      "Training loss: 8.025484784796....\n",
      "Training loss: 8.02046569890865....\n",
      "Training loss: 8.018931956786584....\n",
      "Training loss: 8.018934814166737....\n",
      "Training loss: 8.021458284484345....\n",
      "Training loss: 8.022713550703878....\n",
      "Training loss: 8.026707528915228....\n",
      "Training loss: 8.021617583677319....\n",
      "Training loss: 8.023561963377198....\n",
      "Training loss: 8.020549856888392....\n",
      "Training loss: 8.016110648783103....\n",
      "Training loss: 8.02063599572861....\n",
      "Training loss: 8.02114892036196....\n",
      "Training loss: 8.021205067146049....\n",
      "Training loss: 8.021452666744615....\n",
      "Training loss: 8.018474209540766....\n",
      "Training loss: 8.019323556793422....\n",
      "Training loss: 8.019087784722359....\n",
      "Training loss: 8.017557616843133....\n",
      "Training loss: 8.022634544300521....\n",
      "Training loss: 8.023338780447352....\n",
      "Training loss: 8.019736918222309....\n",
      "Training loss: 8.023204967907459....\n",
      "Training loss: 8.01511835732348....\n",
      "Training loss: 8.019650964454163....\n",
      "Training loss: 8.020091013817458....\n",
      "Training loss: 8.022955229387799....\n",
      "Training loss: 8.02387716894426....\n",
      "Training loss: 8.019557184901373....\n",
      "Training loss: 8.022878500710938....\n",
      "Training loss: 8.02044264853206....\n",
      "Training loss: 8.022493953020042....\n",
      "Training loss: 8.0215312245301....\n",
      "Training loss: 8.019621822880195....\n",
      "Training loss: 8.02149651521413....\n",
      "Training loss: 8.02095254809581....\n",
      "Training loss: 8.017703357091301....\n",
      "Training loss: 8.018539214216311....\n",
      "Training loss: 8.015724323954947....\n",
      "Training loss: 8.022830646736587....\n",
      "Training loss: 8.020896141204828....\n",
      "Training loss: 8.020741913561048....\n",
      "Training loss: 8.01583180861538....\n",
      "Training loss: 8.01829226766259....\n",
      "Training loss: 8.018831260775308....\n",
      "Training loss: 8.020691312178586....\n",
      "Training loss: 8.022856587672118....\n",
      "Training loss: 8.023149987113163....\n",
      "Training loss: 8.017609376255143....\n",
      "Training loss: 8.01947564709395....\n",
      "Training loss: 8.018701183853079....\n",
      "Training loss: 8.018542491063236....\n",
      "Training loss: 8.017831383343035....\n",
      "Training loss: 8.02092017169489....\n",
      "Training loss: 8.023328306082133....\n",
      "Training loss: 8.019757494073867....\n",
      "Training loss: 8.019171265808199....\n",
      "Training loss: 8.017955471869428....\n",
      "Training loss: 8.018638906534258....\n",
      "Training loss: 8.017678264921372....\n",
      "Training loss: 8.019328743351972....\n",
      "Training loss: 8.019183728611301....\n",
      "Training loss: 8.015379181129662....\n",
      "Training loss: 8.021236457944763....\n",
      "Training loss: 8.020544227807628....\n",
      "Training loss: 8.018648701478396....\n",
      "Training loss: 8.020502395386853....\n",
      "Training loss: 8.022029454204203....\n",
      "Training loss: 8.018403954147878....\n",
      "Training loss: 8.015363798702625....\n",
      "###########epoch####### 19\n",
      "Training loss: 8.018958857840511....\n",
      "Training loss: 8.018076168458228....\n",
      "Training loss: 8.018864723912516....\n",
      "Training loss: 8.019997184373729....\n",
      "Training loss: 8.019752779240156....\n",
      "Training loss: 8.019595901958327....\n",
      "Training loss: 8.016897293262723....\n",
      "Training loss: 8.018862373485769....\n",
      "Training loss: 8.021759766492634....\n",
      "Training loss: 8.023030382721217....\n",
      "Training loss: 8.022184803628113....\n",
      "Training loss: 8.023289480446127....\n",
      "Training loss: 8.022944816550106....\n",
      "Training loss: 8.018545883833871....\n",
      "Training loss: 8.016614358020757....\n",
      "Training loss: 8.017176039336706....\n",
      "Training loss: 8.018380508215776....\n",
      "Training loss: 8.017809800092245....\n",
      "Training loss: 8.019849935304103....\n",
      "Training loss: 8.020510123661547....\n",
      "Training loss: 8.01893994611203....\n",
      "Training loss: 8.021218366853327....\n",
      "Training loss: 8.018335654477033....\n",
      "Training loss: 8.015669817166215....\n",
      "Training loss: 8.016268946806811....\n",
      "Training loss: 8.020835621501277....\n",
      "Training loss: 8.012474083679361....\n",
      "Training loss: 8.01704551323972....\n",
      "Training loss: 8.019266638322515....\n",
      "Training loss: 8.015900523754153....\n",
      "Training loss: 8.020504246141423....\n",
      "Training loss: 8.020537420204226....\n",
      "Training loss: 8.019498465370065....\n",
      "Training loss: 8.021872075664492....\n",
      "Training loss: 8.018774296219966....\n",
      "Training loss: 8.017187585236124....\n",
      "Training loss: 8.023296885512137....\n",
      "Training loss: 8.020692693834219....\n",
      "Training loss: 8.01896362124598....\n",
      "Training loss: 8.017337815889563....\n",
      "Training loss: 8.01450682250158....\n",
      "Training loss: 8.019857525733018....\n",
      "Training loss: 8.019276507799207....\n",
      "Training loss: 8.01877262771145....\n",
      "Training loss: 8.018690578041696....\n",
      "Training loss: 8.017792857538339....\n",
      "Training loss: 8.016137736034302....\n",
      "Training loss: 8.02166755569945....\n",
      "Training loss: 8.021413551451444....\n",
      "Training loss: 8.015680700954787....\n",
      "Training loss: 8.020915584536791....\n",
      "Training loss: 8.018993709839052....\n",
      "Training loss: 8.017903573221492....\n",
      "Training loss: 8.017597870651475....\n",
      "Training loss: 8.020641360060424....\n",
      "Training loss: 8.022017557243844....\n",
      "Training loss: 8.021924983191376....\n",
      "Training loss: 8.016916929434545....\n",
      "Training loss: 8.024380967123355....\n",
      "Training loss: 8.020596582813837....\n",
      "Training loss: 8.016845559250019....\n",
      "Training loss: 8.013864312289282....\n",
      "Training loss: 8.019356084549816....\n",
      "Training loss: 8.023523441040854....\n",
      "Training loss: 8.019634423158845....\n",
      "Training loss: 8.021078586594115....\n",
      "Training loss: 8.018850422892449....\n",
      "Training loss: 8.024539832368989....\n",
      "Training loss: 8.01727204451727....\n",
      "Training loss: 8.017914939932028....\n",
      "Training loss: 8.023245929765238....\n",
      "Training loss: 8.01613575392993....\n",
      "Training loss: 8.016712196041912....\n",
      "Training loss: 8.018771035816174....\n",
      "Training loss: 8.023506196660504....\n",
      "Training loss: 8.019879694977288....\n",
      "Training loss: 8.018647751116262....\n",
      "Training loss: 8.017806105177712....\n",
      "Training loss: 8.021925289977984....\n",
      "Training loss: 8.016524871594576....\n",
      "Training loss: 8.02039638760803....\n",
      "Training loss: 8.016520017622154....\n",
      "Training loss: 8.019669508987775....\n",
      "Training loss: 8.019045064700457....\n",
      "Training loss: 8.02121441208339....\n",
      "Training loss: 8.022098021679867....\n",
      "Training loss: 8.016686682320575....\n",
      "Training loss: 8.01591703509022....\n",
      "Training loss: 8.014921059313151....\n",
      "Training loss: 8.01733129919307....\n",
      "Training loss: 8.022753273923144....\n",
      "Training loss: 8.018630087529369....\n",
      "Training loss: 8.020661958007839....\n",
      "Training loss: 8.018573914155457....\n",
      "Training loss: 8.017289169719005....\n",
      "Training loss: 8.020190980923656....\n",
      "Training loss: 8.019692341170266....\n",
      "Training loss: 8.016405874746056....\n",
      "Training loss: 8.018817427820403....\n",
      "Training loss: 8.022989882943802....\n",
      "Training loss: 8.014600580360874....\n",
      "Training loss: 8.017719040057996....\n",
      "Training loss: 8.012044721892345....\n",
      "Training loss: 8.016179053378073....\n",
      "Training loss: 8.018304050309832....\n",
      "Training loss: 8.017639506087882....\n",
      "Training loss: 8.013154680437353....\n",
      "Training loss: 8.02066751993229....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.015851848930543....\n",
      "Training loss: 8.02312759159087....\n",
      "Training loss: 8.01810416004476....\n",
      "Training loss: 8.01663915290747....\n",
      "Training loss: 8.01649170516515....\n",
      "Training loss: 8.019090977618479....\n",
      "Training loss: 8.020409380077904....\n",
      "Training loss: 8.024368097585697....\n",
      "Training loss: 8.019311552787928....\n",
      "Training loss: 8.021244815607014....\n",
      "Training loss: 8.018109264298502....\n",
      "Training loss: 8.013616477055885....\n",
      "Training loss: 8.018328214850499....\n",
      "Training loss: 8.018947043832153....\n",
      "Training loss: 8.018837258614447....\n",
      "Training loss: 8.019060194513838....\n",
      "Training loss: 8.016063797448428....\n",
      "Training loss: 8.016995783182258....\n",
      "Training loss: 8.016742195101559....\n",
      "Training loss: 8.01522017541847....\n",
      "Training loss: 8.020318646824569....\n",
      "Training loss: 8.020988810591332....\n",
      "Training loss: 8.017424407563785....\n",
      "Training loss: 8.020877563996718....\n",
      "Training loss: 8.012707344248659....\n",
      "Training loss: 8.017187584167642....\n",
      "Training loss: 8.017744301331076....\n",
      "Training loss: 8.020618423030093....\n",
      "Training loss: 8.021560558499127....\n",
      "Training loss: 8.017163734219654....\n",
      "Training loss: 8.020497075416168....\n",
      "Training loss: 8.01804491693821....\n",
      "Training loss: 8.020187787422627....\n",
      "Training loss: 8.019161384682807....\n",
      "Training loss: 8.017279370809227....\n",
      "Training loss: 8.019110241532557....\n",
      "Training loss: 8.018635801485802....\n",
      "Training loss: 8.015301637966195....\n",
      "Training loss: 8.016110358492183....\n",
      "Training loss: 8.013194433957553....\n",
      "Training loss: 8.020452296346347....\n",
      "Training loss: 8.01842624523147....\n",
      "Training loss: 8.018386741548932....\n",
      "Training loss: 8.013482702492034....\n",
      "Training loss: 8.015845882106909....\n",
      "Training loss: 8.01652893680035....\n",
      "Training loss: 8.018379777749814....\n",
      "Training loss: 8.020524564666312....\n",
      "Training loss: 8.020776129286546....\n",
      "Training loss: 8.015284109131956....\n",
      "Training loss: 8.017100446006362....\n",
      "Training loss: 8.016351704113559....\n",
      "Training loss: 8.01612293073206....\n",
      "Training loss: 8.01552585125042....\n",
      "Training loss: 8.018616212699163....\n",
      "Training loss: 8.021012164198572....\n",
      "Training loss: 8.017318842637911....\n",
      "Training loss: 8.016717220499158....\n",
      "Training loss: 8.015505507724459....\n",
      "Training loss: 8.016261304554826....\n",
      "Training loss: 8.01523480789647....\n",
      "Training loss: 8.016960610441151....\n",
      "Training loss: 8.016735837554027....\n",
      "Training loss: 8.012949339279732....\n",
      "Training loss: 8.018906373257426....\n",
      "Training loss: 8.018172793465428....\n",
      "Training loss: 8.016282980170978....\n",
      "Training loss: 8.01810513023867....\n",
      "Training loss: 8.019631559618523....\n",
      "Training loss: 8.015940100979376....\n",
      "Training loss: 8.012916633340279....\n",
      "###########epoch####### 20\n",
      "Training loss: 8.016662090811808....\n",
      "Training loss: 8.015679773240718....\n",
      "Training loss: 8.016433443897181....\n",
      "Training loss: 8.017634200211532....\n",
      "Training loss: 8.017322953608586....\n",
      "Training loss: 8.017179128004488....\n",
      "Training loss: 8.014429952047728....\n",
      "Training loss: 8.016489046795794....\n",
      "Training loss: 8.019471551514442....\n",
      "Training loss: 8.020676678416928....\n",
      "Training loss: 8.019947250016367....\n",
      "Training loss: 8.020984598446034....\n",
      "Training loss: 8.020690069393714....\n",
      "Training loss: 8.016183266004296....\n",
      "Training loss: 8.014172260223809....\n",
      "Training loss: 8.014759836614932....\n",
      "Training loss: 8.016045425901172....\n",
      "Training loss: 8.015410302810247....\n",
      "Training loss: 8.0175432859371....\n",
      "Training loss: 8.018170144768161....\n",
      "Training loss: 8.016597168729325....\n",
      "Training loss: 8.018945846298042....\n",
      "Training loss: 8.015959710212131....\n",
      "Training loss: 8.013135570456182....\n",
      "Training loss: 8.013877959790726....\n",
      "Training loss: 8.018489572546805....\n",
      "Training loss: 8.00996660113849....\n",
      "Training loss: 8.014558274646298....\n",
      "Training loss: 8.016907096511149....\n",
      "Training loss: 8.013436589708299....\n",
      "Training loss: 8.018204186362789....\n",
      "Training loss: 8.018095994811192....\n",
      "Training loss: 8.017099231009336....\n",
      "Training loss: 8.019517638005938....\n",
      "Training loss: 8.016330639170087....\n",
      "Training loss: 8.014649967613277....\n",
      "Training loss: 8.020983475213747....\n",
      "Training loss: 8.018485462979447....\n",
      "Training loss: 8.016673656659037....\n",
      "Training loss: 8.014957327572736....\n",
      "Training loss: 8.012030078338526....\n",
      "Training loss: 8.017495876527045....\n",
      "Training loss: 8.01693640403732....\n",
      "Training loss: 8.01631824333501....\n",
      "Training loss: 8.016286147964337....\n",
      "Training loss: 8.015365366807755....\n",
      "Training loss: 8.013731418263404....\n",
      "Training loss: 8.019318809699582....\n",
      "Training loss: 8.018986370508252....\n",
      "Training loss: 8.013156876599236....\n",
      "Training loss: 8.018648680290562....\n",
      "Training loss: 8.01651782415245....\n",
      "Training loss: 8.015367212544986....\n",
      "Training loss: 8.015246611919565....\n",
      "Training loss: 8.018351047389286....\n",
      "Training loss: 8.019692815128886....\n",
      "Training loss: 8.01968198942582....\n",
      "Training loss: 8.014504883430076....\n",
      "Training loss: 8.022022683759634....\n",
      "Training loss: 8.018143516761196....\n",
      "Training loss: 8.014450784348478....\n",
      "Training loss: 8.011411304308815....\n",
      "Training loss: 8.016922143701343....\n",
      "Training loss: 8.021292840525506....\n",
      "Training loss: 8.017130392047447....\n",
      "Training loss: 8.018727395962546....\n",
      "Training loss: 8.016419135540492....\n",
      "Training loss: 8.02224053944057....\n",
      "Training loss: 8.014885265125802....\n",
      "Training loss: 8.015437240054377....\n",
      "Training loss: 8.020923532174473....\n",
      "Training loss: 8.01357197177099....\n",
      "Training loss: 8.014238514125289....\n",
      "Training loss: 8.016426377986633....\n",
      "Training loss: 8.021137876650005....\n",
      "Training loss: 8.01770959556532....\n",
      "Training loss: 8.016283051207115....\n",
      "Training loss: 8.01527129358695....\n",
      "Training loss: 8.019544484105529....\n",
      "Training loss: 8.014114487787907....\n",
      "Training loss: 8.01806969931684....\n",
      "Training loss: 8.014159866424501....\n",
      "Training loss: 8.017323163469413....\n",
      "Training loss: 8.016617418710272....\n",
      "Training loss: 8.018868536414374....\n",
      "Training loss: 8.019713326455225....\n",
      "Training loss: 8.014156983587963....\n",
      "Training loss: 8.013419147269328....\n",
      "Training loss: 8.012345256988324....\n",
      "Training loss: 8.014982676348291....\n",
      "Training loss: 8.02048344412954....\n",
      "Training loss: 8.016277908083756....\n",
      "Training loss: 8.018353349025361....\n",
      "Training loss: 8.016032673188633....\n",
      "Training loss: 8.014782200250249....\n",
      "Training loss: 8.01786964883099....\n",
      "Training loss: 8.017348533668308....\n",
      "Training loss: 8.013840734307198....\n",
      "Training loss: 8.01638865499254....\n",
      "Training loss: 8.02056808441534....\n",
      "Training loss: 8.01208417588936....\n",
      "Training loss: 8.015292551440119....\n",
      "Training loss: 8.009518479607435....\n",
      "Training loss: 8.013698041646732....\n",
      "Training loss: 8.015804325656468....\n",
      "Training loss: 8.01508674498569....\n",
      "Training loss: 8.010639175160385....\n",
      "Training loss: 8.018316593059591....\n",
      "Training loss: 8.013387445299....\n",
      "Training loss: 8.020719642723904....\n",
      "Training loss: 8.015694284251127....\n",
      "Training loss: 8.014293286613842....\n",
      "Training loss: 8.01399512417983....\n",
      "Training loss: 8.016672363412246....\n",
      "Training loss: 8.018054288737881....\n",
      "Training loss: 8.021978382280048....\n",
      "Training loss: 8.01695625949027....\n",
      "Training loss: 8.018877544285045....\n",
      "Training loss: 8.015615675246437....\n",
      "Training loss: 8.01106981166389....\n",
      "Training loss: 8.015970185673345....\n",
      "Training loss: 8.016695290504162....\n",
      "Training loss: 8.016417084026049....\n",
      "Training loss: 8.016615456443178....\n",
      "Training loss: 8.01360284905058....\n",
      "Training loss: 8.014615461721712....\n",
      "Training loss: 8.014342895684399....\n",
      "Training loss: 8.012830675611713....\n",
      "Training loss: 8.017950171007467....\n",
      "Training loss: 8.018587851300646....\n",
      "Training loss: 8.015058916342463....\n",
      "Training loss: 8.018498527512964....\n",
      "Training loss: 8.010245272981352....\n",
      "Training loss: 8.014666505335702....\n",
      "Training loss: 8.015344067480314....\n",
      "Training loss: 8.018228741644831....\n",
      "Training loss: 8.019189160078362....\n",
      "Training loss: 8.014718333568158....\n",
      "Training loss: 8.018064323999415....\n",
      "Training loss: 8.015594760435281....\n",
      "Training loss: 8.017833261939954....\n",
      "Training loss: 8.016735415379843....\n",
      "Training loss: 8.014887280491712....\n",
      "Training loss: 8.016669103517623....\n",
      "Training loss: 8.016268081789809....\n",
      "Training loss: 8.012848394370316....\n",
      "Training loss: 8.013628267275358....\n",
      "Training loss: 8.010609838439475....\n",
      "Training loss: 8.018022427070626....\n",
      "Training loss: 8.015900501008202....\n",
      "Training loss: 8.01598200855822....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.011079013679625....\n",
      "Training loss: 8.013343738995184....\n",
      "Training loss: 8.014174643459171....\n",
      "Training loss: 8.016018551354463....\n",
      "Training loss: 8.01813943859955....\n",
      "Training loss: 8.018348770018846....\n",
      "Training loss: 8.012910010581802....\n",
      "Training loss: 8.014672665758857....\n",
      "Training loss: 8.013953718488857....\n",
      "Training loss: 8.013647817422392....\n",
      "Training loss: 8.013168025224166....\n",
      "Training loss: 8.016262595736745....\n",
      "Training loss: 8.018644715762894....\n",
      "Training loss: 8.014826725829407....\n",
      "Training loss: 8.014210079470374....\n",
      "Training loss: 8.012999977102274....\n",
      "Training loss: 8.013829207254657....\n",
      "Training loss: 8.012736421753175....\n",
      "Training loss: 8.014539584191805....\n",
      "Training loss: 8.014231521311343....\n",
      "Training loss: 8.010466725173263....\n",
      "Training loss: 8.016527855457195....\n",
      "Training loss: 8.015747796916555....\n",
      "Training loss: 8.013858923584406....\n",
      "Training loss: 8.015653354279454....\n",
      "Training loss: 8.017176687769705....\n",
      "Training loss: 8.013421410406771....\n",
      "Training loss: 8.01040578476956....\n"
     ]
    }
   ],
   "source": [
    "W2V = NN(0.01)\n",
    "W2V.add_layer(100,3125,'relu')\n",
    "W2V.add_layer(3125,100,'softmax')\n",
    "j=1\n",
    "for e in range (20):\n",
    "    print(\"###########epoch#######\", j )\n",
    "    for i in range(0, len(titles_corpus), 3000):\n",
    "        if(i+3000> len(titles_corpus)):\n",
    "            training= get_training_data(titles_corpus[i:len(titles_corpus)], 3)\n",
    "        else:\n",
    "            training= get_training_data(titles_corpus[i:i+3000], 3)\n",
    "        X_t=training[:,0,:]\n",
    "        y_t=training[:,1,:]\n",
    "        X_t_arr=np.array(X_t).T\n",
    "        Y_t_arr=np.array(y_t).T\n",
    "        W2V.fit(X_t_arr, Y_t_arr)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zs1sJX1OfETi",
    "outputId": "e705ddec-1dc8-49dd-9476-5e744fe95619"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "W2V.save_weights(\"Emb_Weights.model\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Neural Network (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
