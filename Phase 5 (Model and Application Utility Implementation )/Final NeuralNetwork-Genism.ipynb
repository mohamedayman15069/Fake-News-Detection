{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "def _onehot(a,M):\n",
    "    b = np.zeros( (a.size, M), dtype='int')\n",
    "    b[ np.arange(a.size),a] = 1\n",
    "    return b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all= pd.read_csv('All_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Label']=df_all['2_way_label']\n",
    "df_all['clean_title']=df_all['clean_title'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_all.clean_title, df_all.Label, test_size = 0.25, random_state = 42)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, test_size = 0.15, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from gensim.models import Word2Vec\n",
    "titles_corpus= []\n",
    "for title in df_all[\"dic_words\"][:]:\n",
    "    new_list = list(ast.literal_eval(title).keys())\n",
    "    if (new_list != []):\n",
    "        titles_corpus.append(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=titles_corpus, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16377747058868408, 0.5647750496864319, -0.41527968645095825, -0.11676723510026932, 0.13857972621917725, -0.3116781413555145, -0.3385057747364044, 0.43415331840515137, -0.4088789224624634, -0.3996856212615967, -0.20836211740970612, -0.39092937111854553, -0.03864704817533493, 0.1385619342327118, 0.10820507258176804, -0.2131553292274475, -0.3251692056655884, -0.36633414030075073, -0.21149718761444092, 0.08362990617752075, -0.11325021833181381, 0.14917828142642975, 0.6386159658432007, 0.3253081440925598, 0.1623195856809616, -0.312885582447052, 0.037734195590019226, 0.2604628801345825, -0.04247182235121727, 0.07244925200939178, -0.18588440120220184, 0.2482188493013382, -0.17883893847465515, -0.07369343936443329, -0.16635508835315704, 0.047638796269893646, -0.05068597570061684, -0.39901313185691833, -0.02108852192759514, -0.33227282762527466, 0.2622203230857849, -0.32665762305259705, 0.08170212805271149, -0.02038295939564705, 0.2346808910369873, -0.3373073935508728, -0.362561970949173, 0.7413454055786133, 0.4004182815551758, 0.19269053637981415, 0.011260920204222202, 0.2203473150730133, 0.2689506709575653, 0.18170364201068878, -0.2361200749874115, 0.15676528215408325, 0.20845752954483032, -0.307780921459198, -0.23932190239429474, 0.19003476202487946, -0.8464305996894836, -0.05499236285686493, 0.3329179883003235, -0.4753744900226593, -0.3951995074748993, 0.07641322165727615, 0.05986243113875389, 0.33367422223091125, -0.4020196497440338, -0.11564411967992783, -0.4646027386188507, -0.22765961289405823, 0.12379620224237442, -0.11088909208774567, 0.4402633607387543, 0.07597138732671738, -0.04117153584957123, 0.05331771820783615, 0.14472532272338867, -0.4503725469112396, -0.5693035125732422, -0.11237142235040665, -0.27817046642303467, 0.21069182455539703, -0.28612929582595825, -0.3507012128829956, 0.34766751527786255, 0.2807310223579407, 0.2640424072742462, 0.20106513798236847, 0.3924221098423004, 0.4384964406490326, -0.4632840156555176, 0.16497166454792023, 0.08831718564033508, 0.4733852744102478, 0.018301300704479218, 0.025286750867962837, 0.1776878535747528, -0.5046623349189758]\n"
     ]
    }
   ],
   "source": [
    "train_data= pd.DataFrame()\n",
    "train_data['Word_list']= X_t\n",
    "idx = X_t.index[0]\n",
    "with open('train_word2vec.csv', 'w+') as word2vec_file:\n",
    "    for index, row in train_data.iterrows():\n",
    "        model_vector = (np.mean([model.wv[token] for token in row['Word_list']], axis=0)).tolist()\n",
    "        if index == idx:\n",
    "            header = \",\".join(str(ele) for ele in range(100))\n",
    "            word2vec_file.write(header)\n",
    "            word2vec_file.write(\"\\n\")\n",
    "\n",
    "        if type(model_vector) is list:  \n",
    "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        else:\n",
    "            line1 = \",\".join([str(0) for i in range(100)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(model_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.026493</td>\n",
       "      <td>0.273085</td>\n",
       "      <td>-0.105175</td>\n",
       "      <td>0.175692</td>\n",
       "      <td>0.204757</td>\n",
       "      <td>-0.144984</td>\n",
       "      <td>0.066037</td>\n",
       "      <td>0.453178</td>\n",
       "      <td>-0.128841</td>\n",
       "      <td>-0.222718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>0.163270</td>\n",
       "      <td>-0.118314</td>\n",
       "      <td>0.124291</td>\n",
       "      <td>-0.035473</td>\n",
       "      <td>0.135202</td>\n",
       "      <td>0.173883</td>\n",
       "      <td>-0.210744</td>\n",
       "      <td>0.046654</td>\n",
       "      <td>0.256264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024092</td>\n",
       "      <td>0.140935</td>\n",
       "      <td>-0.062945</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>0.312014</td>\n",
       "      <td>-0.060696</td>\n",
       "      <td>0.095916</td>\n",
       "      <td>0.243580</td>\n",
       "      <td>-0.172785</td>\n",
       "      <td>-0.185306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618464</td>\n",
       "      <td>0.131715</td>\n",
       "      <td>0.296515</td>\n",
       "      <td>-0.024992</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>0.054846</td>\n",
       "      <td>0.376982</td>\n",
       "      <td>0.041725</td>\n",
       "      <td>0.088840</td>\n",
       "      <td>-0.045345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.208747</td>\n",
       "      <td>0.211303</td>\n",
       "      <td>-0.036446</td>\n",
       "      <td>-0.009708</td>\n",
       "      <td>-0.344165</td>\n",
       "      <td>0.084931</td>\n",
       "      <td>0.065079</td>\n",
       "      <td>0.544420</td>\n",
       "      <td>-0.175978</td>\n",
       "      <td>0.071592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283547</td>\n",
       "      <td>-0.402061</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.159023</td>\n",
       "      <td>0.383454</td>\n",
       "      <td>0.454002</td>\n",
       "      <td>0.144163</td>\n",
       "      <td>-0.295373</td>\n",
       "      <td>-0.343769</td>\n",
       "      <td>-0.029811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127795</td>\n",
       "      <td>0.200223</td>\n",
       "      <td>0.206196</td>\n",
       "      <td>0.047698</td>\n",
       "      <td>0.074942</td>\n",
       "      <td>-0.006958</td>\n",
       "      <td>0.211252</td>\n",
       "      <td>0.717210</td>\n",
       "      <td>0.153937</td>\n",
       "      <td>0.065161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379454</td>\n",
       "      <td>0.328201</td>\n",
       "      <td>0.216266</td>\n",
       "      <td>-0.132629</td>\n",
       "      <td>0.143391</td>\n",
       "      <td>0.482718</td>\n",
       "      <td>-0.023113</td>\n",
       "      <td>-0.116327</td>\n",
       "      <td>-0.191275</td>\n",
       "      <td>-0.181619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.054545</td>\n",
       "      <td>-0.126831</td>\n",
       "      <td>-0.056408</td>\n",
       "      <td>-0.187740</td>\n",
       "      <td>-0.180972</td>\n",
       "      <td>-0.518790</td>\n",
       "      <td>-0.005222</td>\n",
       "      <td>0.545287</td>\n",
       "      <td>-0.514759</td>\n",
       "      <td>-0.070856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019798</td>\n",
       "      <td>-0.112812</td>\n",
       "      <td>0.342333</td>\n",
       "      <td>-0.527020</td>\n",
       "      <td>0.624489</td>\n",
       "      <td>0.507466</td>\n",
       "      <td>0.136599</td>\n",
       "      <td>0.059571</td>\n",
       "      <td>0.104774</td>\n",
       "      <td>-0.090414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340556</th>\n",
       "      <td>0.067089</td>\n",
       "      <td>0.111656</td>\n",
       "      <td>0.244485</td>\n",
       "      <td>-0.121622</td>\n",
       "      <td>-0.026950</td>\n",
       "      <td>-0.337015</td>\n",
       "      <td>0.167462</td>\n",
       "      <td>0.302599</td>\n",
       "      <td>-0.163911</td>\n",
       "      <td>-0.083493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150980</td>\n",
       "      <td>-0.062407</td>\n",
       "      <td>-0.065812</td>\n",
       "      <td>-0.074185</td>\n",
       "      <td>0.311019</td>\n",
       "      <td>0.230113</td>\n",
       "      <td>0.179053</td>\n",
       "      <td>-0.080357</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>-0.165776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340557</th>\n",
       "      <td>-0.118440</td>\n",
       "      <td>-0.029005</td>\n",
       "      <td>-0.097191</td>\n",
       "      <td>-0.082335</td>\n",
       "      <td>-0.014213</td>\n",
       "      <td>-0.298692</td>\n",
       "      <td>-0.024797</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.154666</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077871</td>\n",
       "      <td>0.013160</td>\n",
       "      <td>-0.072245</td>\n",
       "      <td>-0.064300</td>\n",
       "      <td>0.596340</td>\n",
       "      <td>0.479162</td>\n",
       "      <td>-0.104697</td>\n",
       "      <td>-0.192866</td>\n",
       "      <td>0.083632</td>\n",
       "      <td>0.128778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340558</th>\n",
       "      <td>0.008359</td>\n",
       "      <td>0.292776</td>\n",
       "      <td>-0.077243</td>\n",
       "      <td>0.026018</td>\n",
       "      <td>0.124446</td>\n",
       "      <td>-0.455630</td>\n",
       "      <td>0.035077</td>\n",
       "      <td>0.376063</td>\n",
       "      <td>-0.215522</td>\n",
       "      <td>-0.313887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441227</td>\n",
       "      <td>0.200245</td>\n",
       "      <td>-0.067028</td>\n",
       "      <td>-0.389513</td>\n",
       "      <td>0.351782</td>\n",
       "      <td>0.006825</td>\n",
       "      <td>0.130678</td>\n",
       "      <td>-0.041800</td>\n",
       "      <td>0.297767</td>\n",
       "      <td>-0.283298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340559</th>\n",
       "      <td>-0.080126</td>\n",
       "      <td>0.108359</td>\n",
       "      <td>-0.026144</td>\n",
       "      <td>-0.198178</td>\n",
       "      <td>-0.015043</td>\n",
       "      <td>-0.206091</td>\n",
       "      <td>0.149321</td>\n",
       "      <td>0.284106</td>\n",
       "      <td>-0.162157</td>\n",
       "      <td>0.047583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342904</td>\n",
       "      <td>0.063289</td>\n",
       "      <td>-0.104586</td>\n",
       "      <td>0.032897</td>\n",
       "      <td>0.555939</td>\n",
       "      <td>0.234286</td>\n",
       "      <td>0.196779</td>\n",
       "      <td>-0.200543</td>\n",
       "      <td>-0.073709</td>\n",
       "      <td>-0.203434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340560</th>\n",
       "      <td>0.163777</td>\n",
       "      <td>0.564775</td>\n",
       "      <td>-0.415280</td>\n",
       "      <td>-0.116767</td>\n",
       "      <td>0.138580</td>\n",
       "      <td>-0.311678</td>\n",
       "      <td>-0.338506</td>\n",
       "      <td>0.434153</td>\n",
       "      <td>-0.408879</td>\n",
       "      <td>-0.399686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392422</td>\n",
       "      <td>0.438496</td>\n",
       "      <td>-0.463284</td>\n",
       "      <td>0.164972</td>\n",
       "      <td>0.088317</td>\n",
       "      <td>0.473385</td>\n",
       "      <td>0.018301</td>\n",
       "      <td>0.025287</td>\n",
       "      <td>0.177688</td>\n",
       "      <td>-0.504662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340561 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0      -0.026493  0.273085 -0.105175  0.175692  0.204757 -0.144984  0.066037   \n",
       "1       0.024092  0.140935 -0.062945  0.003806  0.312014 -0.060696  0.095916   \n",
       "2      -0.208747  0.211303 -0.036446 -0.009708 -0.344165  0.084931  0.065079   \n",
       "3       0.127795  0.200223  0.206196  0.047698  0.074942 -0.006958  0.211252   \n",
       "4      -0.054545 -0.126831 -0.056408 -0.187740 -0.180972 -0.518790 -0.005222   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "340556  0.067089  0.111656  0.244485 -0.121622 -0.026950 -0.337015  0.167462   \n",
       "340557 -0.118440 -0.029005 -0.097191 -0.082335 -0.014213 -0.298692 -0.024797   \n",
       "340558  0.008359  0.292776 -0.077243  0.026018  0.124446 -0.455630  0.035077   \n",
       "340559 -0.080126  0.108359 -0.026144 -0.198178 -0.015043 -0.206091  0.149321   \n",
       "340560  0.163777  0.564775 -0.415280 -0.116767  0.138580 -0.311678 -0.338506   \n",
       "\n",
       "               7         8         9  ...        90        91        92  \\\n",
       "0       0.453178 -0.128841 -0.222718  ...  0.104533  0.163270 -0.118314   \n",
       "1       0.243580 -0.172785 -0.185306  ...  0.618464  0.131715  0.296515   \n",
       "2       0.544420 -0.175978  0.071592  ...  0.283547 -0.402061  0.003822   \n",
       "3       0.717210  0.153937  0.065161  ...  0.379454  0.328201  0.216266   \n",
       "4       0.545287 -0.514759 -0.070856  ... -0.019798 -0.112812  0.342333   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "340556  0.302599 -0.163911 -0.083493  ...  0.150980 -0.062407 -0.065812   \n",
       "340557  0.027542  0.154666  0.022142  ...  0.077871  0.013160 -0.072245   \n",
       "340558  0.376063 -0.215522 -0.313887  ...  0.441227  0.200245 -0.067028   \n",
       "340559  0.284106 -0.162157  0.047583  ...  0.342904  0.063289 -0.104586   \n",
       "340560  0.434153 -0.408879 -0.399686  ...  0.392422  0.438496 -0.463284   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "0       0.124291 -0.035473  0.135202  0.173883 -0.210744  0.046654  0.256264  \n",
       "1      -0.024992  0.025590  0.054846  0.376982  0.041725  0.088840 -0.045345  \n",
       "2       0.159023  0.383454  0.454002  0.144163 -0.295373 -0.343769 -0.029811  \n",
       "3      -0.132629  0.143391  0.482718 -0.023113 -0.116327 -0.191275 -0.181619  \n",
       "4      -0.527020  0.624489  0.507466  0.136599  0.059571  0.104774 -0.090414  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "340556 -0.074185  0.311019  0.230113  0.179053 -0.080357  0.006604 -0.165776  \n",
       "340557 -0.064300  0.596340  0.479162 -0.104697 -0.192866  0.083632  0.128778  \n",
       "340558 -0.389513  0.351782  0.006825  0.130678 -0.041800  0.297767 -0.283298  \n",
       "340559  0.032897  0.555939  0.234286  0.196779 -0.200543 -0.073709 -0.203434  \n",
       "340560  0.164972  0.088317  0.473385  0.018301  0.025287  0.177688 -0.504662  \n",
       "\n",
       "[340561 rows x 100 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2f_df = pd.read_csv('train_word2vec.csv')\n",
    "w2f_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= pd.DataFrame()\n",
    "test_data['Word_list']= X_test\n",
    "idx = X_test.index[0]\n",
    "with open('test_word2vec.csv', 'w+') as word2vec_file:\n",
    "    for index, row in test_data.iterrows():\n",
    "        model_vector = (np.mean([model.wv[token] for token in row['Word_list']], axis=0)).tolist()\n",
    "        if index == idx:\n",
    "            header = \",\".join(str(ele) for ele in range(100))\n",
    "            word2vec_file.write(header)\n",
    "            word2vec_file.write(\"\\n\")\n",
    "\n",
    "        if type(model_vector) is list:  \n",
    "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        else:\n",
    "            line1 = \",\".join([str(0) for i in range(100)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133554, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2f_test_df = pd.read_csv('test_word2vec.csv')\n",
    "w2f_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= pd.DataFrame()\n",
    "test_data['Word_list']= X_val\n",
    "idx = X_val.index[0]\n",
    "with open('val_word2vec.csv', 'w+') as word2vec_file:\n",
    "    for index, row in test_data.iterrows():\n",
    "        model_vector = (np.mean([model.wv[token] for token in row['Word_list']], axis=0)).tolist()\n",
    "        if index == idx:\n",
    "            header = \",\".join(str(ele) for ele in range(100))\n",
    "            word2vec_file.write(header)\n",
    "            word2vec_file.write(\"\\n\")\n",
    "\n",
    "        if type(model_vector) is list:  \n",
    "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        else:\n",
    "            line1 = \",\".join([str(0) for i in range(100)])\n",
    "        word2vec_file.write(line1)\n",
    "        word2vec_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067259</td>\n",
       "      <td>0.061630</td>\n",
       "      <td>-0.029880</td>\n",
       "      <td>-0.131169</td>\n",
       "      <td>-0.143528</td>\n",
       "      <td>-0.052643</td>\n",
       "      <td>0.175737</td>\n",
       "      <td>0.097219</td>\n",
       "      <td>-0.051603</td>\n",
       "      <td>-0.362285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319570</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>0.241056</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.201430</td>\n",
       "      <td>0.286426</td>\n",
       "      <td>0.355454</td>\n",
       "      <td>-0.195918</td>\n",
       "      <td>-0.160301</td>\n",
       "      <td>-0.394020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.186749</td>\n",
       "      <td>0.222959</td>\n",
       "      <td>0.223848</td>\n",
       "      <td>-0.327336</td>\n",
       "      <td>-0.220277</td>\n",
       "      <td>-0.627499</td>\n",
       "      <td>0.038212</td>\n",
       "      <td>0.605497</td>\n",
       "      <td>-0.267189</td>\n",
       "      <td>-0.387551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211565</td>\n",
       "      <td>0.307197</td>\n",
       "      <td>-0.173068</td>\n",
       "      <td>-0.080777</td>\n",
       "      <td>0.761661</td>\n",
       "      <td>-0.229842</td>\n",
       "      <td>0.195148</td>\n",
       "      <td>-0.522059</td>\n",
       "      <td>-0.269665</td>\n",
       "      <td>0.071044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.085390</td>\n",
       "      <td>0.331885</td>\n",
       "      <td>0.102759</td>\n",
       "      <td>-0.177292</td>\n",
       "      <td>0.173854</td>\n",
       "      <td>0.043886</td>\n",
       "      <td>0.204166</td>\n",
       "      <td>0.013307</td>\n",
       "      <td>0.041163</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019850</td>\n",
       "      <td>0.209151</td>\n",
       "      <td>0.396242</td>\n",
       "      <td>-0.014598</td>\n",
       "      <td>-0.161618</td>\n",
       "      <td>0.241337</td>\n",
       "      <td>-0.154291</td>\n",
       "      <td>-0.132980</td>\n",
       "      <td>0.164133</td>\n",
       "      <td>-0.510176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.079577</td>\n",
       "      <td>0.123893</td>\n",
       "      <td>0.212828</td>\n",
       "      <td>-0.126244</td>\n",
       "      <td>-0.054118</td>\n",
       "      <td>-0.313136</td>\n",
       "      <td>0.156757</td>\n",
       "      <td>0.274160</td>\n",
       "      <td>-0.099461</td>\n",
       "      <td>-0.028277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097735</td>\n",
       "      <td>0.102612</td>\n",
       "      <td>-0.152047</td>\n",
       "      <td>-0.267500</td>\n",
       "      <td>0.449232</td>\n",
       "      <td>0.288441</td>\n",
       "      <td>0.243956</td>\n",
       "      <td>-0.045505</td>\n",
       "      <td>0.163987</td>\n",
       "      <td>-0.170189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.297763</td>\n",
       "      <td>0.524712</td>\n",
       "      <td>0.292426</td>\n",
       "      <td>0.094376</td>\n",
       "      <td>-0.346131</td>\n",
       "      <td>-0.280050</td>\n",
       "      <td>0.284786</td>\n",
       "      <td>0.464078</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.366159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189114</td>\n",
       "      <td>0.297404</td>\n",
       "      <td>-0.104970</td>\n",
       "      <td>-0.125647</td>\n",
       "      <td>-0.015946</td>\n",
       "      <td>0.085753</td>\n",
       "      <td>-0.414294</td>\n",
       "      <td>-0.257658</td>\n",
       "      <td>0.220791</td>\n",
       "      <td>-0.130250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60094</th>\n",
       "      <td>-0.309814</td>\n",
       "      <td>0.378096</td>\n",
       "      <td>-0.312121</td>\n",
       "      <td>0.099623</td>\n",
       "      <td>0.049205</td>\n",
       "      <td>-0.389214</td>\n",
       "      <td>0.120131</td>\n",
       "      <td>0.233321</td>\n",
       "      <td>-0.227019</td>\n",
       "      <td>-0.426761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539165</td>\n",
       "      <td>0.022750</td>\n",
       "      <td>-0.082081</td>\n",
       "      <td>-0.306407</td>\n",
       "      <td>0.394647</td>\n",
       "      <td>0.135770</td>\n",
       "      <td>0.568847</td>\n",
       "      <td>-0.160430</td>\n",
       "      <td>0.222288</td>\n",
       "      <td>-0.060794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60095</th>\n",
       "      <td>0.181023</td>\n",
       "      <td>0.125971</td>\n",
       "      <td>-0.139484</td>\n",
       "      <td>-0.241006</td>\n",
       "      <td>-0.307441</td>\n",
       "      <td>-0.482457</td>\n",
       "      <td>0.296401</td>\n",
       "      <td>0.073862</td>\n",
       "      <td>-0.151770</td>\n",
       "      <td>-0.144590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205218</td>\n",
       "      <td>0.107028</td>\n",
       "      <td>0.261340</td>\n",
       "      <td>-0.188674</td>\n",
       "      <td>0.225126</td>\n",
       "      <td>-0.236431</td>\n",
       "      <td>0.403017</td>\n",
       "      <td>-0.305010</td>\n",
       "      <td>-0.171789</td>\n",
       "      <td>0.060446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60096</th>\n",
       "      <td>0.382867</td>\n",
       "      <td>0.173068</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>-0.371700</td>\n",
       "      <td>-0.130959</td>\n",
       "      <td>-0.290055</td>\n",
       "      <td>0.363032</td>\n",
       "      <td>0.301583</td>\n",
       "      <td>0.032835</td>\n",
       "      <td>-0.092692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056357</td>\n",
       "      <td>-0.185071</td>\n",
       "      <td>-0.055577</td>\n",
       "      <td>-0.058568</td>\n",
       "      <td>0.203204</td>\n",
       "      <td>0.372233</td>\n",
       "      <td>-0.052497</td>\n",
       "      <td>-0.112783</td>\n",
       "      <td>-0.016516</td>\n",
       "      <td>-0.344099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60097</th>\n",
       "      <td>0.138738</td>\n",
       "      <td>0.108645</td>\n",
       "      <td>-0.255388</td>\n",
       "      <td>-0.021816</td>\n",
       "      <td>0.053958</td>\n",
       "      <td>-0.458919</td>\n",
       "      <td>-0.213953</td>\n",
       "      <td>0.368486</td>\n",
       "      <td>0.147522</td>\n",
       "      <td>0.072289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311067</td>\n",
       "      <td>0.047907</td>\n",
       "      <td>0.190139</td>\n",
       "      <td>-0.106074</td>\n",
       "      <td>-0.137711</td>\n",
       "      <td>0.158046</td>\n",
       "      <td>-0.093010</td>\n",
       "      <td>-0.103864</td>\n",
       "      <td>-0.061820</td>\n",
       "      <td>-0.375735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60098</th>\n",
       "      <td>-0.237081</td>\n",
       "      <td>0.423911</td>\n",
       "      <td>-0.372480</td>\n",
       "      <td>-0.121001</td>\n",
       "      <td>0.026011</td>\n",
       "      <td>-0.031819</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.163602</td>\n",
       "      <td>-0.241927</td>\n",
       "      <td>-0.005770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513110</td>\n",
       "      <td>0.207174</td>\n",
       "      <td>0.025247</td>\n",
       "      <td>-0.003976</td>\n",
       "      <td>0.295600</td>\n",
       "      <td>-0.282547</td>\n",
       "      <td>-0.163481</td>\n",
       "      <td>-0.061622</td>\n",
       "      <td>-0.237423</td>\n",
       "      <td>-0.094811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60099 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.067259  0.061630 -0.029880 -0.131169 -0.143528 -0.052643  0.175737   \n",
       "1     -0.186749  0.222959  0.223848 -0.327336 -0.220277 -0.627499  0.038212   \n",
       "2     -0.085390  0.331885  0.102759 -0.177292  0.173854  0.043886  0.204166   \n",
       "3     -0.079577  0.123893  0.212828 -0.126244 -0.054118 -0.313136  0.156757   \n",
       "4      0.297763  0.524712  0.292426  0.094376 -0.346131 -0.280050  0.284786   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "60094 -0.309814  0.378096 -0.312121  0.099623  0.049205 -0.389214  0.120131   \n",
       "60095  0.181023  0.125971 -0.139484 -0.241006 -0.307441 -0.482457  0.296401   \n",
       "60096  0.382867  0.173068  0.006572 -0.371700 -0.130959 -0.290055  0.363032   \n",
       "60097  0.138738  0.108645 -0.255388 -0.021816  0.053958 -0.458919 -0.213953   \n",
       "60098 -0.237081  0.423911 -0.372480 -0.121001  0.026011 -0.031819  0.001510   \n",
       "\n",
       "              7         8         9  ...        90        91        92  \\\n",
       "0      0.097219 -0.051603 -0.362285  ...  0.319570 -0.002309  0.241056   \n",
       "1      0.605497 -0.267189 -0.387551  ...  0.211565  0.307197 -0.173068   \n",
       "2      0.013307  0.041163  0.012500  ...  0.019850  0.209151  0.396242   \n",
       "3      0.274160 -0.099461 -0.028277  ... -0.097735  0.102612 -0.152047   \n",
       "4      0.464078  0.011330 -0.366159  ...  0.189114  0.297404 -0.104970   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "60094  0.233321 -0.227019 -0.426761  ...  0.539165  0.022750 -0.082081   \n",
       "60095  0.073862 -0.151770 -0.144590  ...  0.205218  0.107028  0.261340   \n",
       "60096  0.301583  0.032835 -0.092692  ...  0.056357 -0.185071 -0.055577   \n",
       "60097  0.368486  0.147522  0.072289  ...  0.311067  0.047907  0.190139   \n",
       "60098  0.163602 -0.241927 -0.005770  ...  0.513110  0.207174  0.025247   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0      0.002824  0.201430  0.286426  0.355454 -0.195918 -0.160301 -0.394020  \n",
       "1     -0.080777  0.761661 -0.229842  0.195148 -0.522059 -0.269665  0.071044  \n",
       "2     -0.014598 -0.161618  0.241337 -0.154291 -0.132980  0.164133 -0.510176  \n",
       "3     -0.267500  0.449232  0.288441  0.243956 -0.045505  0.163987 -0.170189  \n",
       "4     -0.125647 -0.015946  0.085753 -0.414294 -0.257658  0.220791 -0.130250  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "60094 -0.306407  0.394647  0.135770  0.568847 -0.160430  0.222288 -0.060794  \n",
       "60095 -0.188674  0.225126 -0.236431  0.403017 -0.305010 -0.171789  0.060446  \n",
       "60096 -0.058568  0.203204  0.372233 -0.052497 -0.112783 -0.016516 -0.344099  \n",
       "60097 -0.106074 -0.137711  0.158046 -0.093010 -0.103864 -0.061820 -0.375735  \n",
       "60098 -0.003976  0.295600 -0.282547 -0.163481 -0.061622 -0.237423 -0.094811  \n",
       "\n",
       "[60099 rows x 100 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2f_val_df = pd.read_csv('val_word2vec.csv')\n",
    "w2f_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340561, 100)\n",
      "(340561, 1)\n",
      "(133554, 100)\n",
      "(133554, 1)\n"
     ]
    }
   ],
   "source": [
    "y_tr=np.array(y_t)[np.newaxis].T\n",
    "y_t=np.array(y_test)[np.newaxis].T\n",
    "print(w2f_df.shape)\n",
    "print(y_tr.shape)\n",
    "print(w2f_test_df.shape)\n",
    "print(y_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = w2f_df.values\n",
    "X_test = w2f_test_df.values\n",
    "y_train = y_tr\n",
    "y_test = y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(340561, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 2\n",
    "train_size = y_train.shape[0]\n",
    "y_train = y_train.reshape(1, train_size)\n",
    "y_train = np.eye(classes)[y_train.astype('int32')-1]\n",
    "y_train = y_train.T.reshape(classes, train_size)\n",
    "\n",
    "test_size = y_test.shape[0]\n",
    "y_test = y_test.reshape(1, test_size)\n",
    "y_test = np.eye(classes)[y_test.astype('int32')-1]\n",
    "y_test= y_test.T.reshape(classes, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.T\n",
    "y_test=y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340561, 2)\n",
      "(133554, 2)\n",
      "(340561, 100)\n",
      "(133554, 100)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_units(num):\n",
    "    return (num - np.mean(num)/np.std(num))\n",
    "Scaled = standard_units(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00648817,  0.29308976, -0.08516996, ..., -0.19073929,\n",
       "         0.06665943,  0.27626891],\n",
       "       [ 0.04409733,  0.16094054, -0.04294035, ...,  0.06173003,\n",
       "         0.10884484, -0.02534014],\n",
       "       [-0.18874206,  0.23130817, -0.01644112, ..., -0.27536841,\n",
       "        -0.32376443, -0.00980579],\n",
       "       ...,\n",
       "       [ 0.02836454,  0.31278078, -0.05723837, ..., -0.0217949 ,\n",
       "         0.31777165, -0.26329299],\n",
       "       [-0.06012071,  0.12836419, -0.00613924, ..., -0.18053836,\n",
       "        -0.05370378, -0.18342927],\n",
       "       [ 0.18378253,  0.58478011, -0.39527463, ...,  0.04529181,\n",
       "         0.19769291, -0.48465728]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainn = X_train.copy()\n",
    "X_train = Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of hidden layers3\n"
     ]
    }
   ],
   "source": [
    "NH=int(input('Num of hidden layers'))\n",
    "n_o=2 \n",
    "n_i=X_train.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of nodes in  layers num 1: 50\n",
      "Num of nodes in  layers num 2: 25\n",
      "Num of nodes in  layers num 3: 4\n"
     ]
    }
   ],
   "source": [
    "NN=[]\n",
    "NN.append(n_i)\n",
    "for i in range(NH):\n",
    "    n=int(input('Num of nodes in  layers num %d: '%(i+1)))\n",
    "    NN.append(n)\n",
    "NN.append(n_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2f_val_df = np.array(w2f_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.array(y_val)[np.newaxis].T\n",
    "val_size = y_val.shape[1]\n",
    "y_val = y_val.reshape(1, val_size)\n",
    "y_val = np.eye(classes)[y_val.astype('int32')-1]\n",
    "y_val= y_val.T.reshape(classes, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60099, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(Z):\n",
    "    m=np.max(Z,axis=0).reshape(-1,1)\n",
    "    Z=Z-m.T\n",
    "#     print(Z.shape)\n",
    "    \n",
    "    A=np.exp(Z)/np.sum(np.exp(Z),axis=0,keepdims=True)\n",
    "#     print(f'A{ A.shape}')\n",
    "    return A \n",
    "\n",
    "def Relu(Z):\n",
    "    return np.maximum(Z,-0.01*Z)\n",
    "def initialize_layers(NN,NH):\n",
    "    Wts={}\n",
    "    for layer in range (1,len(NN)):\n",
    "        Wts['W'+str(layer)]=np.random.randn(NN[layer],NN[layer-1])*np.sqrt(2 / NN[layer-1]) \n",
    "        Wts['b'+str(layer)]=np.zeros((NN[layer],1)) \n",
    "    return Wts\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wts=initialize_layers(NN,NH)\n",
    "Wts['b1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward_Prob(Xt,Wts,NN):\n",
    "    parameters=[]\n",
    "    Ai=Xt \n",
    "   \n",
    "    Ap=Ai \n",
    "    for i in range(1,len(NN)-1):\n",
    "        W,b=Wts['W'+str(i)],Wts['b'+str(i)]\n",
    "        Ap=Ai\n",
    "        Z=np.dot(W,Ap.T)+b\n",
    "        actv=Relu(Z)\n",
    "        Ai=actv.T\n",
    "        parameters.append((W,b,Ap,actv,Z))\n",
    "    W,b=Wts['W'+str(len(NN)-1)],Wts['b'+str(len(NN)-1)]\n",
    "    Ap=Ai\n",
    "    Z=np.dot(W,Ap.T)+b\n",
    "    actv=Sigmoid(Z) \n",
    "    Ai=actv.T\n",
    "    \n",
    "    parameters.append((W,b,Ap,actv,Z))\n",
    "    Ao=Ai \n",
    "    return parameters, Ao \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yp(Ao):\n",
    "    y_p= np.array([(np.argmax(a)+1) for a in Ao]).reshape(-1,1)\n",
    "    return y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(y_true,prediction):\n",
    "    y_p=prediction\n",
    "    m=y_true.shape[0]\n",
    "    loss = -1/m*np.sum(np.multiply(y_true, np.log(y_p))+np.multiply(1-y_true, np.log(1-y_p)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Back_Prob(y_true, Ao, parameters,Wts,NN):\n",
    "    dZ =(Ao-y_true)\n",
    "    derivatives={}\n",
    "    m=y_true.shape[0]\n",
    "    W,b,X,A,Z=parameters[len(NN)-2]\n",
    "    \n",
    "    dW=1.0/m*np.dot(dZ.T,X)\n",
    "    db = (1/(m)) * np.sum(dZ, axis =0, keepdims = True).T\n",
    "    derivatives['dW'+str(len(NN)-1)]=dW\n",
    "    derivatives['db'+str(len(NN)-1)]=db\n",
    "    for i in reversed(range(len(NN)-2)):\n",
    "        Wp=W\n",
    "        Xp=X\n",
    "        #dA=dAp\n",
    "        W,b,X,A,Z=parameters[i]\n",
    "        dZp=dZ\n",
    "        grad=np.ones((A.shape))\n",
    "        grad[A<0]=-0.01\n",
    "        dZ=np.dot(dZp,Wp)*grad.T \n",
    "        m=X.shape[0]\n",
    "        dW=1.0/m*np.dot(dZ.T,X)\n",
    "        db = (1/(m)) * np.sum(dZ, axis =0, keepdims = True).T\n",
    "        derivatives['dW'+str(i+1)]=dW\n",
    "        derivatives['db'+str(i+1)]=db\n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(derivatives,Wts,parameters,learning_rate,NN):\n",
    "    for i in range(1,len(NN)):\n",
    "        Wts['b'+str(i)]=Wts['b'+str(i)]-learning_rate*derivatives['db'+str(i)]\n",
    "        Wts['W'+str(i)]=Wts['W'+str(i)]-learning_rate*derivatives['dW'+str(i)]\n",
    "    return Wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def Train_NN(iterations,X_train,y_train,NN,NH,learning_rate):\n",
    "    Wts=initialize_layers(NN,NH)\n",
    "    i=0\n",
    "    loss=1\n",
    "    losses=[]\n",
    "    val_losses=[]\n",
    "    while i< iterations:\n",
    "        parameters,Ao=Forward_Prob(X_train,Wts,NN)\n",
    "        loss=Loss(y_train,Ao)\n",
    "        print(\"Training Loss: \",loss)\n",
    "        derivatives=Back_Prob(y_train,Ao,parameters,Wts,NN)\n",
    "        Wts=update(derivatives,Wts,parameters,learning_rate,NN)\n",
    "        losses.append(loss)\n",
    "        parm_val,Ao_val = Forward_Prob(w2f_val_df,Wts,NN)\n",
    "        val_loss=Loss(y_val,Ao_val)\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"VAL LOSS: \", val_loss)\n",
    "        \n",
    "        i+=1\n",
    "    return (Wts,losses, val_losses)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(Wts,X_test):\n",
    "    parameters,predict=Forward_Prob(X_test,Wts,NN)\n",
    "    predict=np.array(predict)\n",
    "    print(predict.shape)\n",
    "    predictions=get_yp(predict)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _main(X_train):\n",
    "    it=200\n",
    "    eta=0.25\n",
    "    Wts,losses, val_losses=Train_NN(it,X_train,y_train,NN,NH,eta)\n",
    "    y_p=Predict(Wts,X_test)\n",
    "    yt=get_yp(y_test)\n",
    "    from sklearn.metrics import classification_report,average_precision_score,accuracy_score,recall_score,precision_score\n",
    "    ac=accuracy_score(yt,y_p)\n",
    "    pr=precision_score(yt,y_p,average=None)\n",
    "    rc=recall_score(yt,y_p,average=None)\n",
    "    print(classification_report(yt,y_p))\n",
    "    return (Wts,losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  1.5985433493419876\n",
      "VAL LOSS:  1.3862214615569755\n",
      "Training Loss:  1.3844836176192457\n",
      "VAL LOSS:  1.3821009755051494\n",
      "Training Loss:  1.3801471099034464\n",
      "VAL LOSS:  1.3788833298179681\n",
      "Training Loss:  1.3766286167311186\n",
      "VAL LOSS:  1.3761395653129938\n",
      "Training Loss:  1.3735223633171116\n",
      "VAL LOSS:  1.3734915534927756\n",
      "Training Loss:  1.3705433014215322\n",
      "VAL LOSS:  1.3707819585695062\n",
      "Training Loss:  1.367475668074367\n",
      "VAL LOSS:  1.3679376149369593\n",
      "Training Loss:  1.3642317711998997\n",
      "VAL LOSS:  1.3649205370326734\n",
      "Training Loss:  1.3607822786968597\n",
      "VAL LOSS:  1.3617050859665314\n",
      "Training Loss:  1.3570607664933307\n",
      "VAL LOSS:  1.3582724706402147\n",
      "Training Loss:  1.353083461039165\n",
      "VAL LOSS:  1.3546377656498834\n",
      "Training Loss:  1.348909231829662\n",
      "VAL LOSS:  1.3508734249105105\n",
      "Training Loss:  1.344561428165697\n",
      "VAL LOSS:  1.3469789375027224\n",
      "Training Loss:  1.3400753532346412\n",
      "VAL LOSS:  1.3430683661496825\n",
      "Training Loss:  1.3354774995167826\n",
      "VAL LOSS:  1.3391578622434337\n",
      "Training Loss:  1.330949622686958\n",
      "VAL LOSS:  1.335284150702457\n",
      "Training Loss:  1.3264797014616732\n",
      "VAL LOSS:  1.3315298534380584\n",
      "Training Loss:  1.3221908832280007\n",
      "VAL LOSS:  1.32793673449621\n",
      "Training Loss:  1.3181140912806544\n",
      "VAL LOSS:  1.3246242165963509\n",
      "Training Loss:  1.314313300744751\n",
      "VAL LOSS:  1.321548270307824\n",
      "Training Loss:  1.3108091075180797\n",
      "VAL LOSS:  1.3187789877588523\n",
      "Training Loss:  1.3076322027528224\n",
      "VAL LOSS:  1.316353690618884\n",
      "Training Loss:  1.3048214327162895\n",
      "VAL LOSS:  1.3142549165082278\n",
      "Training Loss:  1.3023877540003197\n",
      "VAL LOSS:  1.3124720007043165\n",
      "Training Loss:  1.3003575980420767\n",
      "VAL LOSS:  1.3110142500770505\n",
      "Training Loss:  1.2987330206320473\n",
      "VAL LOSS:  1.3099177395719805\n",
      "Training Loss:  1.297516729884937\n",
      "VAL LOSS:  1.3091661603040294\n",
      "Training Loss:  1.2966794374153667\n",
      "VAL LOSS:  1.308715381689479\n",
      "Training Loss:  1.2962185063070946\n",
      "VAL LOSS:  1.3085033272193751\n",
      "Training Loss:  1.296094573681898\n",
      "VAL LOSS:  1.308505785221563\n",
      "Training Loss:  1.2962625957165408\n",
      "VAL LOSS:  1.3087173343773189\n",
      "Training Loss:  1.296642992628554\n",
      "VAL LOSS:  1.3090705517345835\n",
      "Training Loss:  1.2972121527940164\n",
      "VAL LOSS:  1.3094897649602075\n",
      "Training Loss:  1.297922494937771\n",
      "VAL LOSS:  1.3099113551563397\n",
      "Training Loss:  1.2986407966639446\n",
      "VAL LOSS:  1.3102975030148627\n",
      "Training Loss:  1.299329795808134\n",
      "VAL LOSS:  1.3105873784006932\n",
      "Training Loss:  1.2999141410834054\n",
      "VAL LOSS:  1.3106729378875595\n",
      "Training Loss:  1.300345455808539\n",
      "VAL LOSS:  1.3104266288075272\n",
      "Training Loss:  1.30049026147488\n",
      "VAL LOSS:  1.309806354167229\n",
      "Training Loss:  1.3002915841321503\n",
      "VAL LOSS:  1.308713894312761\n",
      "Training Loss:  1.2996971082227147\n",
      "VAL LOSS:  1.3070846446129911\n",
      "Training Loss:  1.2986192953040876\n",
      "VAL LOSS:  1.3048043206303905\n",
      "Training Loss:  1.297025320050999\n",
      "VAL LOSS:  1.3017176664740413\n",
      "Training Loss:  1.2948929845474593\n",
      "VAL LOSS:  1.2978183115987556\n",
      "Training Loss:  1.2922881926873506\n",
      "VAL LOSS:  1.29312641320049\n",
      "Training Loss:  1.2893880837510807\n",
      "VAL LOSS:  1.2880805496399559\n",
      "Training Loss:  1.2864252376959264\n",
      "VAL LOSS:  1.2829900908390806\n",
      "Training Loss:  1.2835895552564616\n",
      "VAL LOSS:  1.2781120459264292\n",
      "Training Loss:  1.2809704266690323\n",
      "VAL LOSS:  1.2737473900490905\n",
      "Training Loss:  1.278447312207391\n",
      "VAL LOSS:  1.2698324589141772\n",
      "Training Loss:  1.2758257021858497\n",
      "VAL LOSS:  1.266016421134978\n",
      "Training Loss:  1.2731206444684362\n",
      "VAL LOSS:  1.2622789573527573\n",
      "Training Loss:  1.270114159445918\n",
      "VAL LOSS:  1.2587905734274976\n",
      "Training Loss:  1.2669235086854893\n",
      "VAL LOSS:  1.2554304425824878\n",
      "Training Loss:  1.2636070655338965\n",
      "VAL LOSS:  1.25208665539867\n",
      "Training Loss:  1.2602452441711007\n",
      "VAL LOSS:  1.248903860680982\n",
      "Training Loss:  1.2569086119067898\n",
      "VAL LOSS:  1.2458464724629883\n",
      "Training Loss:  1.2535146269052653\n",
      "VAL LOSS:  1.2428248150725174\n",
      "Training Loss:  1.250086430603515\n",
      "VAL LOSS:  1.2400636310184332\n",
      "Training Loss:  1.2466497440887543\n",
      "VAL LOSS:  1.2373923216802196\n",
      "Training Loss:  1.243307208296095\n",
      "VAL LOSS:  1.2348767609440443\n",
      "Training Loss:  1.2399842913793921\n",
      "VAL LOSS:  1.232327740341201\n",
      "Training Loss:  1.236866704521007\n",
      "VAL LOSS:  1.2301730379896734\n",
      "Training Loss:  1.233712268732869\n",
      "VAL LOSS:  1.2277561562556194\n",
      "Training Loss:  1.2309150407837195\n",
      "VAL LOSS:  1.2259883243777399\n",
      "Training Loss:  1.2279552319068407\n",
      "VAL LOSS:  1.2234926130431372\n",
      "Training Loss:  1.2254779292161768\n",
      "VAL LOSS:  1.2223549434135195\n",
      "Training Loss:  1.22268605116351\n",
      "VAL LOSS:  1.2194924324720036\n",
      "Training Loss:  1.2206576572539978\n",
      "VAL LOSS:  1.2194359313343015\n",
      "Training Loss:  1.218001715227675\n",
      "VAL LOSS:  1.215309531432505\n",
      "Training Loss:  1.2164740252347068\n",
      "VAL LOSS:  1.2175795217620196\n",
      "Training Loss:  1.2139290877959021\n",
      "VAL LOSS:  1.210793291554955\n",
      "Training Loss:  1.2131319813915238\n",
      "VAL LOSS:  1.217725091802254\n",
      "Training Loss:  1.2109121360852653\n",
      "VAL LOSS:  1.205799706231391\n",
      "Training Loss:  1.211460991595943\n",
      "VAL LOSS:  1.2221352735504845\n",
      "Training Loss:  1.2105880777168199\n",
      "VAL LOSS:  1.2015066741979412\n",
      "Training Loss:  1.214672840449188\n",
      "VAL LOSS:  1.2374783139185126\n",
      "Training Loss:  1.2188733684114625\n",
      "VAL LOSS:  1.204692883285898\n",
      "Training Loss:  1.2320076826748432\n",
      "VAL LOSS:  1.2744434547014336\n",
      "Training Loss:  1.2460722837255875\n",
      "VAL LOSS:  1.2268737548339672\n",
      "Training Loss:  1.2725464464465228\n",
      "VAL LOSS:  1.3130600565634425\n",
      "Training Loss:  1.2793948178293075\n",
      "VAL LOSS:  1.2311674200839255\n",
      "Training Loss:  1.2748162439155308\n",
      "VAL LOSS:  1.2830912911596055\n",
      "Training Loss:  1.255813476524242\n",
      "VAL LOSS:  1.2029556123147864\n",
      "Training Loss:  1.21898304918485\n",
      "VAL LOSS:  1.2359697636006857\n",
      "Training Loss:  1.2172710472133974\n",
      "VAL LOSS:  1.1980906376710616\n",
      "Training Loss:  1.2005938196425696\n",
      "VAL LOSS:  1.2162943540270312\n",
      "Training Loss:  1.2018495568311036\n",
      "VAL LOSS:  1.197285442824412\n",
      "Training Loss:  1.196030630124643\n",
      "VAL LOSS:  1.2078714164293312\n",
      "Training Loss:  1.196406087119389\n",
      "VAL LOSS:  1.1962346656103091\n",
      "Training Loss:  1.1938836329057243\n",
      "VAL LOSS:  1.2036210189126382\n",
      "Training Loss:  1.1939062859766068\n",
      "VAL LOSS:  1.1951691402333797\n",
      "Training Loss:  1.19247154908517\n",
      "VAL LOSS:  1.20111654871255\n",
      "Training Loss:  1.1923827453996778\n",
      "VAL LOSS:  1.194206454552694\n",
      "Training Loss:  1.1914023829121443\n",
      "VAL LOSS:  1.1994781128944885\n",
      "Training Loss:  1.1912867259656046\n",
      "VAL LOSS:  1.1933367977526208\n",
      "Training Loss:  1.1905275983371406\n",
      "VAL LOSS:  1.1983569732711683\n",
      "Training Loss:  1.19040135738006\n",
      "VAL LOSS:  1.192545775329411\n",
      "Training Loss:  1.189764279671101\n",
      "VAL LOSS:  1.1975274272636103\n",
      "Training Loss:  1.1896533398596472\n",
      "VAL LOSS:  1.1918247954420573\n",
      "Training Loss:  1.1890823901163814\n",
      "VAL LOSS:  1.196882537820695\n",
      "Training Loss:  1.1890042542493098\n",
      "VAL LOSS:  1.1911476732563033\n",
      "Training Loss:  1.1884815395618147\n",
      "VAL LOSS:  1.1963764891584416\n",
      "Training Loss:  1.1884293130295274\n",
      "VAL LOSS:  1.190505414233333\n",
      "Training Loss:  1.1879367318922784\n",
      "VAL LOSS:  1.1959784290797617\n",
      "Training Loss:  1.1879106511818076\n",
      "VAL LOSS:  1.1898919562701489\n",
      "Training Loss:  1.1874407555390636\n",
      "VAL LOSS:  1.1956615141020621\n",
      "Training Loss:  1.1874431371654333\n",
      "VAL LOSS:  1.189301512110794\n",
      "Training Loss:  1.1869830068474485\n",
      "VAL LOSS:  1.1954093987815726\n",
      "Training Loss:  1.1870181164712506\n",
      "VAL LOSS:  1.1887309881917745\n",
      "Training Loss:  1.186561699688621\n",
      "VAL LOSS:  1.195212924779968\n",
      "Training Loss:  1.1866341124206818\n",
      "VAL LOSS:  1.1881810737959495\n",
      "Training Loss:  1.1861866337308657\n",
      "VAL LOSS:  1.195070402262719\n",
      "Training Loss:  1.1862868861097398\n",
      "VAL LOSS:  1.187645567188503\n",
      "Training Loss:  1.1858459147922686\n",
      "VAL LOSS:  1.194975024149889\n",
      "Training Loss:  1.185971088063283\n",
      "VAL LOSS:  1.1871186734678358\n",
      "Training Loss:  1.1855362861615248\n",
      "VAL LOSS:  1.194925991530903\n",
      "Training Loss:  1.1856820328870674\n",
      "VAL LOSS:  1.1866056178326163\n",
      "Training Loss:  1.1852583352301043\n",
      "VAL LOSS:  1.1949154034242397\n",
      "Training Loss:  1.185423094875084\n",
      "VAL LOSS:  1.1861040397685834\n",
      "Training Loss:  1.1850095758619736\n",
      "VAL LOSS:  1.1949394985325268\n",
      "Training Loss:  1.185190120665364\n",
      "VAL LOSS:  1.1856130830949716\n",
      "Training Loss:  1.1847856081837664\n",
      "VAL LOSS:  1.194999749355407\n",
      "Training Loss:  1.1849823418913856\n",
      "VAL LOSS:  1.1851278079676386\n",
      "Training Loss:  1.184581071921149\n",
      "VAL LOSS:  1.1950955502439082\n",
      "Training Loss:  1.1847991625721133\n",
      "VAL LOSS:  1.184648507096551\n",
      "Training Loss:  1.184403728379994\n",
      "VAL LOSS:  1.1952272802646018\n",
      "Training Loss:  1.18463955775248\n",
      "VAL LOSS:  1.1841807976847187\n",
      "Training Loss:  1.1842510603463372\n",
      "VAL LOSS:  1.1953885696644388\n",
      "Training Loss:  1.184506204283685\n",
      "VAL LOSS:  1.1837336205255737\n",
      "Training Loss:  1.1841163020606857\n",
      "VAL LOSS:  1.1955722790859216\n",
      "Training Loss:  1.1843955141601847\n",
      "VAL LOSS:  1.18330053417521\n",
      "Training Loss:  1.1840005265399742\n",
      "VAL LOSS:  1.1957688784984335\n",
      "Training Loss:  1.1843020089385747\n",
      "VAL LOSS:  1.1828855570497994\n",
      "Training Loss:  1.1839004048021313\n",
      "VAL LOSS:  1.1959732466754445\n",
      "Training Loss:  1.1842242432373218\n",
      "VAL LOSS:  1.182486961207885\n",
      "Training Loss:  1.1838120150447664\n",
      "VAL LOSS:  1.1961824822886578\n",
      "Training Loss:  1.1841574701047641\n",
      "VAL LOSS:  1.1821078576642712\n",
      "Training Loss:  1.1837346007956353\n",
      "VAL LOSS:  1.1963857482756597\n",
      "Training Loss:  1.1841003062400866\n",
      "VAL LOSS:  1.1817511783989538\n",
      "Training Loss:  1.1836697121345674\n",
      "VAL LOSS:  1.196573973503033\n",
      "Training Loss:  1.1840472392880093\n",
      "VAL LOSS:  1.1814173615990358\n",
      "Training Loss:  1.1836090370665528\n",
      "VAL LOSS:  1.1967382179753805\n",
      "Training Loss:  1.183991127442765\n",
      "VAL LOSS:  1.1811084164530397\n",
      "Training Loss:  1.1835462999325475\n",
      "VAL LOSS:  1.1968702997851715\n",
      "Training Loss:  1.1839281229617356\n",
      "VAL LOSS:  1.180822342112684\n",
      "Training Loss:  1.1834778642644044\n",
      "VAL LOSS:  1.1969578107570333\n",
      "Training Loss:  1.1838520524616862\n",
      "VAL LOSS:  1.1805609521146152\n",
      "Training Loss:  1.183394701187273\n",
      "VAL LOSS:  1.1969946667012816\n",
      "Training Loss:  1.1837605599332708\n",
      "VAL LOSS:  1.1803198091552916\n",
      "Training Loss:  1.1832922832056882\n",
      "VAL LOSS:  1.1969819386783542\n",
      "Training Loss:  1.1836521120679637\n",
      "VAL LOSS:  1.1800958746160402\n",
      "Training Loss:  1.1831721452707489\n",
      "VAL LOSS:  1.1969161954057397\n",
      "Training Loss:  1.1835252816585993\n",
      "VAL LOSS:  1.1798864529254707\n",
      "Training Loss:  1.1830361297935619\n",
      "VAL LOSS:  1.1967989827609293\n",
      "Training Loss:  1.1833798346713518\n",
      "VAL LOSS:  1.1796917645238056\n",
      "Training Loss:  1.1828838121986214\n",
      "VAL LOSS:  1.196636251474943\n",
      "Training Loss:  1.1832156191711338\n",
      "VAL LOSS:  1.1795096236292273\n",
      "Training Loss:  1.1827165377145636\n",
      "VAL LOSS:  1.1964364635034943\n",
      "Training Loss:  1.1830350081306762\n",
      "VAL LOSS:  1.1793378731162705\n",
      "Training Loss:  1.182538201539234\n",
      "VAL LOSS:  1.196204332739245\n",
      "Training Loss:  1.1828433691621352\n",
      "VAL LOSS:  1.1791767106497812\n",
      "Training Loss:  1.1823507261855113\n",
      "VAL LOSS:  1.1959503516562904\n",
      "Training Loss:  1.1826452987273073\n",
      "VAL LOSS:  1.179024243974629\n",
      "Training Loss:  1.182158565902398\n",
      "VAL LOSS:  1.1956818888654108\n",
      "Training Loss:  1.1824424803043756\n",
      "VAL LOSS:  1.1788796433778763\n",
      "Training Loss:  1.1819630324787427\n",
      "VAL LOSS:  1.1954058922942579\n",
      "Training Loss:  1.182238572792977\n",
      "VAL LOSS:  1.1787407251195272\n",
      "Training Loss:  1.181767602269794\n",
      "VAL LOSS:  1.1951277560243103\n",
      "Training Loss:  1.18203558584015\n",
      "VAL LOSS:  1.1786043988556434\n",
      "Training Loss:  1.1815728731580735\n",
      "VAL LOSS:  1.194854321949028\n",
      "Training Loss:  1.1818369522161043\n",
      "VAL LOSS:  1.1784680694147902\n",
      "Training Loss:  1.1813810579663067\n",
      "VAL LOSS:  1.194594406233478\n",
      "Training Loss:  1.1816439284945337\n",
      "VAL LOSS:  1.1783306727430456\n",
      "Training Loss:  1.1811948498219698\n",
      "VAL LOSS:  1.1943492559217614\n",
      "Training Loss:  1.181457244469616\n",
      "VAL LOSS:  1.1781928140056044\n",
      "Training Loss:  1.1810159818414345\n",
      "VAL LOSS:  1.1941206842639531\n",
      "Training Loss:  1.1812782141786675\n",
      "VAL LOSS:  1.178055134627685\n",
      "Training Loss:  1.1808428516421565\n",
      "VAL LOSS:  1.1939095611447974\n",
      "Training Loss:  1.1811084776611178\n",
      "VAL LOSS:  1.1779155406333752\n",
      "Training Loss:  1.1806781580649854\n",
      "VAL LOSS:  1.1937172476162676\n",
      "Training Loss:  1.180947024798002\n",
      "VAL LOSS:  1.1777741591740427\n",
      "Training Loss:  1.1805218424349788\n",
      "VAL LOSS:  1.1935459295131743\n",
      "Training Loss:  1.180796564589023\n",
      "VAL LOSS:  1.177630666879434\n",
      "Training Loss:  1.1803743615508526\n",
      "VAL LOSS:  1.193396149913589\n",
      "Training Loss:  1.180655155272838\n",
      "VAL LOSS:  1.1774861948882598\n",
      "Training Loss:  1.1802359302085577\n",
      "VAL LOSS:  1.1932683918390279\n",
      "Training Loss:  1.1805239901047115\n",
      "VAL LOSS:  1.177339281785996\n",
      "Training Loss:  1.1801075990483754\n",
      "VAL LOSS:  1.1931598349058905\n",
      "Training Loss:  1.1804030588186099\n",
      "VAL LOSS:  1.177189339613622\n",
      "Training Loss:  1.1799880155262652\n",
      "VAL LOSS:  1.1930707641781408\n",
      "Training Loss:  1.180291127047029\n",
      "VAL LOSS:  1.1770346789316004\n",
      "Training Loss:  1.1798753449297052\n",
      "VAL LOSS:  1.1930007088300032\n",
      "Training Loss:  1.180189613872728\n",
      "VAL LOSS:  1.176877797798566\n",
      "Training Loss:  1.1797701688465057\n",
      "VAL LOSS:  1.1929500345145294\n",
      "Training Loss:  1.1800974389363181\n",
      "VAL LOSS:  1.1767197780183953\n",
      "Training Loss:  1.1796731759246213\n",
      "VAL LOSS:  1.1929184846512295\n",
      "Training Loss:  1.1800145456304953\n",
      "VAL LOSS:  1.176559906136167\n",
      "Training Loss:  1.1795837820891053\n",
      "VAL LOSS:  1.192903704789266\n",
      "Training Loss:  1.17994021104453\n",
      "VAL LOSS:  1.1763990211692545\n",
      "(133554, 2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.75      0.70     63133\n",
      "           2       0.74      0.63      0.68     70421\n",
      "\n",
      "    accuracy                           0.69    133554\n",
      "   macro avg       0.69      0.69      0.69    133554\n",
      "weighted avg       0.70      0.69      0.69    133554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Wts,losses, val_losses = _main(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(0,200)\n",
    "plt.plot(epochs, losses, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Weights.model\", 'wb')\n",
    "pickle.dump(Wts, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if(\"woman\" in model.wv):\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
